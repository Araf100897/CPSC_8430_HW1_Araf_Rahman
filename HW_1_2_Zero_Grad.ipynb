{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d740c3aa-c6d6-4370-b53a-ce45ffd634f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from autograd_lib import autograd_lib\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca6a731-8e44-4f8f-882b-c245f91a9e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "result_folder = \"finalResult_with_hessian4_final1\"\n",
    "\n",
    "if not os.path.exists(result_folder):\n",
    "    os.mkdir(result_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44fc19f-5c68-4922-baa7-2526e1ab9d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_func = lambda x : (torch.sin(5*np.pi*x)) /(5*np.pi*x) \n",
    "num_of_rows = 300\n",
    "X= torch.unsqueeze(torch.linspace(-1,1,num_of_rows),dim=1)\n",
    "Y = Y_func(X)\n",
    "dataset = TensorDataset(X,Y)\n",
    "data_loader = DataLoader(dataset,1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c582707-79b6-43cd-a5db-8ff717770aae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(X, Y, color = \"red\")\n",
    "plt.title('Non-Linear Function Plotting')\n",
    "plt.xlabel('Independent varible')\n",
    "plt.ylabel('Dependent varible')\n",
    "plt.savefig(result_folder+'/func1_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd703c46-2bf3-4144-8d53-199cbfb9060c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FunctionRegressor(nn.Module):\n",
    "    def __init__(self, num_hidden=128):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    def training_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 \n",
    "        loss = loss_fn(out, targets)   \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 \n",
    "        loss = loss_fn(out, targets)    \n",
    "        return {'val_loss': loss.detach()}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def train_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 \n",
    "        loss = loss_fn(out, targets)    \n",
    "        return {'train_loss': loss.detach()}\n",
    "    \n",
    "    def train_epoch_end(self, outputs):\n",
    "        batch_losses = [x['train_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'train_loss': epoch_loss.item()}\n",
    "    \n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch, result['val_loss']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c222fa-886d-4b0e-ad2b-a6c9627e12d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_gradient_norm(model, criterion, train, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    output = model(train)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = []\n",
    "    for p in model.regressor.children():\n",
    "        if isinstance(p, nn.Linear):\n",
    "            param_norm = p.weight.grad.norm(2).item()\n",
    "            grads.append(param_norm)\n",
    "\n",
    "    grad_mean = np.mean(grads) \n",
    "\n",
    "    return grad_mean\n",
    "\n",
    "def save_activations(layer, A, _):\n",
    "    activations[layer] = A\n",
    "\n",
    "def compute_hessain(layer, _, B):\n",
    "    A = activations[layer]\n",
    "    BA = torch.einsum('nl,ni->nli', B, A) \n",
    "    hess[layer] += torch.einsum('nli,nkj->likj', BA, BA)\n",
    "    \n",
    "def calculate_minimum_ratio(model, criterion, train, target):\n",
    "    model.zero_grad()\n",
    "    \n",
    "    with autograd_lib.module_hook(save_activations):\n",
    "        output = model(train)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "    with autograd_lib.module_hook(compute_hessain):\n",
    "        autograd_lib.backward_hessian(output, loss='LeastSquares')\n",
    "\n",
    "    layer_hess = list(hess.values())\n",
    "    minimum_ratio = []\n",
    "\n",
    "    for h in layer_hess:\n",
    "        size = h.shape[0] * h.shape[1]\n",
    "        h = h.reshape(size, size)\n",
    "        \n",
    "        h_eig = torch.linalg.eigvalsh(h)\n",
    "        num_greater = torch.sum(h_eig > 0).item()\n",
    "        minimum_ratio.append(num_greater / len(h_eig))\n",
    "\n",
    "    ratio_mean = np.mean(minimum_ratio) \n",
    "\n",
    "    return ratio_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ee3697-eb06-4a33-aaf5-742b41878e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_norm_minimal_ratio(model,criterion):\n",
    "\n",
    "\n",
    "    gradient_norm = calculate_gradient_norm(model, criterion, X, Y)\n",
    "    minimum_ratio = calculate_minimum_ratio(model, criterion, X, Y)\n",
    "\n",
    "    print('gradient norm: {}, minimum ratio: {}'.format(gradient_norm, minimum_ratio))\n",
    "    result = {}\n",
    "    result[\"gradient_norm\"] = gradient_norm\n",
    "    result[\"ratio\"] = minimum_ratio\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7c3121-f182-4c91-9f44-4bdb03b9f4af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model,loss_fn, val_loader):\n",
    "    outputs = [model.validation_step(batch,loss_fn) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "    grad_all=0.0\n",
    "    grad =0\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad = (p.grad.cpu().data.numpy()**2).sum()\n",
    "            \n",
    "        grad_all+=grad\n",
    "        \n",
    "    gradient_norm=grad_all ** 0.5\n",
    "    return gradient_norm\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, data_loader, criterion,opt_func):\n",
    "    history = []\n",
    "    comparing_epoch_loss =1000.0\n",
    "    gradient_norm_per_epoch={}\n",
    "\n",
    "    \n",
    "#     train_history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "#         gradient_norm_per_epoch[epoch] = get_gradient_norm(model)\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "    \n",
    "\n",
    "        \n",
    "        # Training Phase \n",
    "        prediction = model(X)\n",
    "        loss = criterion(prediction, Y)     # must be (1. nn output, 2. target)\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        \n",
    "        \n",
    "        \n",
    "#             gradient_norm_per_epoch[epoch] = get_gradient_norm(model)\n",
    "        \n",
    "        gradient_norm_per_epoch[epoch] = get_norm_minimal_ratio(model,criterion)\n",
    "        \n",
    "        \n",
    "        optimizer.step() \n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        result = evaluate(model,criterion, data_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        \n",
    "        if epoch == 900:\n",
    "            comparing_epoch_loss= result[\"val_loss\"]\n",
    "  \n",
    "    return history,gradient_norm_per_epoch,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d892c64-4487-4ca9-8cfa-36ca24dbb1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "num_of_rows = 300\n",
    "lr = 0.0004\n",
    "gamma_lr_scheduler = 0.1 \n",
    "weight_decay = 1e-4\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "num_epochs =2500\n",
    "criterion_name = \"MSE_LOSS_\"\n",
    "optimizer_name = \"ADAM_opt\"\n",
    "filename = criterion_name+ optimizer_name+\".png\"\n",
    "gradient_norm_name = \"_gradient_norm_name1_2.png\"\n",
    "result_folder_name = \"result3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad9e742-ee01-49ea-8625-af739d0c786c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "num_epochs =2200\n",
    "criterion_name = \"MSE_LOSS_\"\n",
    "optimizer_name = \"ADAM_opt\"\n",
    "input_size=1\n",
    "output_size=1\n",
    "model= FunctionRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f24ad7b-792e-4a93-ae71-e04fc80d578c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "autograd_lib.register(model)\n",
    "activations = defaultdict(int)\n",
    "hess = defaultdict(float)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5407e3e6-65f9-41b9-b7e1-78fa9531976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arafr/.conda/envs/deepL/lib/python3.12/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.17534957826137543}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1 = evaluate(model,criterion,data_loader)\n",
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cb40142-f87d-447a-9522-f26aa0c1f8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = criterion_name+ optimizer_name+\".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5d245d-e41d-4ac1-bb99-e628c730f60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train,target = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6868f4f-6499-4b31-aa35-15860bf0320b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient norm: 1.3272387273609638, minimum ratio: 0.71484375\n",
      "Epoch [0], val_loss: 0.1842\n",
      "gradient norm: 1.393603229895234, minimum ratio: 0.6953125\n",
      "Epoch [1], val_loss: 0.1934\n",
      "gradient norm: 1.4600272439420223, minimum ratio: 0.734375\n",
      "Epoch [2], val_loss: 0.2031\n",
      "gradient norm: 1.5265095680952072, minimum ratio: 0.70703125\n",
      "Epoch [3], val_loss: 0.2132\n",
      "gradient norm: 1.5930377747863531, minimum ratio: 0.7421875\n",
      "Epoch [4], val_loss: 0.2238\n",
      "gradient norm: 1.65960599668324, minimum ratio: 0.70703125\n",
      "Epoch [5], val_loss: 0.2348\n",
      "gradient norm: 1.72620870731771, minimum ratio: 0.75\n",
      "Epoch [6], val_loss: 0.2463\n",
      "gradient norm: 1.7928659599274397, minimum ratio: 0.72265625\n",
      "Epoch [7], val_loss: 0.2582\n",
      "gradient norm: 1.8595834914594889, minimum ratio: 0.71875\n",
      "Epoch [8], val_loss: 0.2705\n",
      "gradient norm: 1.9263307675719261, minimum ratio: 0.71875\n",
      "Epoch [9], val_loss: 0.2833\n",
      "gradient norm: 1.9931526631116867, minimum ratio: 0.76171875\n",
      "Epoch [10], val_loss: 0.2965\n",
      "gradient norm: 2.06000598333776, minimum ratio: 0.71875\n",
      "Epoch [11], val_loss: 0.3102\n",
      "gradient norm: 2.1269011609256268, minimum ratio: 0.70703125\n",
      "Epoch [12], val_loss: 0.3243\n",
      "gradient norm: 2.1938791386783123, minimum ratio: 0.71484375\n",
      "Epoch [13], val_loss: 0.3389\n",
      "gradient norm: 2.2609098628163338, minimum ratio: 0.7421875\n",
      "Epoch [14], val_loss: 0.3539\n",
      "gradient norm: 2.327999822795391, minimum ratio: 0.7265625\n",
      "Epoch [15], val_loss: 0.3693\n",
      "gradient norm: 2.395144861191511, minimum ratio: 0.70703125\n",
      "Epoch [16], val_loss: 0.3853\n",
      "gradient norm: 2.4623651318252087, minimum ratio: 0.7421875\n",
      "Epoch [17], val_loss: 0.4016\n",
      "gradient norm: 2.5296516828238964, minimum ratio: 0.75390625\n",
      "Epoch [18], val_loss: 0.4184\n",
      "gradient norm: 2.596999056637287, minimum ratio: 0.73828125\n",
      "Epoch [19], val_loss: 0.4357\n",
      "gradient norm: 2.6644210927188396, minimum ratio: 0.74609375\n",
      "Epoch [20], val_loss: 0.4534\n",
      "gradient norm: 2.7319148741662502, minimum ratio: 0.7421875\n",
      "Epoch [21], val_loss: 0.4716\n",
      "gradient norm: 2.799479641020298, minimum ratio: 0.74609375\n",
      "Epoch [22], val_loss: 0.4902\n",
      "gradient norm: 2.867116827517748, minimum ratio: 0.72265625\n",
      "Epoch [23], val_loss: 0.5093\n",
      "gradient norm: 2.9348847232759, minimum ratio: 0.73828125\n",
      "Epoch [24], val_loss: 0.5288\n",
      "gradient norm: 3.002690400928259, minimum ratio: 0.71875\n",
      "Epoch [25], val_loss: 0.5488\n",
      "gradient norm: 3.0705968104302883, minimum ratio: 0.74609375\n",
      "Epoch [26], val_loss: 0.5692\n",
      "gradient norm: 3.1385791450738907, minimum ratio: 0.70703125\n",
      "Epoch [27], val_loss: 0.5901\n",
      "gradient norm: 3.2066348008811474, minimum ratio: 0.73046875\n",
      "Epoch [28], val_loss: 0.6115\n",
      "gradient norm: 3.2747686244547367, minimum ratio: 0.7109375\n",
      "Epoch [29], val_loss: 0.6333\n",
      "gradient norm: 3.3430255614221096, minimum ratio: 0.734375\n",
      "Epoch [30], val_loss: 0.6556\n",
      "gradient norm: 3.4113754592835903, minimum ratio: 0.75\n",
      "Epoch [31], val_loss: 0.6784\n",
      "gradient norm: 3.479815661907196, minimum ratio: 0.74609375\n",
      "Epoch [32], val_loss: 0.7016\n",
      "gradient norm: 3.5483525954186916, minimum ratio: 0.73828125\n",
      "Epoch [33], val_loss: 0.7253\n",
      "gradient norm: 3.617004584521055, minimum ratio: 0.75390625\n",
      "Epoch [34], val_loss: 0.7494\n",
      "gradient norm: 3.6857659965753555, minimum ratio: 0.734375\n",
      "Epoch [35], val_loss: 0.7741\n",
      "gradient norm: 3.754654549062252, minimum ratio: 0.71484375\n",
      "Epoch [36], val_loss: 0.7992\n",
      "gradient norm: 3.8236809745430946, minimum ratio: 0.7421875\n",
      "Epoch [37], val_loss: 0.8247\n",
      "gradient norm: 3.8927697613835335, minimum ratio: 0.73046875\n",
      "Epoch [38], val_loss: 0.8508\n",
      "gradient norm: 3.9619515389204025, minimum ratio: 0.75390625\n",
      "Epoch [39], val_loss: 0.8773\n",
      "gradient norm: 4.031282842159271, minimum ratio: 0.71875\n",
      "Epoch [40], val_loss: 0.9043\n",
      "gradient norm: 4.100720211863518, minimum ratio: 0.71875\n",
      "Epoch [41], val_loss: 0.9318\n",
      "gradient norm: 4.170297242701054, minimum ratio: 0.7421875\n",
      "Epoch [42], val_loss: 0.9598\n",
      "gradient norm: 4.239966094493866, minimum ratio: 0.7109375\n",
      "Epoch [43], val_loss: 0.9882\n",
      "gradient norm: 4.309758439660072, minimum ratio: 0.75390625\n",
      "Epoch [44], val_loss: 1.0171\n",
      "gradient norm: 4.379709906876087, minimum ratio: 0.76953125\n",
      "Epoch [45], val_loss: 1.0465\n",
      "gradient norm: 4.449781373143196, minimum ratio: 0.7421875\n",
      "Epoch [46], val_loss: 1.0764\n",
      "gradient norm: 4.519995145499706, minimum ratio: 0.7578125\n",
      "Epoch [47], val_loss: 1.1068\n",
      "gradient norm: 4.5903085842728615, minimum ratio: 0.734375\n",
      "Epoch [48], val_loss: 1.1377\n",
      "gradient norm: 4.660805009305477, minimum ratio: 0.74609375\n",
      "Epoch [49], val_loss: 1.1691\n",
      "gradient norm: 4.7314325496554375, minimum ratio: 0.74609375\n",
      "Epoch [50], val_loss: 1.2009\n",
      "gradient norm: 4.80217520147562, minimum ratio: 0.7421875\n",
      "Epoch [51], val_loss: 1.2333\n",
      "gradient norm: 4.873036079108715, minimum ratio: 0.74609375\n",
      "Epoch [52], val_loss: 1.2661\n",
      "gradient norm: 4.944083958864212, minimum ratio: 0.7265625\n",
      "Epoch [53], val_loss: 1.2995\n",
      "gradient norm: 5.0152300372719765, minimum ratio: 0.75\n",
      "Epoch [54], val_loss: 1.3334\n",
      "gradient norm: 5.086542084813118, minimum ratio: 0.7421875\n",
      "Epoch [55], val_loss: 1.3677\n",
      "gradient norm: 5.158000826835632, minimum ratio: 0.75\n",
      "Epoch [56], val_loss: 1.4026\n",
      "gradient norm: 5.229651667177677, minimum ratio: 0.74609375\n",
      "Epoch [57], val_loss: 1.4380\n",
      "gradient norm: 5.301430836319923, minimum ratio: 0.74609375\n",
      "Epoch [58], val_loss: 1.4739\n",
      "gradient norm: 5.373354576528072, minimum ratio: 0.7578125\n",
      "Epoch [59], val_loss: 1.5103\n",
      "gradient norm: 5.445473611354828, minimum ratio: 0.7421875\n",
      "Epoch [60], val_loss: 1.5472\n",
      "gradient norm: 5.517693176865578, minimum ratio: 0.7421875\n",
      "Epoch [61], val_loss: 1.5846\n",
      "gradient norm: 5.59007702767849, minimum ratio: 0.734375\n",
      "Epoch [62], val_loss: 1.6225\n",
      "gradient norm: 5.6626390144228935, minimum ratio: 0.7578125\n",
      "Epoch [63], val_loss: 1.6610\n",
      "gradient norm: 5.735304571688175, minimum ratio: 0.7421875\n",
      "Epoch [64], val_loss: 1.7000\n",
      "gradient norm: 5.808258682489395, minimum ratio: 0.76953125\n",
      "Epoch [65], val_loss: 1.7395\n",
      "gradient norm: 5.881314113736153, minimum ratio: 0.7421875\n",
      "Epoch [66], val_loss: 1.7795\n",
      "gradient norm: 5.954502113163471, minimum ratio: 0.73828125\n",
      "Epoch [67], val_loss: 1.8201\n",
      "gradient norm: 6.027887642383575, minimum ratio: 0.74609375\n",
      "Epoch [68], val_loss: 1.8612\n",
      "gradient norm: 6.101530805230141, minimum ratio: 0.76171875\n",
      "Epoch [69], val_loss: 1.9028\n",
      "gradient norm: 6.1753218322992325, minimum ratio: 0.7578125\n",
      "Epoch [70], val_loss: 1.9450\n",
      "gradient norm: 6.249246880412102, minimum ratio: 0.75\n",
      "Epoch [71], val_loss: 1.9877\n",
      "gradient norm: 6.3233955055475235, minimum ratio: 0.7265625\n",
      "Epoch [72], val_loss: 2.0310\n",
      "gradient norm: 6.397708311676979, minimum ratio: 0.73828125\n",
      "Epoch [73], val_loss: 2.0748\n",
      "gradient norm: 6.472240075469017, minimum ratio: 0.734375\n",
      "Epoch [74], val_loss: 2.1191\n",
      "gradient norm: 6.546920523047447, minimum ratio: 0.75390625\n",
      "Epoch [75], val_loss: 2.1640\n",
      "gradient norm: 6.621831014752388, minimum ratio: 0.75\n",
      "Epoch [76], val_loss: 2.2094\n",
      "gradient norm: 6.696940362453461, minimum ratio: 0.7734375\n",
      "Epoch [77], val_loss: 2.2554\n",
      "gradient norm: 6.772200435400009, minimum ratio: 0.73828125\n",
      "Epoch [78], val_loss: 2.3020\n",
      "gradient norm: 6.847648486495018, minimum ratio: 0.75\n",
      "Epoch [79], val_loss: 2.3491\n",
      "gradient norm: 6.923265367746353, minimum ratio: 0.7421875\n",
      "Epoch [80], val_loss: 2.3968\n",
      "gradient norm: 6.999111518263817, minimum ratio: 0.74609375\n",
      "Epoch [81], val_loss: 2.4450\n",
      "gradient norm: 7.075239941477776, minimum ratio: 0.73828125\n",
      "Epoch [82], val_loss: 2.4938\n",
      "gradient norm: 7.151490807533264, minimum ratio: 0.73828125\n",
      "Epoch [83], val_loss: 2.5432\n",
      "gradient norm: 7.228013515472412, minimum ratio: 0.75390625\n",
      "Epoch [84], val_loss: 2.5931\n",
      "gradient norm: 7.30471833050251, minimum ratio: 0.74609375\n",
      "Epoch [85], val_loss: 2.6436\n",
      "gradient norm: 7.381510838866234, minimum ratio: 0.75390625\n",
      "Epoch [86], val_loss: 2.6947\n",
      "gradient norm: 7.458600416779518, minimum ratio: 0.75390625\n",
      "Epoch [87], val_loss: 2.7464\n",
      "gradient norm: 7.535955041646957, minimum ratio: 0.73828125\n",
      "Epoch [88], val_loss: 2.7987\n",
      "gradient norm: 7.613446548581123, minimum ratio: 0.74609375\n",
      "Epoch [89], val_loss: 2.8515\n",
      "gradient norm: 7.691162332892418, minimum ratio: 0.76171875\n",
      "Epoch [90], val_loss: 2.9049\n",
      "gradient norm: 7.769057944417, minimum ratio: 0.75390625\n",
      "Epoch [91], val_loss: 2.9590\n",
      "gradient norm: 7.847233459353447, minimum ratio: 0.7578125\n",
      "Epoch [92], val_loss: 3.0136\n",
      "gradient norm: 7.925640374422073, minimum ratio: 0.765625\n",
      "Epoch [93], val_loss: 3.0688\n",
      "gradient norm: 8.00422140955925, minimum ratio: 0.75390625\n",
      "Epoch [94], val_loss: 3.1246\n",
      "gradient norm: 8.083038315176964, minimum ratio: 0.76953125\n",
      "Epoch [95], val_loss: 3.1810\n",
      "gradient norm: 8.162140935659409, minimum ratio: 0.76953125\n",
      "Epoch [96], val_loss: 3.2380\n",
      "gradient norm: 8.241503208875656, minimum ratio: 0.73828125\n",
      "Epoch [97], val_loss: 3.2957\n",
      "gradient norm: 8.320943683385849, minimum ratio: 0.73828125\n",
      "Epoch [98], val_loss: 3.3539\n",
      "gradient norm: 8.400695115327835, minimum ratio: 0.73828125\n",
      "Epoch [99], val_loss: 3.4128\n",
      "gradient norm: 8.480728149414062, minimum ratio: 0.73828125\n",
      "Epoch [100], val_loss: 3.4723\n",
      "gradient norm: 8.561051845550537, minimum ratio: 0.7421875\n",
      "Epoch [101], val_loss: 3.5324\n",
      "gradient norm: 8.6414086073637, minimum ratio: 0.75390625\n",
      "Epoch [102], val_loss: 3.5931\n",
      "gradient norm: 8.722150832414627, minimum ratio: 0.74609375\n",
      "Epoch [103], val_loss: 3.6545\n",
      "gradient norm: 8.8031345307827, minimum ratio: 0.7578125\n",
      "Epoch [104], val_loss: 3.7165\n",
      "gradient norm: 8.884280383586884, minimum ratio: 0.7421875\n",
      "Epoch [105], val_loss: 3.7791\n",
      "gradient norm: 8.965828150510788, minimum ratio: 0.75\n",
      "Epoch [106], val_loss: 3.8424\n",
      "gradient norm: 9.047532394528389, minimum ratio: 0.73828125\n",
      "Epoch [107], val_loss: 3.9063\n",
      "gradient norm: 9.129377946257591, minimum ratio: 0.7421875\n",
      "Epoch [108], val_loss: 3.9709\n",
      "gradient norm: 9.211579039692879, minimum ratio: 0.7578125\n",
      "Epoch [109], val_loss: 4.0361\n",
      "gradient norm: 9.294071346521378, minimum ratio: 0.76171875\n",
      "Epoch [110], val_loss: 4.1020\n",
      "gradient norm: 9.376799657940865, minimum ratio: 0.765625\n",
      "Epoch [111], val_loss: 4.1685\n",
      "gradient norm: 9.459779247641563, minimum ratio: 0.76953125\n",
      "Epoch [112], val_loss: 4.2357\n",
      "gradient norm: 9.543135777115822, minimum ratio: 0.75\n",
      "Epoch [113], val_loss: 4.3036\n",
      "gradient norm: 9.626532956957817, minimum ratio: 0.7578125\n",
      "Epoch [114], val_loss: 4.3721\n",
      "gradient norm: 9.710454657673836, minimum ratio: 0.78515625\n",
      "Epoch [115], val_loss: 4.4413\n",
      "gradient norm: 9.794511690735817, minimum ratio: 0.7578125\n",
      "Epoch [116], val_loss: 4.5112\n",
      "gradient norm: 9.878753870725632, minimum ratio: 0.76171875\n",
      "Epoch [117], val_loss: 4.5817\n",
      "gradient norm: 9.963281571865082, minimum ratio: 0.7734375\n",
      "Epoch [118], val_loss: 4.6530\n",
      "gradient norm: 10.048309057950974, minimum ratio: 0.7109375\n",
      "Epoch [119], val_loss: 4.7249\n",
      "gradient norm: 10.133461564779282, minimum ratio: 0.74609375\n",
      "Epoch [120], val_loss: 4.7975\n",
      "gradient norm: 10.219005167484283, minimum ratio: 0.74609375\n",
      "Epoch [121], val_loss: 4.8708\n",
      "gradient norm: 10.304569631814957, minimum ratio: 0.75390625\n",
      "Epoch [122], val_loss: 4.9449\n",
      "gradient norm: 10.390750348567963, minimum ratio: 0.75\n",
      "Epoch [123], val_loss: 5.0196\n",
      "gradient norm: 10.476885944604874, minimum ratio: 0.734375\n",
      "Epoch [124], val_loss: 5.0950\n",
      "gradient norm: 10.563546687364578, minimum ratio: 0.7734375\n",
      "Epoch [125], val_loss: 5.1712\n",
      "gradient norm: 10.650442332029343, minimum ratio: 0.77734375\n",
      "Epoch [126], val_loss: 5.2480\n",
      "gradient norm: 10.737578839063644, minimum ratio: 0.765625\n",
      "Epoch [127], val_loss: 5.3256\n",
      "gradient norm: 10.825143039226532, minimum ratio: 0.73828125\n",
      "Epoch [128], val_loss: 5.4039\n",
      "gradient norm: 10.912828415632248, minimum ratio: 0.7578125\n",
      "Epoch [129], val_loss: 5.4829\n",
      "gradient norm: 11.00077748298645, minimum ratio: 0.74609375\n",
      "Epoch [130], val_loss: 5.5627\n",
      "gradient norm: 11.089299440383911, minimum ratio: 0.73828125\n",
      "Epoch [131], val_loss: 5.6432\n",
      "gradient norm: 11.177935659885406, minimum ratio: 0.75\n",
      "Epoch [132], val_loss: 5.7244\n",
      "gradient norm: 11.267056375741959, minimum ratio: 0.77734375\n",
      "Epoch [133], val_loss: 5.8064\n",
      "gradient norm: 11.356210559606552, minimum ratio: 0.76171875\n",
      "Epoch [134], val_loss: 5.8892\n",
      "gradient norm: 11.445740699768066, minimum ratio: 0.7421875\n",
      "Epoch [135], val_loss: 5.9726\n",
      "gradient norm: 11.535787045955658, minimum ratio: 0.76171875\n",
      "Epoch [136], val_loss: 6.0569\n",
      "gradient norm: 11.625998944044113, minimum ratio: 0.75\n",
      "Epoch [137], val_loss: 6.1419\n",
      "gradient norm: 11.716429889202118, minimum ratio: 0.75390625\n",
      "Epoch [138], val_loss: 6.2277\n",
      "gradient norm: 11.807380706071854, minimum ratio: 0.73828125\n",
      "Epoch [139], val_loss: 6.3143\n",
      "gradient norm: 11.898489892482758, minimum ratio: 0.7578125\n",
      "Epoch [140], val_loss: 6.4016\n",
      "gradient norm: 11.990175575017929, minimum ratio: 0.7578125\n",
      "Epoch [141], val_loss: 6.4897\n",
      "gradient norm: 12.081920206546783, minimum ratio: 0.75390625\n",
      "Epoch [142], val_loss: 6.5786\n",
      "gradient norm: 12.174093514680862, minimum ratio: 0.7578125\n",
      "Epoch [143], val_loss: 6.6683\n",
      "gradient norm: 12.266764611005783, minimum ratio: 0.75\n",
      "Epoch [144], val_loss: 6.7588\n",
      "gradient norm: 12.359584271907806, minimum ratio: 0.7734375\n",
      "Epoch [145], val_loss: 6.8501\n",
      "gradient norm: 12.45274305343628, minimum ratio: 0.76171875\n",
      "Epoch [146], val_loss: 6.9422\n",
      "gradient norm: 12.546266317367554, minimum ratio: 0.78125\n",
      "Epoch [147], val_loss: 7.0352\n",
      "gradient norm: 12.639903485774994, minimum ratio: 0.7734375\n",
      "Epoch [148], val_loss: 7.1289\n",
      "gradient norm: 12.734217673540115, minimum ratio: 0.75\n",
      "Epoch [149], val_loss: 7.2235\n",
      "gradient norm: 12.828870922327042, minimum ratio: 0.74609375\n",
      "Epoch [150], val_loss: 7.3189\n",
      "gradient norm: 12.923717081546783, minimum ratio: 0.7421875\n",
      "Epoch [151], val_loss: 7.4151\n",
      "gradient norm: 13.018764197826385, minimum ratio: 0.7421875\n",
      "Epoch [152], val_loss: 7.5121\n",
      "gradient norm: 13.114079922437668, minimum ratio: 0.7734375\n",
      "Epoch [153], val_loss: 7.6100\n",
      "gradient norm: 13.210312277078629, minimum ratio: 0.75390625\n",
      "Epoch [154], val_loss: 7.7088\n",
      "gradient norm: 13.306661933660507, minimum ratio: 0.76171875\n",
      "Epoch [155], val_loss: 7.8084\n",
      "gradient norm: 13.403321593999863, minimum ratio: 0.7734375\n",
      "Epoch [156], val_loss: 7.9088\n",
      "gradient norm: 13.500409424304962, minimum ratio: 0.7578125\n",
      "Epoch [157], val_loss: 8.0102\n",
      "gradient norm: 13.597803324460983, minimum ratio: 0.78125\n",
      "Epoch [158], val_loss: 8.1123\n",
      "gradient norm: 13.69547438621521, minimum ratio: 0.765625\n",
      "Epoch [159], val_loss: 8.2154\n",
      "gradient norm: 13.793410956859589, minimum ratio: 0.75390625\n",
      "Epoch [160], val_loss: 8.3193\n",
      "gradient norm: 13.892038762569427, minimum ratio: 0.7421875\n",
      "Epoch [161], val_loss: 8.4242\n",
      "gradient norm: 13.99116462469101, minimum ratio: 0.7578125\n",
      "Epoch [162], val_loss: 8.5299\n",
      "gradient norm: 14.090282768011093, minimum ratio: 0.7578125\n",
      "Epoch [163], val_loss: 8.6365\n",
      "gradient norm: 14.189568132162094, minimum ratio: 0.77734375\n",
      "Epoch [164], val_loss: 8.7440\n",
      "gradient norm: 14.289610713720322, minimum ratio: 0.79296875\n",
      "Epoch [165], val_loss: 8.8524\n",
      "gradient norm: 14.389895975589752, minimum ratio: 0.7734375\n",
      "Epoch [166], val_loss: 8.9617\n",
      "gradient norm: 14.49107575416565, minimum ratio: 0.765625\n",
      "Epoch [167], val_loss: 9.0720\n",
      "gradient norm: 14.59216234087944, minimum ratio: 0.74609375\n",
      "Epoch [168], val_loss: 9.1832\n",
      "gradient norm: 14.69356682896614, minimum ratio: 0.7421875\n",
      "Epoch [169], val_loss: 9.2954\n",
      "gradient norm: 14.795424073934555, minimum ratio: 0.7890625\n",
      "Epoch [170], val_loss: 9.4084\n",
      "gradient norm: 14.897979825735092, minimum ratio: 0.76171875\n",
      "Epoch [171], val_loss: 9.5224\n",
      "gradient norm: 15.000550538301468, minimum ratio: 0.78125\n",
      "Epoch [172], val_loss: 9.6374\n",
      "gradient norm: 15.103919476270676, minimum ratio: 0.76953125\n",
      "Epoch [173], val_loss: 9.7533\n",
      "gradient norm: 15.207093983888626, minimum ratio: 0.7421875\n",
      "Epoch [174], val_loss: 9.8702\n",
      "gradient norm: 15.310974657535553, minimum ratio: 0.765625\n",
      "Epoch [175], val_loss: 9.9880\n",
      "gradient norm: 15.415560483932495, minimum ratio: 0.76953125\n",
      "Epoch [176], val_loss: 10.1069\n",
      "gradient norm: 15.520171880722046, minimum ratio: 0.765625\n",
      "Epoch [177], val_loss: 10.2267\n",
      "gradient norm: 15.6251540184021, minimum ratio: 0.734375\n",
      "Epoch [178], val_loss: 10.3474\n",
      "gradient norm: 15.731088519096375, minimum ratio: 0.765625\n",
      "Epoch [179], val_loss: 10.4692\n",
      "gradient norm: 15.837016999721527, minimum ratio: 0.74609375\n",
      "Epoch [180], val_loss: 10.5920\n",
      "gradient norm: 15.943196594715118, minimum ratio: 0.765625\n",
      "Epoch [181], val_loss: 10.7158\n",
      "gradient norm: 16.049849092960358, minimum ratio: 0.7734375\n",
      "Epoch [182], val_loss: 10.8406\n",
      "gradient norm: 16.157190442085266, minimum ratio: 0.7578125\n",
      "Epoch [183], val_loss: 10.9664\n",
      "gradient norm: 16.264725923538208, minimum ratio: 0.75390625\n",
      "Epoch [184], val_loss: 11.0932\n",
      "gradient norm: 16.372973680496216, minimum ratio: 0.78515625\n",
      "Epoch [185], val_loss: 11.2210\n",
      "gradient norm: 16.481441974639893, minimum ratio: 0.79296875\n",
      "Epoch [186], val_loss: 11.3499\n",
      "gradient norm: 16.59033763408661, minimum ratio: 0.7734375\n",
      "Epoch [187], val_loss: 11.4799\n",
      "gradient norm: 16.699710071086884, minimum ratio: 0.7578125\n",
      "Epoch [188], val_loss: 11.6109\n",
      "gradient norm: 16.809578478336334, minimum ratio: 0.765625\n",
      "Epoch [189], val_loss: 11.7430\n",
      "gradient norm: 16.91995882987976, minimum ratio: 0.74609375\n",
      "Epoch [190], val_loss: 11.8761\n",
      "gradient norm: 17.031018614768982, minimum ratio: 0.76953125\n",
      "Epoch [191], val_loss: 12.0103\n",
      "gradient norm: 17.141946732997894, minimum ratio: 0.765625\n",
      "Epoch [192], val_loss: 12.1456\n",
      "gradient norm: 17.253682851791382, minimum ratio: 0.75390625\n",
      "Epoch [193], val_loss: 12.2820\n",
      "gradient norm: 17.365785658359528, minimum ratio: 0.78125\n",
      "Epoch [194], val_loss: 12.4194\n",
      "gradient norm: 17.478001058101654, minimum ratio: 0.765625\n",
      "Epoch [195], val_loss: 12.5580\n",
      "gradient norm: 17.59096658229828, minimum ratio: 0.765625\n",
      "Epoch [196], val_loss: 12.6976\n",
      "gradient norm: 17.704331815242767, minimum ratio: 0.77734375\n",
      "Epoch [197], val_loss: 12.8384\n",
      "gradient norm: 17.81843614578247, minimum ratio: 0.7734375\n",
      "Epoch [198], val_loss: 12.9803\n",
      "gradient norm: 17.932753264904022, minimum ratio: 0.78125\n",
      "Epoch [199], val_loss: 13.1233\n",
      "gradient norm: 18.047398388385773, minimum ratio: 0.78515625\n",
      "Epoch [200], val_loss: 13.2675\n",
      "gradient norm: 18.162581622600555, minimum ratio: 0.765625\n",
      "Epoch [201], val_loss: 13.4128\n",
      "gradient norm: 18.27871286869049, minimum ratio: 0.78125\n",
      "Epoch [202], val_loss: 13.5593\n",
      "gradient norm: 18.394768238067627, minimum ratio: 0.76171875\n",
      "Epoch [203], val_loss: 13.7069\n",
      "gradient norm: 18.51147300004959, minimum ratio: 0.78515625\n",
      "Epoch [204], val_loss: 13.8557\n",
      "gradient norm: 18.62841647863388, minimum ratio: 0.76953125\n",
      "Epoch [205], val_loss: 14.0056\n",
      "gradient norm: 18.745935082435608, minimum ratio: 0.7734375\n",
      "Epoch [206], val_loss: 14.1567\n",
      "gradient norm: 18.864170253276825, minimum ratio: 0.7734375\n",
      "Epoch [207], val_loss: 14.3090\n",
      "gradient norm: 18.982711255550385, minimum ratio: 0.79296875\n",
      "Epoch [208], val_loss: 14.4625\n",
      "gradient norm: 19.101886987686157, minimum ratio: 0.7734375\n",
      "Epoch [209], val_loss: 14.6172\n",
      "gradient norm: 19.221373558044434, minimum ratio: 0.7578125\n",
      "Epoch [210], val_loss: 14.7731\n",
      "gradient norm: 19.341625571250916, minimum ratio: 0.765625\n",
      "Epoch [211], val_loss: 14.9302\n",
      "gradient norm: 19.462221086025238, minimum ratio: 0.76953125\n",
      "Epoch [212], val_loss: 15.0885\n",
      "gradient norm: 19.583232522010803, minimum ratio: 0.77734375\n",
      "Epoch [213], val_loss: 15.2480\n",
      "gradient norm: 19.704752206802368, minimum ratio: 0.765625\n",
      "Epoch [214], val_loss: 15.4088\n",
      "gradient norm: 19.826224446296692, minimum ratio: 0.80078125\n",
      "Epoch [215], val_loss: 15.5708\n",
      "gradient norm: 19.94884157180786, minimum ratio: 0.78125\n",
      "Epoch [216], val_loss: 15.7341\n",
      "gradient norm: 20.07178556919098, minimum ratio: 0.78125\n",
      "Epoch [217], val_loss: 15.8986\n",
      "gradient norm: 20.195055842399597, minimum ratio: 0.74609375\n",
      "Epoch [218], val_loss: 16.0644\n",
      "gradient norm: 20.31881672143936, minimum ratio: 0.78125\n",
      "Epoch [219], val_loss: 16.2314\n",
      "gradient norm: 20.44358789920807, minimum ratio: 0.75\n",
      "Epoch [220], val_loss: 16.3998\n",
      "gradient norm: 20.568585097789764, minimum ratio: 0.7734375\n",
      "Epoch [221], val_loss: 16.5695\n",
      "gradient norm: 20.694271624088287, minimum ratio: 0.76171875\n",
      "Epoch [222], val_loss: 16.7404\n",
      "gradient norm: 20.820148766040802, minimum ratio: 0.77734375\n",
      "Epoch [223], val_loss: 16.9126\n",
      "gradient norm: 20.94649440050125, minimum ratio: 0.7421875\n",
      "Epoch [224], val_loss: 17.0862\n",
      "gradient norm: 21.07378476858139, minimum ratio: 0.78515625\n",
      "Epoch [225], val_loss: 17.2610\n",
      "gradient norm: 21.201270401477814, minimum ratio: 0.76171875\n",
      "Epoch [226], val_loss: 17.4372\n",
      "gradient norm: 21.329369723796844, minimum ratio: 0.7578125\n",
      "Epoch [227], val_loss: 17.6147\n",
      "gradient norm: 21.457706809043884, minimum ratio: 0.76953125\n",
      "Epoch [228], val_loss: 17.7936\n",
      "gradient norm: 21.58712089061737, minimum ratio: 0.734375\n",
      "Epoch [229], val_loss: 17.9738\n",
      "gradient norm: 21.716824293136597, minimum ratio: 0.7421875\n",
      "Epoch [230], val_loss: 18.1553\n",
      "gradient norm: 21.846672356128693, minimum ratio: 0.75390625\n",
      "Epoch [231], val_loss: 18.3383\n",
      "gradient norm: 21.97707736492157, minimum ratio: 0.7890625\n",
      "Epoch [232], val_loss: 18.5225\n",
      "gradient norm: 22.108453571796417, minimum ratio: 0.75390625\n",
      "Epoch [233], val_loss: 18.7082\n",
      "gradient norm: 22.24029529094696, minimum ratio: 0.765625\n",
      "Epoch [234], val_loss: 18.8953\n",
      "gradient norm: 22.372337222099304, minimum ratio: 0.77734375\n",
      "Epoch [235], val_loss: 19.0837\n",
      "gradient norm: 22.50506627559662, minimum ratio: 0.77734375\n",
      "Epoch [236], val_loss: 19.2735\n",
      "gradient norm: 22.63826984167099, minimum ratio: 0.78125\n",
      "Epoch [237], val_loss: 19.4648\n",
      "gradient norm: 22.77217733860016, minimum ratio: 0.77734375\n",
      "Epoch [238], val_loss: 19.6574\n",
      "gradient norm: 22.906286597251892, minimum ratio: 0.76953125\n",
      "Epoch [239], val_loss: 19.8515\n",
      "gradient norm: 23.041310787200928, minimum ratio: 0.7578125\n",
      "Epoch [240], val_loss: 20.0470\n",
      "gradient norm: 23.176580667495728, minimum ratio: 0.76953125\n",
      "Epoch [241], val_loss: 20.2440\n",
      "gradient norm: 23.312507331371307, minimum ratio: 0.77734375\n",
      "Epoch [242], val_loss: 20.4424\n",
      "gradient norm: 23.449271142482758, minimum ratio: 0.7578125\n",
      "Epoch [243], val_loss: 20.6423\n",
      "gradient norm: 23.58624166250229, minimum ratio: 0.78515625\n",
      "Epoch [244], val_loss: 20.8436\n",
      "gradient norm: 23.723709881305695, minimum ratio: 0.7578125\n",
      "Epoch [245], val_loss: 21.0465\n",
      "gradient norm: 23.86178195476532, minimum ratio: 0.765625\n",
      "Epoch [246], val_loss: 21.2508\n",
      "gradient norm: 24.000246047973633, minimum ratio: 0.78125\n",
      "Epoch [247], val_loss: 21.4566\n",
      "gradient norm: 24.13915252685547, minimum ratio: 0.7734375\n",
      "Epoch [248], val_loss: 21.6639\n",
      "gradient norm: 24.27927553653717, minimum ratio: 0.7890625\n",
      "Epoch [249], val_loss: 21.8728\n",
      "gradient norm: 24.419912815093994, minimum ratio: 0.7578125\n",
      "Epoch [250], val_loss: 22.0831\n",
      "gradient norm: 24.560725331306458, minimum ratio: 0.76171875\n",
      "Epoch [251], val_loss: 22.2950\n",
      "gradient norm: 24.701594829559326, minimum ratio: 0.75390625\n",
      "Epoch [252], val_loss: 22.5084\n",
      "gradient norm: 24.8441104888916, minimum ratio: 0.76171875\n",
      "Epoch [253], val_loss: 22.7234\n",
      "gradient norm: 24.986870765686035, minimum ratio: 0.76171875\n",
      "Epoch [254], val_loss: 22.9399\n",
      "gradient norm: 25.13000512123108, minimum ratio: 0.765625\n",
      "Epoch [255], val_loss: 23.1580\n",
      "gradient norm: 25.27361249923706, minimum ratio: 0.77734375\n",
      "Epoch [256], val_loss: 23.3777\n",
      "gradient norm: 25.41790008544922, minimum ratio: 0.76171875\n",
      "Epoch [257], val_loss: 23.5990\n",
      "gradient norm: 25.562822937965393, minimum ratio: 0.7734375\n",
      "Epoch [258], val_loss: 23.8218\n",
      "gradient norm: 25.708543181419373, minimum ratio: 0.78125\n",
      "Epoch [259], val_loss: 24.0463\n",
      "gradient norm: 25.85481321811676, minimum ratio: 0.78125\n",
      "Epoch [260], val_loss: 24.2723\n",
      "gradient norm: 26.00069499015808, minimum ratio: 0.7734375\n",
      "Epoch [261], val_loss: 24.5000\n",
      "gradient norm: 26.14835488796234, minimum ratio: 0.73828125\n",
      "Epoch [262], val_loss: 24.7293\n",
      "gradient norm: 26.29592204093933, minimum ratio: 0.765625\n",
      "Epoch [263], val_loss: 24.9602\n",
      "gradient norm: 26.44367206096649, minimum ratio: 0.8046875\n",
      "Epoch [264], val_loss: 25.1927\n",
      "gradient norm: 26.59276509284973, minimum ratio: 0.78515625\n",
      "Epoch [265], val_loss: 25.4269\n",
      "gradient norm: 26.742352604866028, minimum ratio: 0.76953125\n",
      "Epoch [266], val_loss: 25.6628\n",
      "gradient norm: 26.892893195152283, minimum ratio: 0.77734375\n",
      "Epoch [267], val_loss: 25.9003\n",
      "gradient norm: 27.043786644935608, minimum ratio: 0.765625\n",
      "Epoch [268], val_loss: 26.1395\n",
      "gradient norm: 27.194628596305847, minimum ratio: 0.78125\n",
      "Epoch [269], val_loss: 26.3804\n",
      "gradient norm: 27.346291303634644, minimum ratio: 0.734375\n",
      "Epoch [270], val_loss: 26.6230\n",
      "gradient norm: 27.49903154373169, minimum ratio: 0.77734375\n",
      "Epoch [271], val_loss: 26.8674\n",
      "gradient norm: 27.652106285095215, minimum ratio: 0.765625\n",
      "Epoch [272], val_loss: 27.1134\n",
      "gradient norm: 27.805696606636047, minimum ratio: 0.76953125\n",
      "Epoch [273], val_loss: 27.3610\n",
      "gradient norm: 27.959954857826233, minimum ratio: 0.75\n",
      "Epoch [274], val_loss: 27.6105\n",
      "gradient norm: 28.11468732357025, minimum ratio: 0.76171875\n",
      "Epoch [275], val_loss: 27.8617\n",
      "gradient norm: 28.26985538005829, minimum ratio: 0.76171875\n",
      "Epoch [276], val_loss: 28.1146\n",
      "gradient norm: 28.425896286964417, minimum ratio: 0.77734375\n",
      "Epoch [277], val_loss: 28.3693\n",
      "gradient norm: 28.58265733718872, minimum ratio: 0.75\n",
      "Epoch [278], val_loss: 28.6257\n",
      "gradient norm: 28.739858508110046, minimum ratio: 0.78515625\n",
      "Epoch [279], val_loss: 28.8840\n",
      "gradient norm: 28.898081421852112, minimum ratio: 0.80078125\n",
      "Epoch [280], val_loss: 29.1440\n",
      "gradient norm: 29.055899262428284, minimum ratio: 0.76171875\n",
      "Epoch [281], val_loss: 29.4059\n",
      "gradient norm: 29.215160965919495, minimum ratio: 0.76953125\n",
      "Epoch [282], val_loss: 29.6695\n",
      "gradient norm: 29.37424063682556, minimum ratio: 0.78125\n",
      "Epoch [283], val_loss: 29.9350\n",
      "gradient norm: 29.5346759557724, minimum ratio: 0.7734375\n",
      "Epoch [284], val_loss: 30.2023\n",
      "gradient norm: 29.695329904556274, minimum ratio: 0.7890625\n",
      "Epoch [285], val_loss: 30.4715\n",
      "gradient norm: 29.85700237751007, minimum ratio: 0.796875\n",
      "Epoch [286], val_loss: 30.7426\n",
      "gradient norm: 30.01938307285309, minimum ratio: 0.78125\n",
      "Epoch [287], val_loss: 31.0155\n",
      "gradient norm: 30.181984543800354, minimum ratio: 0.76953125\n",
      "Epoch [288], val_loss: 31.2903\n",
      "gradient norm: 30.345494747161865, minimum ratio: 0.76171875\n",
      "Epoch [289], val_loss: 31.5670\n",
      "gradient norm: 30.509477376937866, minimum ratio: 0.76953125\n",
      "Epoch [290], val_loss: 31.8455\n",
      "gradient norm: 30.67408037185669, minimum ratio: 0.77734375\n",
      "Epoch [291], val_loss: 32.1260\n",
      "gradient norm: 30.839555382728577, minimum ratio: 0.76953125\n",
      "Epoch [292], val_loss: 32.4083\n",
      "gradient norm: 31.006603121757507, minimum ratio: 0.77734375\n",
      "Epoch [293], val_loss: 32.6926\n",
      "gradient norm: 31.1726655960083, minimum ratio: 0.76953125\n",
      "Epoch [294], val_loss: 32.9788\n",
      "gradient norm: 31.339122533798218, minimum ratio: 0.75390625\n",
      "Epoch [295], val_loss: 33.2669\n",
      "gradient norm: 31.50662052631378, minimum ratio: 0.79296875\n",
      "Epoch [296], val_loss: 33.5569\n",
      "gradient norm: 31.675524711608887, minimum ratio: 0.75390625\n",
      "Epoch [297], val_loss: 33.8490\n",
      "gradient norm: 31.845024704933167, minimum ratio: 0.74609375\n",
      "Epoch [298], val_loss: 34.1430\n",
      "gradient norm: 32.01534855365753, minimum ratio: 0.79296875\n",
      "Epoch [299], val_loss: 34.4390\n",
      "gradient norm: 32.185391902923584, minimum ratio: 0.7578125\n",
      "Epoch [300], val_loss: 34.7371\n",
      "gradient norm: 32.35620450973511, minimum ratio: 0.78125\n",
      "Epoch [301], val_loss: 35.0371\n",
      "gradient norm: 32.528305411338806, minimum ratio: 0.76953125\n",
      "Epoch [302], val_loss: 35.3391\n",
      "gradient norm: 32.70118188858032, minimum ratio: 0.7734375\n",
      "Epoch [303], val_loss: 35.6431\n",
      "gradient norm: 32.873958230018616, minimum ratio: 0.75390625\n",
      "Epoch [304], val_loss: 35.9492\n",
      "gradient norm: 33.04788112640381, minimum ratio: 0.76953125\n",
      "Epoch [305], val_loss: 36.2573\n",
      "gradient norm: 33.22168457508087, minimum ratio: 0.765625\n",
      "Epoch [306], val_loss: 36.5674\n",
      "gradient norm: 33.397424817085266, minimum ratio: 0.7734375\n",
      "Epoch [307], val_loss: 36.8796\n",
      "gradient norm: 33.57203781604767, minimum ratio: 0.77734375\n",
      "Epoch [308], val_loss: 37.1939\n",
      "gradient norm: 33.74798285961151, minimum ratio: 0.78125\n",
      "Epoch [309], val_loss: 37.5103\n",
      "gradient norm: 33.92516505718231, minimum ratio: 0.765625\n",
      "Epoch [310], val_loss: 37.8287\n",
      "gradient norm: 34.102701902389526, minimum ratio: 0.76953125\n",
      "Epoch [311], val_loss: 38.1492\n",
      "gradient norm: 34.282018065452576, minimum ratio: 0.76953125\n",
      "Epoch [312], val_loss: 38.4720\n",
      "gradient norm: 34.46059763431549, minimum ratio: 0.76953125\n",
      "Epoch [313], val_loss: 38.7968\n",
      "gradient norm: 34.6399245262146, minimum ratio: 0.76953125\n",
      "Epoch [314], val_loss: 39.1238\n",
      "gradient norm: 34.819557785987854, minimum ratio: 0.796875\n",
      "Epoch [315], val_loss: 39.4529\n",
      "gradient norm: 35.0008202791214, minimum ratio: 0.765625\n",
      "Epoch [316], val_loss: 39.7842\n",
      "gradient norm: 35.18311631679535, minimum ratio: 0.78125\n",
      "Epoch [317], val_loss: 40.1177\n",
      "gradient norm: 35.36609375476837, minimum ratio: 0.75390625\n",
      "Epoch [318], val_loss: 40.4534\n",
      "gradient norm: 35.54897451400757, minimum ratio: 0.7734375\n",
      "Epoch [319], val_loss: 40.7913\n",
      "gradient norm: 35.732123017311096, minimum ratio: 0.765625\n",
      "Epoch [320], val_loss: 41.1313\n",
      "gradient norm: 35.91589426994324, minimum ratio: 0.77734375\n",
      "Epoch [321], val_loss: 41.4736\n",
      "gradient norm: 36.10158669948578, minimum ratio: 0.78125\n",
      "Epoch [322], val_loss: 41.8181\n",
      "gradient norm: 36.28791308403015, minimum ratio: 0.7734375\n",
      "Epoch [323], val_loss: 42.1649\n",
      "gradient norm: 36.47458612918854, minimum ratio: 0.76953125\n",
      "Epoch [324], val_loss: 42.5139\n",
      "gradient norm: 36.66194140911102, minimum ratio: 0.765625\n",
      "Epoch [325], val_loss: 42.8652\n",
      "gradient norm: 36.84932553768158, minimum ratio: 0.7734375\n",
      "Epoch [326], val_loss: 43.2187\n",
      "gradient norm: 37.03783309459686, minimum ratio: 0.796875\n",
      "Epoch [327], val_loss: 43.5745\n",
      "gradient norm: 37.22708761692047, minimum ratio: 0.79296875\n",
      "Epoch [328], val_loss: 43.9326\n",
      "gradient norm: 37.41729283332825, minimum ratio: 0.73828125\n",
      "Epoch [329], val_loss: 44.2931\n",
      "gradient norm: 37.607577323913574, minimum ratio: 0.77734375\n",
      "Epoch [330], val_loss: 44.6559\n",
      "gradient norm: 37.799235343933105, minimum ratio: 0.79296875\n",
      "Epoch [331], val_loss: 45.0210\n",
      "gradient norm: 37.99120259284973, minimum ratio: 0.76953125\n",
      "Epoch [332], val_loss: 45.3884\n",
      "gradient norm: 38.184134006500244, minimum ratio: 0.7734375\n",
      "Epoch [333], val_loss: 45.7582\n",
      "gradient norm: 38.3781476020813, minimum ratio: 0.7734375\n",
      "Epoch [334], val_loss: 46.1303\n",
      "gradient norm: 38.571516275405884, minimum ratio: 0.796875\n",
      "Epoch [335], val_loss: 46.5048\n",
      "gradient norm: 38.76641011238098, minimum ratio: 0.77734375\n",
      "Epoch [336], val_loss: 46.8818\n",
      "gradient norm: 38.962186098098755, minimum ratio: 0.77734375\n",
      "Epoch [337], val_loss: 47.2611\n",
      "gradient norm: 39.158870220184326, minimum ratio: 0.765625\n",
      "Epoch [338], val_loss: 47.6428\n",
      "gradient norm: 39.35637187957764, minimum ratio: 0.76953125\n",
      "Epoch [339], val_loss: 48.0269\n",
      "gradient norm: 39.55440902709961, minimum ratio: 0.7890625\n",
      "Epoch [340], val_loss: 48.4135\n",
      "gradient norm: 39.75271511077881, minimum ratio: 0.77734375\n",
      "Epoch [341], val_loss: 48.8025\n",
      "gradient norm: 39.9521381855011, minimum ratio: 0.75\n",
      "Epoch [342], val_loss: 49.1940\n",
      "gradient norm: 40.15196967124939, minimum ratio: 0.7734375\n",
      "Epoch [343], val_loss: 49.5879\n",
      "gradient norm: 40.35286259651184, minimum ratio: 0.78125\n",
      "Epoch [344], val_loss: 49.9842\n",
      "gradient norm: 40.55444049835205, minimum ratio: 0.76953125\n",
      "Epoch [345], val_loss: 50.3831\n",
      "gradient norm: 40.75673460960388, minimum ratio: 0.7578125\n",
      "Epoch [346], val_loss: 50.7844\n",
      "gradient norm: 40.95958471298218, minimum ratio: 0.78125\n",
      "Epoch [347], val_loss: 51.1882\n",
      "gradient norm: 41.161980628967285, minimum ratio: 0.76171875\n",
      "Epoch [348], val_loss: 51.5946\n",
      "gradient norm: 41.36566424369812, minimum ratio: 0.76953125\n",
      "Epoch [349], val_loss: 52.0035\n",
      "gradient norm: 41.57059335708618, minimum ratio: 0.7890625\n",
      "Epoch [350], val_loss: 52.4149\n",
      "gradient norm: 41.77624201774597, minimum ratio: 0.76171875\n",
      "Epoch [351], val_loss: 52.8288\n",
      "gradient norm: 41.98330354690552, minimum ratio: 0.75390625\n",
      "Epoch [352], val_loss: 53.2454\n",
      "gradient norm: 42.19046139717102, minimum ratio: 0.78125\n",
      "Epoch [353], val_loss: 53.6645\n",
      "gradient norm: 42.398000717163086, minimum ratio: 0.7734375\n",
      "Epoch [354], val_loss: 54.0863\n",
      "gradient norm: 42.60657238960266, minimum ratio: 0.78515625\n",
      "Epoch [355], val_loss: 54.5108\n",
      "gradient norm: 42.81582307815552, minimum ratio: 0.8046875\n",
      "Epoch [356], val_loss: 54.9378\n",
      "gradient norm: 43.026315689086914, minimum ratio: 0.80078125\n",
      "Epoch [357], val_loss: 55.3675\n",
      "gradient norm: 43.23787975311279, minimum ratio: 0.7890625\n",
      "Epoch [358], val_loss: 55.7999\n",
      "gradient norm: 43.449955701828, minimum ratio: 0.80078125\n",
      "Epoch [359], val_loss: 56.2349\n",
      "gradient norm: 43.66207504272461, minimum ratio: 0.76171875\n",
      "Epoch [360], val_loss: 56.6727\n",
      "gradient norm: 43.875754833221436, minimum ratio: 0.76953125\n",
      "Epoch [361], val_loss: 57.1131\n",
      "gradient norm: 44.0890474319458, minimum ratio: 0.78125\n",
      "Epoch [362], val_loss: 57.5561\n",
      "gradient norm: 44.30408501625061, minimum ratio: 0.7734375\n",
      "Epoch [363], val_loss: 58.0018\n",
      "gradient norm: 44.51921486854553, minimum ratio: 0.7890625\n",
      "Epoch [364], val_loss: 58.4503\n",
      "gradient norm: 44.73639941215515, minimum ratio: 0.78125\n",
      "Epoch [365], val_loss: 58.9016\n",
      "gradient norm: 44.95401906967163, minimum ratio: 0.7890625\n",
      "Epoch [366], val_loss: 59.3556\n",
      "gradient norm: 45.17104434967041, minimum ratio: 0.7734375\n",
      "Epoch [367], val_loss: 59.8124\n",
      "gradient norm: 45.389514446258545, minimum ratio: 0.76953125\n",
      "Epoch [368], val_loss: 60.2720\n",
      "gradient norm: 45.608667850494385, minimum ratio: 0.765625\n",
      "Epoch [369], val_loss: 60.7343\n",
      "gradient norm: 45.82883167266846, minimum ratio: 0.76171875\n",
      "Epoch [370], val_loss: 61.1994\n",
      "gradient norm: 46.04927158355713, minimum ratio: 0.75\n",
      "Epoch [371], val_loss: 61.6674\n",
      "gradient norm: 46.271812915802, minimum ratio: 0.78515625\n",
      "Epoch [372], val_loss: 62.1381\n",
      "gradient norm: 46.49439787864685, minimum ratio: 0.7734375\n",
      "Epoch [373], val_loss: 62.6118\n",
      "gradient norm: 46.717085123062134, minimum ratio: 0.78125\n",
      "Epoch [374], val_loss: 63.0882\n",
      "gradient norm: 46.94135761260986, minimum ratio: 0.79296875\n",
      "Epoch [375], val_loss: 63.5676\n",
      "gradient norm: 47.16493463516235, minimum ratio: 0.7578125\n",
      "Epoch [376], val_loss: 64.0499\n",
      "gradient norm: 47.39039158821106, minimum ratio: 0.77734375\n",
      "Epoch [377], val_loss: 64.5350\n",
      "gradient norm: 47.617377042770386, minimum ratio: 0.7890625\n",
      "Epoch [378], val_loss: 65.0230\n",
      "gradient norm: 47.84530830383301, minimum ratio: 0.80078125\n",
      "Epoch [379], val_loss: 65.5139\n",
      "gradient norm: 48.073323249816895, minimum ratio: 0.7578125\n",
      "Epoch [380], val_loss: 66.0078\n",
      "gradient norm: 48.30136156082153, minimum ratio: 0.7734375\n",
      "Epoch [381], val_loss: 66.5047\n",
      "gradient norm: 48.530616760253906, minimum ratio: 0.78125\n",
      "Epoch [382], val_loss: 67.0045\n",
      "gradient norm: 48.75985527038574, minimum ratio: 0.78125\n",
      "Epoch [383], val_loss: 67.5074\n",
      "gradient norm: 48.992390871047974, minimum ratio: 0.75\n",
      "Epoch [384], val_loss: 68.0132\n",
      "gradient norm: 49.22423076629639, minimum ratio: 0.77734375\n",
      "Epoch [385], val_loss: 68.5221\n",
      "gradient norm: 49.45765280723572, minimum ratio: 0.7890625\n",
      "Epoch [386], val_loss: 69.0338\n",
      "gradient norm: 49.69104790687561, minimum ratio: 0.78125\n",
      "Epoch [387], val_loss: 69.5486\n",
      "gradient norm: 49.92478966712952, minimum ratio: 0.77734375\n",
      "Epoch [388], val_loss: 70.0665\n",
      "gradient norm: 50.15983819961548, minimum ratio: 0.7734375\n",
      "Epoch [389], val_loss: 70.5873\n",
      "gradient norm: 50.39545130729675, minimum ratio: 0.76953125\n",
      "Epoch [390], val_loss: 71.1112\n",
      "gradient norm: 50.6322877407074, minimum ratio: 0.7578125\n",
      "Epoch [391], val_loss: 71.6382\n",
      "gradient norm: 50.86981010437012, minimum ratio: 0.7578125\n",
      "Epoch [392], val_loss: 72.1682\n",
      "gradient norm: 51.108434438705444, minimum ratio: 0.78515625\n",
      "Epoch [393], val_loss: 72.7013\n",
      "gradient norm: 51.346693992614746, minimum ratio: 0.79296875\n",
      "Epoch [394], val_loss: 73.2376\n",
      "gradient norm: 51.5870578289032, minimum ratio: 0.7890625\n",
      "Epoch [395], val_loss: 73.7770\n",
      "gradient norm: 51.82690382003784, minimum ratio: 0.7578125\n",
      "Epoch [396], val_loss: 74.3196\n",
      "gradient norm: 52.06849479675293, minimum ratio: 0.80859375\n",
      "Epoch [397], val_loss: 74.8653\n",
      "gradient norm: 52.310521602630615, minimum ratio: 0.78515625\n",
      "Epoch [398], val_loss: 75.4142\n",
      "gradient norm: 52.55374455451965, minimum ratio: 0.765625\n",
      "Epoch [399], val_loss: 75.9662\n",
      "gradient norm: 52.7981436252594, minimum ratio: 0.78125\n",
      "Epoch [400], val_loss: 76.5215\n",
      "gradient norm: 53.04361701011658, minimum ratio: 0.79296875\n",
      "Epoch [401], val_loss: 77.0800\n",
      "gradient norm: 53.289395809173584, minimum ratio: 0.78125\n",
      "Epoch [402], val_loss: 77.6418\n",
      "gradient norm: 53.53540110588074, minimum ratio: 0.7578125\n",
      "Epoch [403], val_loss: 78.2069\n",
      "gradient norm: 53.78147268295288, minimum ratio: 0.80078125\n",
      "Epoch [404], val_loss: 78.7752\n",
      "gradient norm: 54.029757022857666, minimum ratio: 0.76171875\n",
      "Epoch [405], val_loss: 79.3469\n",
      "gradient norm: 54.27883434295654, minimum ratio: 0.7734375\n",
      "Epoch [406], val_loss: 79.9219\n",
      "gradient norm: 54.52985692024231, minimum ratio: 0.76953125\n",
      "Epoch [407], val_loss: 80.5001\n",
      "gradient norm: 54.779369592666626, minimum ratio: 0.76953125\n",
      "Epoch [408], val_loss: 81.0817\n",
      "gradient norm: 55.03154897689819, minimum ratio: 0.76171875\n",
      "Epoch [409], val_loss: 81.6666\n",
      "gradient norm: 55.28399419784546, minimum ratio: 0.78515625\n",
      "Epoch [410], val_loss: 82.2549\n",
      "gradient norm: 55.53618621826172, minimum ratio: 0.78125\n",
      "Epoch [411], val_loss: 82.8465\n",
      "gradient norm: 55.79036593437195, minimum ratio: 0.7578125\n",
      "Epoch [412], val_loss: 83.4417\n",
      "gradient norm: 56.04651641845703, minimum ratio: 0.78515625\n",
      "Epoch [413], val_loss: 84.0401\n",
      "gradient norm: 56.30184721946716, minimum ratio: 0.7890625\n",
      "Epoch [414], val_loss: 84.6421\n",
      "gradient norm: 56.55933618545532, minimum ratio: 0.78125\n",
      "Epoch [415], val_loss: 85.2475\n",
      "gradient norm: 56.81707239151001, minimum ratio: 0.765625\n",
      "Epoch [416], val_loss: 85.8562\n",
      "gradient norm: 57.07542324066162, minimum ratio: 0.7578125\n",
      "Epoch [417], val_loss: 86.4684\n",
      "gradient norm: 57.33316421508789, minimum ratio: 0.78515625\n",
      "Epoch [418], val_loss: 87.0839\n",
      "gradient norm: 57.59260392189026, minimum ratio: 0.765625\n",
      "Epoch [419], val_loss: 87.7030\n",
      "gradient norm: 57.85382914543152, minimum ratio: 0.76171875\n",
      "Epoch [420], val_loss: 88.3255\n",
      "gradient norm: 58.11520075798035, minimum ratio: 0.76953125\n",
      "Epoch [421], val_loss: 88.9515\n",
      "gradient norm: 58.37825870513916, minimum ratio: 0.765625\n",
      "Epoch [422], val_loss: 89.5810\n",
      "gradient norm: 58.6416540145874, minimum ratio: 0.7734375\n",
      "Epoch [423], val_loss: 90.2140\n",
      "gradient norm: 58.90585470199585, minimum ratio: 0.77734375\n",
      "Epoch [424], val_loss: 90.8505\n",
      "gradient norm: 59.17177963256836, minimum ratio: 0.7734375\n",
      "Epoch [425], val_loss: 91.4906\n",
      "gradient norm: 59.43797039985657, minimum ratio: 0.7734375\n",
      "Epoch [426], val_loss: 92.1343\n",
      "gradient norm: 59.70516633987427, minimum ratio: 0.78515625\n",
      "Epoch [427], val_loss: 92.7815\n",
      "gradient norm: 59.97235608100891, minimum ratio: 0.7734375\n",
      "Epoch [428], val_loss: 93.4324\n",
      "gradient norm: 60.24071288108826, minimum ratio: 0.77734375\n",
      "Epoch [429], val_loss: 94.0871\n",
      "gradient norm: 60.51023817062378, minimum ratio: 0.80078125\n",
      "Epoch [430], val_loss: 94.7453\n",
      "gradient norm: 60.78070259094238, minimum ratio: 0.8125\n",
      "Epoch [431], val_loss: 95.4072\n",
      "gradient norm: 61.05229091644287, minimum ratio: 0.7890625\n",
      "Epoch [432], val_loss: 96.0728\n",
      "gradient norm: 61.323495388031006, minimum ratio: 0.78515625\n",
      "Epoch [433], val_loss: 96.7421\n",
      "gradient norm: 61.59647822380066, minimum ratio: 0.75390625\n",
      "Epoch [434], val_loss: 97.4150\n",
      "gradient norm: 61.87032890319824, minimum ratio: 0.7734375\n",
      "Epoch [435], val_loss: 98.0916\n",
      "gradient norm: 62.14510631561279, minimum ratio: 0.78515625\n",
      "Epoch [436], val_loss: 98.7720\n",
      "gradient norm: 62.42236089706421, minimum ratio: 0.765625\n",
      "Epoch [437], val_loss: 99.4560\n",
      "gradient norm: 62.69838094711304, minimum ratio: 0.76953125\n",
      "Epoch [438], val_loss: 100.1436\n",
      "gradient norm: 62.97634553909302, minimum ratio: 0.7734375\n",
      "Epoch [439], val_loss: 100.8351\n",
      "gradient norm: 63.25493907928467, minimum ratio: 0.76953125\n",
      "Epoch [440], val_loss: 101.5303\n",
      "gradient norm: 63.53308916091919, minimum ratio: 0.7734375\n",
      "Epoch [441], val_loss: 102.2293\n",
      "gradient norm: 63.81291675567627, minimum ratio: 0.7421875\n",
      "Epoch [442], val_loss: 102.9321\n",
      "gradient norm: 64.09394979476929, minimum ratio: 0.77734375\n",
      "Epoch [443], val_loss: 103.6387\n",
      "gradient norm: 64.37492084503174, minimum ratio: 0.7890625\n",
      "Epoch [444], val_loss: 104.3493\n",
      "gradient norm: 64.65894174575806, minimum ratio: 0.7734375\n",
      "Epoch [445], val_loss: 105.0637\n",
      "gradient norm: 64.94199132919312, minimum ratio: 0.7890625\n",
      "Epoch [446], val_loss: 105.7819\n",
      "gradient norm: 65.22618246078491, minimum ratio: 0.80078125\n",
      "Epoch [447], val_loss: 106.5040\n",
      "gradient norm: 65.51132774353027, minimum ratio: 0.78125\n",
      "Epoch [448], val_loss: 107.2300\n",
      "gradient norm: 65.79834318161011, minimum ratio: 0.7578125\n",
      "Epoch [449], val_loss: 107.9600\n",
      "gradient norm: 66.08576917648315, minimum ratio: 0.80078125\n",
      "Epoch [450], val_loss: 108.6939\n",
      "gradient norm: 66.37413454055786, minimum ratio: 0.78515625\n",
      "Epoch [451], val_loss: 109.4319\n",
      "gradient norm: 66.66282272338867, minimum ratio: 0.7578125\n",
      "Epoch [452], val_loss: 110.1738\n",
      "gradient norm: 66.95202589035034, minimum ratio: 0.76171875\n",
      "Epoch [453], val_loss: 110.9196\n",
      "gradient norm: 67.24282932281494, minimum ratio: 0.7734375\n",
      "Epoch [454], val_loss: 111.6693\n",
      "gradient norm: 67.53417444229126, minimum ratio: 0.77734375\n",
      "Epoch [455], val_loss: 112.4230\n",
      "gradient norm: 67.82731199264526, minimum ratio: 0.76171875\n",
      "Epoch [456], val_loss: 113.1808\n",
      "gradient norm: 68.12073516845703, minimum ratio: 0.78515625\n",
      "Epoch [457], val_loss: 113.9426\n",
      "gradient norm: 68.41520071029663, minimum ratio: 0.78515625\n",
      "Epoch [458], val_loss: 114.7085\n",
      "gradient norm: 68.7103796005249, minimum ratio: 0.78515625\n",
      "Epoch [459], val_loss: 115.4785\n",
      "gradient norm: 69.00745391845703, minimum ratio: 0.78515625\n",
      "Epoch [460], val_loss: 116.2526\n",
      "gradient norm: 69.30441379547119, minimum ratio: 0.765625\n",
      "Epoch [461], val_loss: 117.0309\n",
      "gradient norm: 69.60330486297607, minimum ratio: 0.765625\n",
      "Epoch [462], val_loss: 117.8133\n",
      "gradient norm: 69.90092134475708, minimum ratio: 0.765625\n",
      "Epoch [463], val_loss: 118.5999\n",
      "gradient norm: 70.20157766342163, minimum ratio: 0.77734375\n",
      "Epoch [464], val_loss: 119.3906\n",
      "gradient norm: 70.50252056121826, minimum ratio: 0.7890625\n",
      "Epoch [465], val_loss: 120.1854\n",
      "gradient norm: 70.80384492874146, minimum ratio: 0.75390625\n",
      "Epoch [466], val_loss: 120.9843\n",
      "gradient norm: 71.10536432266235, minimum ratio: 0.7734375\n",
      "Epoch [467], val_loss: 121.7874\n",
      "gradient norm: 71.40991640090942, minimum ratio: 0.74609375\n",
      "Epoch [468], val_loss: 122.5947\n",
      "gradient norm: 71.7146668434143, minimum ratio: 0.78125\n",
      "Epoch [469], val_loss: 123.4065\n",
      "gradient norm: 72.02083444595337, minimum ratio: 0.7578125\n",
      "Epoch [470], val_loss: 124.2223\n",
      "gradient norm: 72.32764959335327, minimum ratio: 0.78515625\n",
      "Epoch [471], val_loss: 125.0425\n",
      "gradient norm: 72.6346788406372, minimum ratio: 0.76953125\n",
      "Epoch [472], val_loss: 125.8669\n",
      "gradient norm: 72.94344711303711, minimum ratio: 0.78515625\n",
      "Epoch [473], val_loss: 126.6955\n",
      "gradient norm: 73.2519941329956, minimum ratio: 0.796875\n",
      "Epoch [474], val_loss: 127.5284\n",
      "gradient norm: 73.56260871887207, minimum ratio: 0.76953125\n",
      "Epoch [475], val_loss: 128.3658\n",
      "gradient norm: 73.87483072280884, minimum ratio: 0.796875\n",
      "Epoch [476], val_loss: 129.2074\n",
      "gradient norm: 74.18696069717407, minimum ratio: 0.79296875\n",
      "Epoch [477], val_loss: 130.0534\n",
      "gradient norm: 74.50026273727417, minimum ratio: 0.77734375\n",
      "Epoch [478], val_loss: 130.9037\n",
      "gradient norm: 74.81362056732178, minimum ratio: 0.76171875\n",
      "Epoch [479], val_loss: 131.7584\n",
      "gradient norm: 75.12760305404663, minimum ratio: 0.76953125\n",
      "Epoch [480], val_loss: 132.6176\n",
      "gradient norm: 75.44452714920044, minimum ratio: 0.76953125\n",
      "Epoch [481], val_loss: 133.4811\n",
      "gradient norm: 75.76107835769653, minimum ratio: 0.76953125\n",
      "Epoch [482], val_loss: 134.3490\n",
      "gradient norm: 76.0779242515564, minimum ratio: 0.76171875\n",
      "Epoch [483], val_loss: 135.2213\n",
      "gradient norm: 76.3980302810669, minimum ratio: 0.78515625\n",
      "Epoch [484], val_loss: 136.0979\n",
      "gradient norm: 76.71782112121582, minimum ratio: 0.7734375\n",
      "Epoch [485], val_loss: 136.9791\n",
      "gradient norm: 77.03753852844238, minimum ratio: 0.77734375\n",
      "Epoch [486], val_loss: 137.8647\n",
      "gradient norm: 77.35970163345337, minimum ratio: 0.75390625\n",
      "Epoch [487], val_loss: 138.7547\n",
      "gradient norm: 77.68331861495972, minimum ratio: 0.7734375\n",
      "Epoch [488], val_loss: 139.6492\n",
      "gradient norm: 78.00522994995117, minimum ratio: 0.78125\n",
      "Epoch [489], val_loss: 140.5483\n",
      "gradient norm: 78.33030223846436, minimum ratio: 0.7578125\n",
      "Epoch [490], val_loss: 141.4520\n",
      "gradient norm: 78.65545988082886, minimum ratio: 0.77734375\n",
      "Epoch [491], val_loss: 142.3604\n",
      "gradient norm: 78.98190593719482, minimum ratio: 0.77734375\n",
      "Epoch [492], val_loss: 143.2734\n",
      "gradient norm: 79.3086838722229, minimum ratio: 0.77734375\n",
      "Epoch [493], val_loss: 144.1910\n",
      "gradient norm: 79.63442611694336, minimum ratio: 0.79296875\n",
      "Epoch [494], val_loss: 145.1134\n",
      "gradient norm: 79.9650206565857, minimum ratio: 0.7734375\n",
      "Epoch [495], val_loss: 146.0404\n",
      "gradient norm: 80.29489946365356, minimum ratio: 0.79296875\n",
      "Epoch [496], val_loss: 146.9720\n",
      "gradient norm: 80.62655448913574, minimum ratio: 0.77734375\n",
      "Epoch [497], val_loss: 147.9084\n",
      "gradient norm: 80.95906496047974, minimum ratio: 0.75\n",
      "Epoch [498], val_loss: 148.8494\n",
      "gradient norm: 81.29307174682617, minimum ratio: 0.8046875\n",
      "Epoch [499], val_loss: 149.7952\n",
      "gradient norm: 81.62827587127686, minimum ratio: 0.765625\n",
      "Epoch [500], val_loss: 150.7457\n",
      "gradient norm: 81.9636402130127, minimum ratio: 0.765625\n",
      "Epoch [501], val_loss: 151.7008\n",
      "gradient norm: 82.29958772659302, minimum ratio: 0.75390625\n",
      "Epoch [502], val_loss: 152.6606\n",
      "gradient norm: 82.63730192184448, minimum ratio: 0.76171875\n",
      "Epoch [503], val_loss: 153.6248\n",
      "gradient norm: 82.97468328475952, minimum ratio: 0.76953125\n",
      "Epoch [504], val_loss: 154.5938\n",
      "gradient norm: 83.31357097625732, minimum ratio: 0.7734375\n",
      "Epoch [505], val_loss: 155.5676\n",
      "gradient norm: 83.65399837493896, minimum ratio: 0.76953125\n",
      "Epoch [506], val_loss: 156.5462\n",
      "gradient norm: 83.99475812911987, minimum ratio: 0.76171875\n",
      "Epoch [507], val_loss: 157.5298\n",
      "gradient norm: 84.33842182159424, minimum ratio: 0.76171875\n",
      "Epoch [508], val_loss: 158.5183\n",
      "gradient norm: 84.67926788330078, minimum ratio: 0.765625\n",
      "Epoch [509], val_loss: 159.5116\n",
      "gradient norm: 85.02186822891235, minimum ratio: 0.76953125\n",
      "Epoch [510], val_loss: 160.5097\n",
      "gradient norm: 85.36766147613525, minimum ratio: 0.78515625\n",
      "Epoch [511], val_loss: 161.5128\n",
      "gradient norm: 85.71307849884033, minimum ratio: 0.78125\n",
      "Epoch [512], val_loss: 162.5207\n",
      "gradient norm: 86.06078672409058, minimum ratio: 0.79296875\n",
      "Epoch [513], val_loss: 163.5337\n",
      "gradient norm: 86.40943384170532, minimum ratio: 0.7890625\n",
      "Epoch [514], val_loss: 164.5514\n",
      "gradient norm: 86.75737524032593, minimum ratio: 0.78125\n",
      "Epoch [515], val_loss: 165.5742\n",
      "gradient norm: 87.107168674469, minimum ratio: 0.80078125\n",
      "Epoch [516], val_loss: 166.6019\n",
      "gradient norm: 87.45681381225586, minimum ratio: 0.80078125\n",
      "Epoch [517], val_loss: 167.6345\n",
      "gradient norm: 87.80831050872803, minimum ratio: 0.79296875\n",
      "Epoch [518], val_loss: 168.6720\n",
      "gradient norm: 88.1602144241333, minimum ratio: 0.7734375\n",
      "Epoch [519], val_loss: 169.7143\n",
      "gradient norm: 88.51313495635986, minimum ratio: 0.79296875\n",
      "Epoch [520], val_loss: 170.7619\n",
      "gradient norm: 88.86808061599731, minimum ratio: 0.7890625\n",
      "Epoch [521], val_loss: 171.8145\n",
      "gradient norm: 89.2248969078064, minimum ratio: 0.75390625\n",
      "Epoch [522], val_loss: 172.8723\n",
      "gradient norm: 89.58228015899658, minimum ratio: 0.76171875\n",
      "Epoch [523], val_loss: 173.9351\n",
      "gradient norm: 89.93952655792236, minimum ratio: 0.7734375\n",
      "Epoch [524], val_loss: 175.0031\n",
      "gradient norm: 90.29809999465942, minimum ratio: 0.75\n",
      "Epoch [525], val_loss: 176.0761\n",
      "gradient norm: 90.65683460235596, minimum ratio: 0.7734375\n",
      "Epoch [526], val_loss: 177.1542\n",
      "gradient norm: 91.01590251922607, minimum ratio: 0.7578125\n",
      "Epoch [527], val_loss: 178.2375\n",
      "gradient norm: 91.37896299362183, minimum ratio: 0.75390625\n",
      "Epoch [528], val_loss: 179.3260\n",
      "gradient norm: 91.74220514297485, minimum ratio: 0.76171875\n",
      "Epoch [529], val_loss: 180.4197\n",
      "gradient norm: 92.10725545883179, minimum ratio: 0.7890625\n",
      "Epoch [530], val_loss: 181.5185\n",
      "gradient norm: 92.47209310531616, minimum ratio: 0.77734375\n",
      "Epoch [531], val_loss: 182.6225\n",
      "gradient norm: 92.83649349212646, minimum ratio: 0.74609375\n",
      "Epoch [532], val_loss: 183.7319\n",
      "gradient norm: 93.20427751541138, minimum ratio: 0.7734375\n",
      "Epoch [533], val_loss: 184.8468\n",
      "gradient norm: 93.5720067024231, minimum ratio: 0.76171875\n",
      "Epoch [534], val_loss: 185.9668\n",
      "gradient norm: 93.93994426727295, minimum ratio: 0.78125\n",
      "Epoch [535], val_loss: 187.0922\n",
      "gradient norm: 94.3096399307251, minimum ratio: 0.7734375\n",
      "Epoch [536], val_loss: 188.2228\n",
      "gradient norm: 94.68195152282715, minimum ratio: 0.77734375\n",
      "Epoch [537], val_loss: 189.3586\n",
      "gradient norm: 95.0544786453247, minimum ratio: 0.77734375\n",
      "Epoch [538], val_loss: 190.4998\n",
      "gradient norm: 95.42618036270142, minimum ratio: 0.7734375\n",
      "Epoch [539], val_loss: 191.6462\n",
      "gradient norm: 95.79872512817383, minimum ratio: 0.796875\n",
      "Epoch [540], val_loss: 192.7982\n",
      "gradient norm: 96.1725378036499, minimum ratio: 0.76953125\n",
      "Epoch [541], val_loss: 193.9557\n",
      "gradient norm: 96.54936599731445, minimum ratio: 0.7890625\n",
      "Epoch [542], val_loss: 195.1185\n",
      "gradient norm: 96.92601680755615, minimum ratio: 0.76953125\n",
      "Epoch [543], val_loss: 196.2866\n",
      "gradient norm: 97.30422067642212, minimum ratio: 0.78125\n",
      "Epoch [544], val_loss: 197.4599\n",
      "gradient norm: 97.68297386169434, minimum ratio: 0.77734375\n",
      "Epoch [545], val_loss: 198.6387\n",
      "gradient norm: 98.06190061569214, minimum ratio: 0.76953125\n",
      "Epoch [546], val_loss: 199.8230\n",
      "gradient norm: 98.44335746765137, minimum ratio: 0.78515625\n",
      "Epoch [547], val_loss: 201.0128\n",
      "gradient norm: 98.82479953765869, minimum ratio: 0.81640625\n",
      "Epoch [548], val_loss: 202.2080\n",
      "gradient norm: 99.20869159698486, minimum ratio: 0.78125\n",
      "Epoch [549], val_loss: 203.4088\n",
      "gradient norm: 99.59459352493286, minimum ratio: 0.78515625\n",
      "Epoch [550], val_loss: 204.6151\n",
      "gradient norm: 99.98084354400635, minimum ratio: 0.76171875\n",
      "Epoch [551], val_loss: 205.8271\n",
      "gradient norm: 100.36773490905762, minimum ratio: 0.7734375\n",
      "Epoch [552], val_loss: 207.0446\n",
      "gradient norm: 100.75410842895508, minimum ratio: 0.77734375\n",
      "Epoch [553], val_loss: 208.2677\n",
      "gradient norm: 101.14337396621704, minimum ratio: 0.77734375\n",
      "Epoch [554], val_loss: 209.4963\n",
      "gradient norm: 101.53321695327759, minimum ratio: 0.7578125\n",
      "Epoch [555], val_loss: 210.7306\n",
      "gradient norm: 101.92359924316406, minimum ratio: 0.7734375\n",
      "Epoch [556], val_loss: 211.9705\n",
      "gradient norm: 102.31142807006836, minimum ratio: 0.80078125\n",
      "Epoch [557], val_loss: 213.2161\n",
      "gradient norm: 102.70513725280762, minimum ratio: 0.7578125\n",
      "Epoch [558], val_loss: 214.4674\n",
      "gradient norm: 103.10001182556152, minimum ratio: 0.7890625\n",
      "Epoch [559], val_loss: 215.7245\n",
      "gradient norm: 103.49290084838867, minimum ratio: 0.8125\n",
      "Epoch [560], val_loss: 216.9873\n",
      "gradient norm: 103.88801527023315, minimum ratio: 0.7890625\n",
      "Epoch [561], val_loss: 218.2560\n",
      "gradient norm: 104.28399515151978, minimum ratio: 0.79296875\n",
      "Epoch [562], val_loss: 219.5304\n",
      "gradient norm: 104.6810998916626, minimum ratio: 0.78515625\n",
      "Epoch [563], val_loss: 220.8106\n",
      "gradient norm: 105.08060073852539, minimum ratio: 0.76953125\n",
      "Epoch [564], val_loss: 222.0965\n",
      "gradient norm: 105.4807538986206, minimum ratio: 0.78125\n",
      "Epoch [565], val_loss: 223.3881\n",
      "gradient norm: 105.88211250305176, minimum ratio: 0.7734375\n",
      "Epoch [566], val_loss: 224.6854\n",
      "gradient norm: 106.28402137756348, minimum ratio: 0.75390625\n",
      "Epoch [567], val_loss: 225.9885\n",
      "gradient norm: 106.68685531616211, minimum ratio: 0.796875\n",
      "Epoch [568], val_loss: 227.2973\n",
      "gradient norm: 107.09137439727783, minimum ratio: 0.7734375\n",
      "Epoch [569], val_loss: 228.6120\n",
      "gradient norm: 107.49501514434814, minimum ratio: 0.78125\n",
      "Epoch [570], val_loss: 229.9328\n",
      "gradient norm: 107.90210819244385, minimum ratio: 0.79296875\n",
      "Epoch [571], val_loss: 231.2595\n",
      "gradient norm: 108.30981922149658, minimum ratio: 0.79296875\n",
      "Epoch [572], val_loss: 232.5918\n",
      "gradient norm: 108.71810817718506, minimum ratio: 0.78125\n",
      "Epoch [573], val_loss: 233.9301\n",
      "gradient norm: 109.12621974945068, minimum ratio: 0.78125\n",
      "Epoch [574], val_loss: 235.2744\n",
      "gradient norm: 109.5377721786499, minimum ratio: 0.76953125\n",
      "Epoch [575], val_loss: 236.6246\n",
      "gradient norm: 109.94795608520508, minimum ratio: 0.78125\n",
      "Epoch [576], val_loss: 237.9810\n",
      "gradient norm: 110.36198616027832, minimum ratio: 0.79296875\n",
      "Epoch [577], val_loss: 239.3433\n",
      "gradient norm: 110.77515125274658, minimum ratio: 0.78125\n",
      "Epoch [578], val_loss: 240.7115\n",
      "gradient norm: 111.18991661071777, minimum ratio: 0.8125\n",
      "Epoch [579], val_loss: 242.0857\n",
      "gradient norm: 111.60450553894043, minimum ratio: 0.7734375\n",
      "Epoch [580], val_loss: 243.4661\n",
      "gradient norm: 112.02070236206055, minimum ratio: 0.79296875\n",
      "Epoch [581], val_loss: 244.8526\n",
      "gradient norm: 112.43900871276855, minimum ratio: 0.77734375\n",
      "Epoch [582], val_loss: 246.2454\n",
      "gradient norm: 112.85716915130615, minimum ratio: 0.76953125\n",
      "Epoch [583], val_loss: 247.6446\n",
      "gradient norm: 113.2749490737915, minimum ratio: 0.79296875\n",
      "Epoch [584], val_loss: 249.0500\n",
      "gradient norm: 113.6964168548584, minimum ratio: 0.7734375\n",
      "Epoch [585], val_loss: 250.4615\n",
      "gradient norm: 114.12178039550781, minimum ratio: 0.76953125\n",
      "Epoch [586], val_loss: 251.8791\n",
      "gradient norm: 114.54378890991211, minimum ratio: 0.77734375\n",
      "Epoch [587], val_loss: 253.3028\n",
      "gradient norm: 114.96851253509521, minimum ratio: 0.78125\n",
      "Epoch [588], val_loss: 254.7326\n",
      "gradient norm: 115.39417934417725, minimum ratio: 0.77734375\n",
      "Epoch [589], val_loss: 256.1686\n",
      "gradient norm: 115.8220272064209, minimum ratio: 0.79296875\n",
      "Epoch [590], val_loss: 257.6106\n",
      "gradient norm: 116.24864101409912, minimum ratio: 0.75390625\n",
      "Epoch [591], val_loss: 259.0589\n",
      "gradient norm: 116.67652225494385, minimum ratio: 0.78515625\n",
      "Epoch [592], val_loss: 260.5134\n",
      "gradient norm: 117.10581874847412, minimum ratio: 0.80078125\n",
      "Epoch [593], val_loss: 261.9742\n",
      "gradient norm: 117.53582382202148, minimum ratio: 0.765625\n",
      "Epoch [594], val_loss: 263.4414\n",
      "gradient norm: 117.96873664855957, minimum ratio: 0.77734375\n",
      "Epoch [595], val_loss: 264.9149\n",
      "gradient norm: 118.40078449249268, minimum ratio: 0.79296875\n",
      "Epoch [596], val_loss: 266.3945\n",
      "gradient norm: 118.83394622802734, minimum ratio: 0.76953125\n",
      "Epoch [597], val_loss: 267.8805\n",
      "gradient norm: 119.26921558380127, minimum ratio: 0.765625\n",
      "Epoch [598], val_loss: 269.3731\n",
      "gradient norm: 119.70642566680908, minimum ratio: 0.7734375\n",
      "Epoch [599], val_loss: 270.8720\n",
      "gradient norm: 120.14404392242432, minimum ratio: 0.765625\n",
      "Epoch [600], val_loss: 272.3773\n",
      "gradient norm: 120.5799560546875, minimum ratio: 0.7734375\n",
      "Epoch [601], val_loss: 273.8889\n",
      "gradient norm: 121.01974582672119, minimum ratio: 0.76953125\n",
      "Epoch [602], val_loss: 275.4068\n",
      "gradient norm: 121.45957660675049, minimum ratio: 0.765625\n",
      "Epoch [603], val_loss: 276.9311\n",
      "gradient norm: 121.90208911895752, minimum ratio: 0.75390625\n",
      "Epoch [604], val_loss: 278.4619\n",
      "gradient norm: 122.34523487091064, minimum ratio: 0.79296875\n",
      "Epoch [605], val_loss: 279.9990\n",
      "gradient norm: 122.78884696960449, minimum ratio: 0.75390625\n",
      "Epoch [606], val_loss: 281.5427\n",
      "gradient norm: 123.23323440551758, minimum ratio: 0.76953125\n",
      "Epoch [607], val_loss: 283.0930\n",
      "gradient norm: 123.67729759216309, minimum ratio: 0.76171875\n",
      "Epoch [608], val_loss: 284.6497\n",
      "gradient norm: 124.1237096786499, minimum ratio: 0.76171875\n",
      "Epoch [609], val_loss: 286.2132\n",
      "gradient norm: 124.57234954833984, minimum ratio: 0.76953125\n",
      "Epoch [610], val_loss: 287.7831\n",
      "gradient norm: 125.02194404602051, minimum ratio: 0.75390625\n",
      "Epoch [611], val_loss: 289.3596\n",
      "gradient norm: 125.46933841705322, minimum ratio: 0.7734375\n",
      "Epoch [612], val_loss: 290.9426\n",
      "gradient norm: 125.92263984680176, minimum ratio: 0.79296875\n",
      "Epoch [613], val_loss: 292.5321\n",
      "gradient norm: 126.37499809265137, minimum ratio: 0.73828125\n",
      "Epoch [614], val_loss: 294.1283\n",
      "gradient norm: 126.82997989654541, minimum ratio: 0.78125\n",
      "Epoch [615], val_loss: 295.7310\n",
      "gradient norm: 127.28324604034424, minimum ratio: 0.7890625\n",
      "Epoch [616], val_loss: 297.3403\n",
      "gradient norm: 127.73885345458984, minimum ratio: 0.765625\n",
      "Epoch [617], val_loss: 298.9566\n",
      "gradient norm: 128.19572162628174, minimum ratio: 0.76171875\n",
      "Epoch [618], val_loss: 300.5797\n",
      "gradient norm: 128.65277862548828, minimum ratio: 0.76953125\n",
      "Epoch [619], val_loss: 302.2095\n",
      "gradient norm: 129.11099529266357, minimum ratio: 0.78125\n",
      "Epoch [620], val_loss: 303.8458\n",
      "gradient norm: 129.57083702087402, minimum ratio: 0.76953125\n",
      "Epoch [621], val_loss: 305.4889\n",
      "gradient norm: 130.03199100494385, minimum ratio: 0.796875\n",
      "Epoch [622], val_loss: 307.1388\n",
      "gradient norm: 130.49621200561523, minimum ratio: 0.75\n",
      "Epoch [623], val_loss: 308.7955\n",
      "gradient norm: 130.9570655822754, minimum ratio: 0.78125\n",
      "Epoch [624], val_loss: 310.4593\n",
      "gradient norm: 131.42246532440186, minimum ratio: 0.765625\n",
      "Epoch [625], val_loss: 312.1300\n",
      "gradient norm: 131.88705348968506, minimum ratio: 0.765625\n",
      "Epoch [626], val_loss: 313.8074\n",
      "gradient norm: 132.3541660308838, minimum ratio: 0.7578125\n",
      "Epoch [627], val_loss: 315.4918\n",
      "gradient norm: 132.82104682922363, minimum ratio: 0.78125\n",
      "Epoch [628], val_loss: 317.1831\n",
      "gradient norm: 133.28904914855957, minimum ratio: 0.77734375\n",
      "Epoch [629], val_loss: 318.8814\n",
      "gradient norm: 133.7587251663208, minimum ratio: 0.7734375\n",
      "Epoch [630], val_loss: 320.5866\n",
      "gradient norm: 134.2297945022583, minimum ratio: 0.75\n",
      "Epoch [631], val_loss: 322.2987\n",
      "gradient norm: 134.70373916625977, minimum ratio: 0.7578125\n",
      "Epoch [632], val_loss: 324.0180\n",
      "gradient norm: 135.1750316619873, minimum ratio: 0.765625\n",
      "Epoch [633], val_loss: 325.7444\n",
      "gradient norm: 135.6498737335205, minimum ratio: 0.78515625\n",
      "Epoch [634], val_loss: 327.4778\n",
      "gradient norm: 136.12538051605225, minimum ratio: 0.7421875\n",
      "Epoch [635], val_loss: 329.2181\n",
      "gradient norm: 136.60237312316895, minimum ratio: 0.765625\n",
      "Epoch [636], val_loss: 330.9656\n",
      "gradient norm: 137.08086585998535, minimum ratio: 0.78125\n",
      "Epoch [637], val_loss: 332.7203\n",
      "gradient norm: 137.56199741363525, minimum ratio: 0.7734375\n",
      "Epoch [638], val_loss: 334.4820\n",
      "gradient norm: 138.03937339782715, minimum ratio: 0.7734375\n",
      "Epoch [639], val_loss: 336.2506\n",
      "gradient norm: 138.52101039886475, minimum ratio: 0.7734375\n",
      "Epoch [640], val_loss: 338.0266\n",
      "gradient norm: 139.00224685668945, minimum ratio: 0.734375\n",
      "Epoch [641], val_loss: 339.8096\n",
      "gradient norm: 139.48856830596924, minimum ratio: 0.77734375\n",
      "Epoch [642], val_loss: 341.5998\n",
      "gradient norm: 139.97405529022217, minimum ratio: 0.77734375\n",
      "Epoch [643], val_loss: 343.3971\n",
      "gradient norm: 140.4592456817627, minimum ratio: 0.78515625\n",
      "Epoch [644], val_loss: 345.2016\n",
      "gradient norm: 140.94734001159668, minimum ratio: 0.7734375\n",
      "Epoch [645], val_loss: 347.0138\n",
      "gradient norm: 141.43645477294922, minimum ratio: 0.76953125\n",
      "Epoch [646], val_loss: 348.8334\n",
      "gradient norm: 141.9244089126587, minimum ratio: 0.7734375\n",
      "Epoch [647], val_loss: 350.6601\n",
      "gradient norm: 142.41310024261475, minimum ratio: 0.765625\n",
      "Epoch [648], val_loss: 352.4942\n",
      "gradient norm: 142.89972972869873, minimum ratio: 0.8125\n",
      "Epoch [649], val_loss: 354.3357\n",
      "gradient norm: 143.3933343887329, minimum ratio: 0.76171875\n",
      "Epoch [650], val_loss: 356.1845\n",
      "gradient norm: 143.88767337799072, minimum ratio: 0.796875\n",
      "Epoch [651], val_loss: 358.0406\n",
      "gradient norm: 144.38434982299805, minimum ratio: 0.765625\n",
      "Epoch [652], val_loss: 359.9043\n",
      "gradient norm: 144.87919521331787, minimum ratio: 0.78515625\n",
      "Epoch [653], val_loss: 361.7754\n",
      "gradient norm: 145.3778419494629, minimum ratio: 0.76953125\n",
      "Epoch [654], val_loss: 363.6538\n",
      "gradient norm: 145.87808513641357, minimum ratio: 0.7734375\n",
      "Epoch [655], val_loss: 365.5399\n",
      "gradient norm: 146.37808799743652, minimum ratio: 0.75390625\n",
      "Epoch [656], val_loss: 367.4329\n",
      "gradient norm: 146.88084888458252, minimum ratio: 0.765625\n",
      "Epoch [657], val_loss: 369.3336\n",
      "gradient norm: 147.38383197784424, minimum ratio: 0.78125\n",
      "Epoch [658], val_loss: 371.2416\n",
      "gradient norm: 147.88356494903564, minimum ratio: 0.7734375\n",
      "Epoch [659], val_loss: 373.1573\n",
      "gradient norm: 148.38894844055176, minimum ratio: 0.796875\n",
      "Epoch [660], val_loss: 375.0805\n",
      "gradient norm: 148.89534950256348, minimum ratio: 0.74609375\n",
      "Epoch [661], val_loss: 377.0108\n",
      "gradient norm: 149.40384674072266, minimum ratio: 0.77734375\n",
      "Epoch [662], val_loss: 378.9487\n",
      "gradient norm: 149.90803718566895, minimum ratio: 0.77734375\n",
      "Epoch [663], val_loss: 380.8943\n",
      "gradient norm: 150.41395568847656, minimum ratio: 0.76953125\n",
      "Epoch [664], val_loss: 382.8470\n",
      "gradient norm: 150.9248924255371, minimum ratio: 0.78125\n",
      "Epoch [665], val_loss: 384.8075\n",
      "gradient norm: 151.43810939788818, minimum ratio: 0.7578125\n",
      "Epoch [666], val_loss: 386.7754\n",
      "gradient norm: 151.95223236083984, minimum ratio: 0.78515625\n",
      "Epoch [667], val_loss: 388.7510\n",
      "gradient norm: 152.46451377868652, minimum ratio: 0.7890625\n",
      "Epoch [668], val_loss: 390.7345\n",
      "gradient norm: 152.97788429260254, minimum ratio: 0.76953125\n",
      "Epoch [669], val_loss: 392.7257\n",
      "gradient norm: 153.49491596221924, minimum ratio: 0.7890625\n",
      "Epoch [670], val_loss: 394.7246\n",
      "gradient norm: 154.00998306274414, minimum ratio: 0.77734375\n",
      "Epoch [671], val_loss: 396.7309\n",
      "gradient norm: 154.530424118042, minimum ratio: 0.78125\n",
      "Epoch [672], val_loss: 398.7450\n",
      "gradient norm: 155.05012798309326, minimum ratio: 0.77734375\n",
      "Epoch [673], val_loss: 400.7670\n",
      "gradient norm: 155.5706615447998, minimum ratio: 0.78515625\n",
      "Epoch [674], val_loss: 402.7970\n",
      "gradient norm: 156.09056758880615, minimum ratio: 0.77734375\n",
      "Epoch [675], val_loss: 404.8347\n",
      "gradient norm: 156.61608219146729, minimum ratio: 0.78125\n",
      "Epoch [676], val_loss: 406.8806\n",
      "gradient norm: 157.13659477233887, minimum ratio: 0.765625\n",
      "Epoch [677], val_loss: 408.9345\n",
      "gradient norm: 157.6606683731079, minimum ratio: 0.7734375\n",
      "Epoch [678], val_loss: 410.9960\n",
      "gradient norm: 158.18243026733398, minimum ratio: 0.765625\n",
      "Epoch [679], val_loss: 413.0650\n",
      "gradient norm: 158.70897102355957, minimum ratio: 0.77734375\n",
      "Epoch [680], val_loss: 415.1420\n",
      "gradient norm: 159.2397165298462, minimum ratio: 0.7890625\n",
      "Epoch [681], val_loss: 417.2269\n",
      "gradient norm: 159.77209854125977, minimum ratio: 0.7734375\n",
      "Epoch [682], val_loss: 419.3196\n",
      "gradient norm: 160.30391883850098, minimum ratio: 0.76953125\n",
      "Epoch [683], val_loss: 421.4205\n",
      "gradient norm: 160.83562755584717, minimum ratio: 0.76953125\n",
      "Epoch [684], val_loss: 423.5292\n",
      "gradient norm: 161.36936473846436, minimum ratio: 0.7734375\n",
      "Epoch [685], val_loss: 425.6458\n",
      "gradient norm: 161.90105724334717, minimum ratio: 0.7890625\n",
      "Epoch [686], val_loss: 427.7703\n",
      "gradient norm: 162.43699264526367, minimum ratio: 0.78125\n",
      "Epoch [687], val_loss: 429.9028\n",
      "gradient norm: 162.9759702682495, minimum ratio: 0.7890625\n",
      "Epoch [688], val_loss: 432.0437\n",
      "gradient norm: 163.51337909698486, minimum ratio: 0.77734375\n",
      "Epoch [689], val_loss: 434.1927\n",
      "gradient norm: 164.05012702941895, minimum ratio: 0.7890625\n",
      "Epoch [690], val_loss: 436.3500\n",
      "gradient norm: 164.59241199493408, minimum ratio: 0.77734375\n",
      "Epoch [691], val_loss: 438.5154\n",
      "gradient norm: 165.13508319854736, minimum ratio: 0.7578125\n",
      "Epoch [692], val_loss: 440.6894\n",
      "gradient norm: 165.6781349182129, minimum ratio: 0.765625\n",
      "Epoch [693], val_loss: 442.8708\n",
      "gradient norm: 166.2239122390747, minimum ratio: 0.76171875\n",
      "Epoch [694], val_loss: 445.0601\n",
      "gradient norm: 166.76664638519287, minimum ratio: 0.7734375\n",
      "Epoch [695], val_loss: 447.2572\n",
      "gradient norm: 167.30982494354248, minimum ratio: 0.79296875\n",
      "Epoch [696], val_loss: 449.4628\n",
      "gradient norm: 167.85913372039795, minimum ratio: 0.765625\n",
      "Epoch [697], val_loss: 451.6767\n",
      "gradient norm: 168.40788459777832, minimum ratio: 0.78515625\n",
      "Epoch [698], val_loss: 453.8987\n",
      "gradient norm: 168.9559907913208, minimum ratio: 0.77734375\n",
      "Epoch [699], val_loss: 456.1288\n",
      "gradient norm: 169.50634860992432, minimum ratio: 0.77734375\n",
      "Epoch [700], val_loss: 458.3668\n",
      "gradient norm: 170.0580530166626, minimum ratio: 0.7578125\n",
      "Epoch [701], val_loss: 460.6135\n",
      "gradient norm: 170.612286567688, minimum ratio: 0.76953125\n",
      "Epoch [702], val_loss: 462.8685\n",
      "gradient norm: 171.1672601699829, minimum ratio: 0.78515625\n",
      "Epoch [703], val_loss: 465.1319\n",
      "gradient norm: 171.7224817276001, minimum ratio: 0.78515625\n",
      "Epoch [704], val_loss: 467.4034\n",
      "gradient norm: 172.27638912200928, minimum ratio: 0.7734375\n",
      "Epoch [705], val_loss: 469.6834\n",
      "gradient norm: 172.83104991912842, minimum ratio: 0.77734375\n",
      "Epoch [706], val_loss: 471.9721\n",
      "gradient norm: 173.3898515701294, minimum ratio: 0.79296875\n",
      "Epoch [707], val_loss: 474.2693\n",
      "gradient norm: 173.95052242279053, minimum ratio: 0.7890625\n",
      "Epoch [708], val_loss: 476.5746\n",
      "gradient norm: 174.51312065124512, minimum ratio: 0.80078125\n",
      "Epoch [709], val_loss: 478.8884\n",
      "gradient norm: 175.07687950134277, minimum ratio: 0.78125\n",
      "Epoch [710], val_loss: 481.2106\n",
      "gradient norm: 175.64063930511475, minimum ratio: 0.8125\n",
      "Epoch [711], val_loss: 483.5409\n",
      "gradient norm: 176.20371055603027, minimum ratio: 0.7734375\n",
      "Epoch [712], val_loss: 485.8801\n",
      "gradient norm: 176.7735834121704, minimum ratio: 0.78125\n",
      "Epoch [713], val_loss: 488.2282\n",
      "gradient norm: 177.34297466278076, minimum ratio: 0.7421875\n",
      "Epoch [714], val_loss: 490.5846\n",
      "gradient norm: 177.91159057617188, minimum ratio: 0.765625\n",
      "Epoch [715], val_loss: 492.9500\n",
      "gradient norm: 178.47869968414307, minimum ratio: 0.78125\n",
      "Epoch [716], val_loss: 495.3232\n",
      "gradient norm: 179.05125522613525, minimum ratio: 0.80078125\n",
      "Epoch [717], val_loss: 497.7050\n",
      "gradient norm: 179.61922550201416, minimum ratio: 0.77734375\n",
      "Epoch [718], val_loss: 500.0954\n",
      "gradient norm: 180.19378662109375, minimum ratio: 0.7734375\n",
      "Epoch [719], val_loss: 502.4944\n",
      "gradient norm: 180.76961612701416, minimum ratio: 0.76171875\n",
      "Epoch [720], val_loss: 504.9022\n",
      "gradient norm: 181.3447093963623, minimum ratio: 0.7890625\n",
      "Epoch [721], val_loss: 507.3190\n",
      "gradient norm: 181.9206418991089, minimum ratio: 0.76953125\n",
      "Epoch [722], val_loss: 509.7447\n",
      "gradient norm: 182.49659252166748, minimum ratio: 0.78125\n",
      "Epoch [723], val_loss: 512.1791\n",
      "gradient norm: 183.07938385009766, minimum ratio: 0.78125\n",
      "Epoch [724], val_loss: 514.6225\n",
      "gradient norm: 183.65770530700684, minimum ratio: 0.796875\n",
      "Epoch [725], val_loss: 517.0750\n",
      "gradient norm: 184.24008560180664, minimum ratio: 0.7578125\n",
      "Epoch [726], val_loss: 519.5363\n",
      "gradient norm: 184.81927299499512, minimum ratio: 0.7734375\n",
      "Epoch [727], val_loss: 522.0063\n",
      "gradient norm: 185.40628242492676, minimum ratio: 0.765625\n",
      "Epoch [728], val_loss: 524.4849\n",
      "gradient norm: 185.98951530456543, minimum ratio: 0.78125\n",
      "Epoch [729], val_loss: 526.9721\n",
      "gradient norm: 186.5799674987793, minimum ratio: 0.76953125\n",
      "Epoch [730], val_loss: 529.4682\n",
      "gradient norm: 187.16963386535645, minimum ratio: 0.79296875\n",
      "Epoch [731], val_loss: 531.9732\n",
      "gradient norm: 187.75705528259277, minimum ratio: 0.77734375\n",
      "Epoch [732], val_loss: 534.4871\n",
      "gradient norm: 188.34965133666992, minimum ratio: 0.76171875\n",
      "Epoch [733], val_loss: 537.0097\n",
      "gradient norm: 188.94366645812988, minimum ratio: 0.77734375\n",
      "Epoch [734], val_loss: 539.5413\n",
      "gradient norm: 189.5329532623291, minimum ratio: 0.765625\n",
      "Epoch [735], val_loss: 542.0818\n",
      "gradient norm: 190.12424278259277, minimum ratio: 0.76171875\n",
      "Epoch [736], val_loss: 544.6312\n",
      "gradient norm: 190.7155990600586, minimum ratio: 0.7421875\n",
      "Epoch [737], val_loss: 547.1896\n",
      "gradient norm: 191.31289291381836, minimum ratio: 0.74609375\n",
      "Epoch [738], val_loss: 549.7571\n",
      "gradient norm: 191.91073417663574, minimum ratio: 0.75390625\n",
      "Epoch [739], val_loss: 552.3336\n",
      "gradient norm: 192.51129150390625, minimum ratio: 0.77734375\n",
      "Epoch [740], val_loss: 554.9192\n",
      "gradient norm: 193.11260604858398, minimum ratio: 0.77734375\n",
      "Epoch [741], val_loss: 557.5141\n",
      "gradient norm: 193.71748733520508, minimum ratio: 0.7734375\n",
      "Epoch [742], val_loss: 560.1177\n",
      "gradient norm: 194.32325172424316, minimum ratio: 0.76171875\n",
      "Epoch [743], val_loss: 562.7294\n",
      "gradient norm: 194.92353057861328, minimum ratio: 0.7734375\n",
      "Epoch [744], val_loss: 565.3504\n",
      "gradient norm: 195.52915000915527, minimum ratio: 0.765625\n",
      "Epoch [745], val_loss: 567.9808\n",
      "gradient norm: 196.1353702545166, minimum ratio: 0.765625\n",
      "Epoch [746], val_loss: 570.6208\n",
      "gradient norm: 196.73957061767578, minimum ratio: 0.75390625\n",
      "Epoch [747], val_loss: 573.2692\n",
      "gradient norm: 197.34916496276855, minimum ratio: 0.78125\n",
      "Epoch [748], val_loss: 575.9268\n",
      "gradient norm: 197.95527267456055, minimum ratio: 0.77734375\n",
      "Epoch [749], val_loss: 578.5936\n",
      "gradient norm: 198.5626564025879, minimum ratio: 0.76953125\n",
      "Epoch [750], val_loss: 581.2699\n",
      "gradient norm: 199.1772632598877, minimum ratio: 0.73828125\n",
      "Epoch [751], val_loss: 583.9555\n",
      "gradient norm: 199.78933143615723, minimum ratio: 0.79296875\n",
      "Epoch [752], val_loss: 586.6505\n",
      "gradient norm: 200.40295791625977, minimum ratio: 0.79296875\n",
      "Epoch [753], val_loss: 589.3546\n",
      "gradient norm: 201.01840209960938, minimum ratio: 0.7734375\n",
      "Epoch [754], val_loss: 592.0679\n",
      "gradient norm: 201.63680458068848, minimum ratio: 0.77734375\n",
      "Epoch [755], val_loss: 594.7910\n",
      "gradient norm: 202.2532787322998, minimum ratio: 0.77734375\n",
      "Epoch [756], val_loss: 597.5233\n",
      "gradient norm: 202.8735294342041, minimum ratio: 0.79296875\n",
      "Epoch [757], val_loss: 600.2653\n",
      "gradient norm: 203.49342155456543, minimum ratio: 0.76171875\n",
      "Epoch [758], val_loss: 603.0167\n",
      "gradient norm: 204.1150951385498, minimum ratio: 0.7734375\n",
      "Epoch [759], val_loss: 605.7781\n",
      "gradient norm: 204.73562622070312, minimum ratio: 0.76171875\n",
      "Epoch [760], val_loss: 608.5488\n",
      "gradient norm: 205.35809516906738, minimum ratio: 0.78125\n",
      "Epoch [761], val_loss: 611.3290\n",
      "gradient norm: 205.9850730895996, minimum ratio: 0.77734375\n",
      "Epoch [762], val_loss: 614.1194\n",
      "gradient norm: 206.6116008758545, minimum ratio: 0.765625\n",
      "Epoch [763], val_loss: 616.9194\n",
      "gradient norm: 207.2398796081543, minimum ratio: 0.77734375\n",
      "Epoch [764], val_loss: 619.7291\n",
      "gradient norm: 207.8736057281494, minimum ratio: 0.73046875\n",
      "Epoch [765], val_loss: 622.5488\n",
      "gradient norm: 208.5038833618164, minimum ratio: 0.75\n",
      "Epoch [766], val_loss: 625.3775\n",
      "gradient norm: 209.13631057739258, minimum ratio: 0.78515625\n",
      "Epoch [767], val_loss: 628.2158\n",
      "gradient norm: 209.76690673828125, minimum ratio: 0.78125\n",
      "Epoch [768], val_loss: 631.0639\n",
      "gradient norm: 210.4011573791504, minimum ratio: 0.80078125\n",
      "Epoch [769], val_loss: 633.9218\n",
      "gradient norm: 211.03765678405762, minimum ratio: 0.76171875\n",
      "Epoch [770], val_loss: 636.7888\n",
      "gradient norm: 211.67300415039062, minimum ratio: 0.76953125\n",
      "Epoch [771], val_loss: 639.6653\n",
      "gradient norm: 212.31138801574707, minimum ratio: 0.7890625\n",
      "Epoch [772], val_loss: 642.5516\n",
      "gradient norm: 212.9522533416748, minimum ratio: 0.76953125\n",
      "Epoch [773], val_loss: 645.4478\n",
      "gradient norm: 213.59198188781738, minimum ratio: 0.77734375\n",
      "Epoch [774], val_loss: 648.3542\n",
      "gradient norm: 214.23265266418457, minimum ratio: 0.7421875\n",
      "Epoch [775], val_loss: 651.2706\n",
      "gradient norm: 214.87779235839844, minimum ratio: 0.79296875\n",
      "Epoch [776], val_loss: 654.1968\n",
      "gradient norm: 215.52118301391602, minimum ratio: 0.7578125\n",
      "Epoch [777], val_loss: 657.1323\n",
      "gradient norm: 216.17082023620605, minimum ratio: 0.7734375\n",
      "Epoch [778], val_loss: 660.0779\n",
      "gradient norm: 216.81902694702148, minimum ratio: 0.73828125\n",
      "Epoch [779], val_loss: 663.0332\n",
      "gradient norm: 217.46926498413086, minimum ratio: 0.765625\n",
      "Epoch [780], val_loss: 665.9984\n",
      "gradient norm: 218.11647415161133, minimum ratio: 0.765625\n",
      "Epoch [781], val_loss: 668.9734\n",
      "gradient norm: 218.77127647399902, minimum ratio: 0.7734375\n",
      "Epoch [782], val_loss: 671.9586\n",
      "gradient norm: 219.41819381713867, minimum ratio: 0.78515625\n",
      "Epoch [783], val_loss: 674.9544\n",
      "gradient norm: 220.069974899292, minimum ratio: 0.78515625\n",
      "Epoch [784], val_loss: 677.9596\n",
      "gradient norm: 220.7212905883789, minimum ratio: 0.7578125\n",
      "Epoch [785], val_loss: 680.9750\n",
      "gradient norm: 221.37624168395996, minimum ratio: 0.75390625\n",
      "Epoch [786], val_loss: 684.0005\n",
      "gradient norm: 222.03577041625977, minimum ratio: 0.78125\n",
      "Epoch [787], val_loss: 687.0356\n",
      "gradient norm: 222.69427680969238, minimum ratio: 0.77734375\n",
      "Epoch [788], val_loss: 690.0809\n",
      "gradient norm: 223.35394096374512, minimum ratio: 0.79296875\n",
      "Epoch [789], val_loss: 693.1370\n",
      "gradient norm: 224.02195930480957, minimum ratio: 0.8046875\n",
      "Epoch [790], val_loss: 696.2031\n",
      "gradient norm: 224.6826343536377, minimum ratio: 0.7734375\n",
      "Epoch [791], val_loss: 699.2793\n",
      "gradient norm: 225.34439659118652, minimum ratio: 0.80078125\n",
      "Epoch [792], val_loss: 702.3654\n",
      "gradient norm: 226.00977516174316, minimum ratio: 0.77734375\n",
      "Epoch [793], val_loss: 705.4615\n",
      "gradient norm: 226.67813110351562, minimum ratio: 0.765625\n",
      "Epoch [794], val_loss: 708.5677\n",
      "gradient norm: 227.34686470031738, minimum ratio: 0.75390625\n",
      "Epoch [795], val_loss: 711.6841\n",
      "gradient norm: 228.01238822937012, minimum ratio: 0.7734375\n",
      "Epoch [796], val_loss: 714.8109\n",
      "gradient norm: 228.68229866027832, minimum ratio: 0.75\n",
      "Epoch [797], val_loss: 717.9482\n",
      "gradient norm: 229.35350608825684, minimum ratio: 0.76953125\n",
      "Epoch [798], val_loss: 721.0957\n",
      "gradient norm: 230.0253143310547, minimum ratio: 0.77734375\n",
      "Epoch [799], val_loss: 724.2535\n",
      "gradient norm: 230.6963176727295, minimum ratio: 0.7734375\n",
      "Epoch [800], val_loss: 727.4218\n",
      "gradient norm: 231.3713550567627, minimum ratio: 0.78515625\n",
      "Epoch [801], val_loss: 730.6010\n",
      "gradient norm: 232.04792594909668, minimum ratio: 0.7890625\n",
      "Epoch [802], val_loss: 733.7904\n",
      "gradient norm: 232.7255744934082, minimum ratio: 0.7734375\n",
      "Epoch [803], val_loss: 736.9903\n",
      "gradient norm: 233.40734672546387, minimum ratio: 0.77734375\n",
      "Epoch [804], val_loss: 740.2002\n",
      "gradient norm: 234.0898838043213, minimum ratio: 0.7734375\n",
      "Epoch [805], val_loss: 743.4213\n",
      "gradient norm: 234.76902389526367, minimum ratio: 0.80078125\n",
      "Epoch [806], val_loss: 746.6525\n",
      "gradient norm: 235.45489311218262, minimum ratio: 0.78125\n",
      "Epoch [807], val_loss: 749.8934\n",
      "gradient norm: 236.14299774169922, minimum ratio: 0.76171875\n",
      "Epoch [808], val_loss: 753.1451\n",
      "gradient norm: 236.82660484313965, minimum ratio: 0.7890625\n",
      "Epoch [809], val_loss: 756.4069\n",
      "gradient norm: 237.51228141784668, minimum ratio: 0.76953125\n",
      "Epoch [810], val_loss: 759.6790\n",
      "gradient norm: 238.1976318359375, minimum ratio: 0.7734375\n",
      "Epoch [811], val_loss: 762.9618\n",
      "gradient norm: 238.89006233215332, minimum ratio: 0.78515625\n",
      "Epoch [812], val_loss: 766.2550\n",
      "gradient norm: 239.58292770385742, minimum ratio: 0.8046875\n",
      "Epoch [813], val_loss: 769.5594\n",
      "gradient norm: 240.2773151397705, minimum ratio: 0.7734375\n",
      "Epoch [814], val_loss: 772.8737\n",
      "gradient norm: 240.96793174743652, minimum ratio: 0.765625\n",
      "Epoch [815], val_loss: 776.1994\n",
      "gradient norm: 241.66020011901855, minimum ratio: 0.78125\n",
      "Epoch [816], val_loss: 779.5359\n",
      "gradient norm: 242.3523120880127, minimum ratio: 0.77734375\n",
      "Epoch [817], val_loss: 782.8826\n",
      "gradient norm: 243.04656600952148, minimum ratio: 0.796875\n",
      "Epoch [818], val_loss: 786.2405\n",
      "gradient norm: 243.74719619750977, minimum ratio: 0.77734375\n",
      "Epoch [819], val_loss: 789.6091\n",
      "gradient norm: 244.45038414001465, minimum ratio: 0.76953125\n",
      "Epoch [820], val_loss: 792.9887\n",
      "gradient norm: 245.1504306793213, minimum ratio: 0.765625\n",
      "Epoch [821], val_loss: 796.3793\n",
      "gradient norm: 245.8505916595459, minimum ratio: 0.76171875\n",
      "Epoch [822], val_loss: 799.7804\n",
      "gradient norm: 246.55231857299805, minimum ratio: 0.765625\n",
      "Epoch [823], val_loss: 803.1926\n",
      "gradient norm: 247.26054191589355, minimum ratio: 0.77734375\n",
      "Epoch [824], val_loss: 806.6158\n",
      "gradient norm: 247.97056007385254, minimum ratio: 0.75390625\n",
      "Epoch [825], val_loss: 810.0503\n",
      "gradient norm: 248.6803684234619, minimum ratio: 0.74609375\n",
      "Epoch [826], val_loss: 813.4957\n",
      "gradient norm: 249.3879909515381, minimum ratio: 0.78125\n",
      "Epoch [827], val_loss: 816.9520\n",
      "gradient norm: 250.0986270904541, minimum ratio: 0.78515625\n",
      "Epoch [828], val_loss: 820.4197\n",
      "gradient norm: 250.80297088623047, minimum ratio: 0.76171875\n",
      "Epoch [829], val_loss: 823.8986\n",
      "gradient norm: 251.52062034606934, minimum ratio: 0.78125\n",
      "Epoch [830], val_loss: 827.3885\n",
      "gradient norm: 252.23607635498047, minimum ratio: 0.76953125\n",
      "Epoch [831], val_loss: 830.8895\n",
      "gradient norm: 252.95202827453613, minimum ratio: 0.78125\n",
      "Epoch [832], val_loss: 834.4008\n",
      "gradient norm: 253.66607856750488, minimum ratio: 0.7890625\n",
      "Epoch [833], val_loss: 837.9232\n",
      "gradient norm: 254.3817310333252, minimum ratio: 0.79296875\n",
      "Epoch [834], val_loss: 841.4571\n",
      "gradient norm: 255.0957794189453, minimum ratio: 0.79296875\n",
      "Epoch [835], val_loss: 845.0017\n",
      "gradient norm: 255.8096809387207, minimum ratio: 0.78125\n",
      "Epoch [836], val_loss: 848.5563\n",
      "gradient norm: 256.53604888916016, minimum ratio: 0.7421875\n",
      "Epoch [837], val_loss: 852.1219\n",
      "gradient norm: 257.26398849487305, minimum ratio: 0.7578125\n",
      "Epoch [838], val_loss: 855.6982\n",
      "gradient norm: 257.9896011352539, minimum ratio: 0.7890625\n",
      "Epoch [839], val_loss: 859.2851\n",
      "gradient norm: 258.71533584594727, minimum ratio: 0.7734375\n",
      "Epoch [840], val_loss: 862.8832\n",
      "gradient norm: 259.44480323791504, minimum ratio: 0.75390625\n",
      "Epoch [841], val_loss: 866.4925\n",
      "gradient norm: 260.1748390197754, minimum ratio: 0.765625\n",
      "Epoch [842], val_loss: 870.1129\n",
      "gradient norm: 260.90869903564453, minimum ratio: 0.765625\n",
      "Epoch [843], val_loss: 873.7448\n",
      "gradient norm: 261.63901138305664, minimum ratio: 0.75\n",
      "Epoch [844], val_loss: 877.3877\n",
      "gradient norm: 262.37139892578125, minimum ratio: 0.77734375\n",
      "Epoch [845], val_loss: 881.0422\n",
      "gradient norm: 263.1032524108887, minimum ratio: 0.7734375\n",
      "Epoch [846], val_loss: 884.7076\n",
      "gradient norm: 263.8401679992676, minimum ratio: 0.75390625\n",
      "Epoch [847], val_loss: 888.3843\n",
      "gradient norm: 264.57572174072266, minimum ratio: 0.7890625\n",
      "Epoch [848], val_loss: 892.0728\n",
      "gradient norm: 265.31248474121094, minimum ratio: 0.765625\n",
      "Epoch [849], val_loss: 895.7739\n",
      "gradient norm: 266.0496139526367, minimum ratio: 0.75\n",
      "Epoch [850], val_loss: 899.4863\n",
      "gradient norm: 266.7931900024414, minimum ratio: 0.77734375\n",
      "Epoch [851], val_loss: 903.2099\n",
      "gradient norm: 267.530574798584, minimum ratio: 0.7734375\n",
      "Epoch [852], val_loss: 906.9448\n",
      "gradient norm: 268.2720031738281, minimum ratio: 0.7890625\n",
      "Epoch [853], val_loss: 910.6919\n",
      "gradient norm: 269.0132637023926, minimum ratio: 0.7734375\n",
      "Epoch [854], val_loss: 914.4510\n",
      "gradient norm: 269.75199127197266, minimum ratio: 0.7578125\n",
      "Epoch [855], val_loss: 918.2219\n",
      "gradient norm: 270.49994468688965, minimum ratio: 0.78515625\n",
      "Epoch [856], val_loss: 922.0030\n",
      "gradient norm: 271.25304412841797, minimum ratio: 0.796875\n",
      "Epoch [857], val_loss: 925.7955\n",
      "gradient norm: 272.00281715393066, minimum ratio: 0.77734375\n",
      "Epoch [858], val_loss: 929.5992\n",
      "gradient norm: 272.7539348602295, minimum ratio: 0.77734375\n",
      "Epoch [859], val_loss: 933.4136\n",
      "gradient norm: 273.5077819824219, minimum ratio: 0.78125\n",
      "Epoch [860], val_loss: 937.2398\n",
      "gradient norm: 274.2638111114502, minimum ratio: 0.76953125\n",
      "Epoch [861], val_loss: 941.0775\n",
      "gradient norm: 275.01485443115234, minimum ratio: 0.76953125\n",
      "Epoch [862], val_loss: 944.9260\n",
      "gradient norm: 275.7610092163086, minimum ratio: 0.78125\n",
      "Epoch [863], val_loss: 948.7863\n",
      "gradient norm: 276.521520614624, minimum ratio: 0.75390625\n",
      "Epoch [864], val_loss: 952.6581\n",
      "gradient norm: 277.28448486328125, minimum ratio: 0.75390625\n",
      "Epoch [865], val_loss: 956.5419\n",
      "gradient norm: 278.0450954437256, minimum ratio: 0.7734375\n",
      "Epoch [866], val_loss: 960.4374\n",
      "gradient norm: 278.8045463562012, minimum ratio: 0.7578125\n",
      "Epoch [867], val_loss: 964.3454\n",
      "gradient norm: 279.5634250640869, minimum ratio: 0.78125\n",
      "Epoch [868], val_loss: 968.2652\n",
      "gradient norm: 280.332706451416, minimum ratio: 0.7578125\n",
      "Epoch [869], val_loss: 972.1965\n",
      "gradient norm: 281.09911155700684, minimum ratio: 0.75390625\n",
      "Epoch [870], val_loss: 976.1404\n",
      "gradient norm: 281.8652153015137, minimum ratio: 0.765625\n",
      "Epoch [871], val_loss: 980.0966\n",
      "gradient norm: 282.63402366638184, minimum ratio: 0.7890625\n",
      "Epoch [872], val_loss: 984.0641\n",
      "gradient norm: 283.4067497253418, minimum ratio: 0.76953125\n",
      "Epoch [873], val_loss: 988.0439\n",
      "gradient norm: 284.1784496307373, minimum ratio: 0.77734375\n",
      "Epoch [874], val_loss: 992.0354\n",
      "gradient norm: 284.94068145751953, minimum ratio: 0.75\n",
      "Epoch [875], val_loss: 996.0388\n",
      "gradient norm: 285.7179946899414, minimum ratio: 0.765625\n",
      "Epoch [876], val_loss: 1000.0541\n",
      "gradient norm: 286.4878234863281, minimum ratio: 0.76171875\n",
      "Epoch [877], val_loss: 1004.0817\n",
      "gradient norm: 287.2635154724121, minimum ratio: 0.7734375\n",
      "Epoch [878], val_loss: 1008.1216\n",
      "gradient norm: 288.04489517211914, minimum ratio: 0.77734375\n",
      "Epoch [879], val_loss: 1012.1730\n",
      "gradient norm: 288.82362937927246, minimum ratio: 0.7890625\n",
      "Epoch [880], val_loss: 1016.2370\n",
      "gradient norm: 289.609188079834, minimum ratio: 0.7734375\n",
      "Epoch [881], val_loss: 1020.3134\n",
      "gradient norm: 290.3880844116211, minimum ratio: 0.79296875\n",
      "Epoch [882], val_loss: 1024.4020\n",
      "gradient norm: 291.1729145050049, minimum ratio: 0.7578125\n",
      "Epoch [883], val_loss: 1028.5026\n",
      "gradient norm: 291.95537185668945, minimum ratio: 0.76171875\n",
      "Epoch [884], val_loss: 1032.6150\n",
      "gradient norm: 292.7386531829834, minimum ratio: 0.796875\n",
      "Epoch [885], val_loss: 1036.7396\n",
      "gradient norm: 293.52842903137207, minimum ratio: 0.76953125\n",
      "Epoch [886], val_loss: 1040.8763\n",
      "gradient norm: 294.31897163391113, minimum ratio: 0.77734375\n",
      "Epoch [887], val_loss: 1045.0253\n",
      "gradient norm: 295.1013641357422, minimum ratio: 0.77734375\n",
      "Epoch [888], val_loss: 1049.1863\n",
      "gradient norm: 295.89414405822754, minimum ratio: 0.78125\n",
      "Epoch [889], val_loss: 1053.3588\n",
      "gradient norm: 296.68888092041016, minimum ratio: 0.7578125\n",
      "Epoch [890], val_loss: 1057.5442\n",
      "gradient norm: 297.4799633026123, minimum ratio: 0.77734375\n",
      "Epoch [891], val_loss: 1061.7418\n",
      "gradient norm: 298.2678966522217, minimum ratio: 0.75390625\n",
      "Epoch [892], val_loss: 1065.9528\n",
      "gradient norm: 299.0602321624756, minimum ratio: 0.78125\n",
      "Epoch [893], val_loss: 1070.1758\n",
      "gradient norm: 299.85252571105957, minimum ratio: 0.77734375\n",
      "Epoch [894], val_loss: 1074.4115\n",
      "gradient norm: 300.65739822387695, minimum ratio: 0.796875\n",
      "Epoch [895], val_loss: 1078.6589\n",
      "gradient norm: 301.4594955444336, minimum ratio: 0.78125\n",
      "Epoch [896], val_loss: 1082.9189\n",
      "gradient norm: 302.2602596282959, minimum ratio: 0.7890625\n",
      "Epoch [897], val_loss: 1087.1914\n",
      "gradient norm: 303.06651496887207, minimum ratio: 0.76953125\n",
      "Epoch [898], val_loss: 1091.4767\n",
      "gradient norm: 303.8737487792969, minimum ratio: 0.78125\n",
      "Epoch [899], val_loss: 1095.7734\n",
      "gradient norm: 304.67809295654297, minimum ratio: 0.765625\n",
      "Epoch [900], val_loss: 1100.0834\n",
      "gradient norm: 305.4840316772461, minimum ratio: 0.76953125\n",
      "Epoch [901], val_loss: 1104.4054\n",
      "gradient norm: 306.29025650024414, minimum ratio: 0.75390625\n",
      "Epoch [902], val_loss: 1108.7404\n",
      "gradient norm: 307.10228157043457, minimum ratio: 0.7734375\n",
      "Epoch [903], val_loss: 1113.0870\n",
      "gradient norm: 307.91743087768555, minimum ratio: 0.7578125\n",
      "Epoch [904], val_loss: 1117.4475\n",
      "gradient norm: 308.72711753845215, minimum ratio: 0.765625\n",
      "Epoch [905], val_loss: 1121.8202\n",
      "gradient norm: 309.54163360595703, minimum ratio: 0.76953125\n",
      "Epoch [906], val_loss: 1126.2062\n",
      "gradient norm: 310.35324478149414, minimum ratio: 0.76953125\n",
      "Epoch [907], val_loss: 1130.6047\n",
      "gradient norm: 311.174596786499, minimum ratio: 0.76171875\n",
      "Epoch [908], val_loss: 1135.0170\n",
      "gradient norm: 311.9982395172119, minimum ratio: 0.74609375\n",
      "Epoch [909], val_loss: 1139.4415\n",
      "gradient norm: 312.81076431274414, minimum ratio: 0.76171875\n",
      "Epoch [910], val_loss: 1143.8794\n",
      "gradient norm: 313.6381893157959, minimum ratio: 0.76953125\n",
      "Epoch [911], val_loss: 1148.3297\n",
      "gradient norm: 314.4610900878906, minimum ratio: 0.75390625\n",
      "Epoch [912], val_loss: 1152.7925\n",
      "gradient norm: 315.28553009033203, minimum ratio: 0.76953125\n",
      "Epoch [913], val_loss: 1157.2677\n",
      "gradient norm: 316.11288833618164, minimum ratio: 0.76953125\n",
      "Epoch [914], val_loss: 1161.7559\n",
      "gradient norm: 316.9386157989502, minimum ratio: 0.77734375\n",
      "Epoch [915], val_loss: 1166.2562\n",
      "gradient norm: 317.7645683288574, minimum ratio: 0.76953125\n",
      "Epoch [916], val_loss: 1170.7695\n",
      "gradient norm: 318.59117698669434, minimum ratio: 0.78515625\n",
      "Epoch [917], val_loss: 1175.2958\n",
      "gradient norm: 319.4240322113037, minimum ratio: 0.7578125\n",
      "Epoch [918], val_loss: 1179.8356\n",
      "gradient norm: 320.2538013458252, minimum ratio: 0.79296875\n",
      "Epoch [919], val_loss: 1184.3881\n",
      "gradient norm: 321.0899715423584, minimum ratio: 0.75390625\n",
      "Epoch [920], val_loss: 1188.9532\n",
      "gradient norm: 321.92416763305664, minimum ratio: 0.78125\n",
      "Epoch [921], val_loss: 1193.5310\n",
      "gradient norm: 322.7527503967285, minimum ratio: 0.76171875\n",
      "Epoch [922], val_loss: 1198.1215\n",
      "gradient norm: 323.59429931640625, minimum ratio: 0.7734375\n",
      "Epoch [923], val_loss: 1202.7257\n",
      "gradient norm: 324.43421173095703, minimum ratio: 0.7578125\n",
      "Epoch [924], val_loss: 1207.3430\n",
      "gradient norm: 325.2713985443115, minimum ratio: 0.76171875\n",
      "Epoch [925], val_loss: 1211.9752\n",
      "gradient norm: 326.1108093261719, minimum ratio: 0.7421875\n",
      "Epoch [926], val_loss: 1216.6198\n",
      "gradient norm: 326.950159072876, minimum ratio: 0.76171875\n",
      "Epoch [927], val_loss: 1221.2773\n",
      "gradient norm: 327.79736709594727, minimum ratio: 0.74609375\n",
      "Epoch [928], val_loss: 1225.9481\n",
      "gradient norm: 328.64735412597656, minimum ratio: 0.7734375\n",
      "Epoch [929], val_loss: 1230.6315\n",
      "gradient norm: 329.49534606933594, minimum ratio: 0.75390625\n",
      "Epoch [930], val_loss: 1235.3280\n",
      "gradient norm: 330.3463954925537, minimum ratio: 0.78515625\n",
      "Epoch [931], val_loss: 1240.0380\n",
      "gradient norm: 331.1942939758301, minimum ratio: 0.76953125\n",
      "Epoch [932], val_loss: 1244.7606\n",
      "gradient norm: 332.03756523132324, minimum ratio: 0.76171875\n",
      "Epoch [933], val_loss: 1249.4954\n",
      "gradient norm: 332.88731384277344, minimum ratio: 0.7734375\n",
      "Epoch [934], val_loss: 1254.2440\n",
      "gradient norm: 333.7434310913086, minimum ratio: 0.7734375\n",
      "Epoch [935], val_loss: 1259.0061\n",
      "gradient norm: 334.6052074432373, minimum ratio: 0.78515625\n",
      "Epoch [936], val_loss: 1263.7806\n",
      "gradient norm: 335.45491218566895, minimum ratio: 0.765625\n",
      "Epoch [937], val_loss: 1268.5679\n",
      "gradient norm: 336.3149471282959, minimum ratio: 0.765625\n",
      "Epoch [938], val_loss: 1273.3685\n",
      "gradient norm: 337.17593574523926, minimum ratio: 0.77734375\n",
      "Epoch [939], val_loss: 1278.1836\n",
      "gradient norm: 338.0359802246094, minimum ratio: 0.765625\n",
      "Epoch [940], val_loss: 1283.0115\n",
      "gradient norm: 338.90167808532715, minimum ratio: 0.75\n",
      "Epoch [941], val_loss: 1287.8536\n",
      "gradient norm: 339.76644134521484, minimum ratio: 0.77734375\n",
      "Epoch [942], val_loss: 1292.7096\n",
      "gradient norm: 340.63369369506836, minimum ratio: 0.78125\n",
      "Epoch [943], val_loss: 1297.5800\n",
      "gradient norm: 341.50125885009766, minimum ratio: 0.7578125\n",
      "Epoch [944], val_loss: 1302.4640\n",
      "gradient norm: 342.37368392944336, minimum ratio: 0.75\n",
      "Epoch [945], val_loss: 1307.3618\n",
      "gradient norm: 343.2423782348633, minimum ratio: 0.76171875\n",
      "Epoch [946], val_loss: 1312.2740\n",
      "gradient norm: 344.1152801513672, minimum ratio: 0.7578125\n",
      "Epoch [947], val_loss: 1317.2004\n",
      "gradient norm: 344.98655700683594, minimum ratio: 0.765625\n",
      "Epoch [948], val_loss: 1322.1403\n",
      "gradient norm: 345.8381996154785, minimum ratio: 0.76171875\n",
      "Epoch [949], val_loss: 1327.0934\n",
      "gradient norm: 346.7208251953125, minimum ratio: 0.78125\n",
      "Epoch [950], val_loss: 1332.0594\n",
      "gradient norm: 347.5931968688965, minimum ratio: 0.77734375\n",
      "Epoch [951], val_loss: 1337.0392\n",
      "gradient norm: 348.47348403930664, minimum ratio: 0.77734375\n",
      "Epoch [952], val_loss: 1342.0325\n",
      "gradient norm: 349.34806060791016, minimum ratio: 0.7578125\n",
      "Epoch [953], val_loss: 1347.0394\n",
      "gradient norm: 350.2301902770996, minimum ratio: 0.77734375\n",
      "Epoch [954], val_loss: 1352.0623\n",
      "gradient norm: 351.11980056762695, minimum ratio: 0.7734375\n",
      "Epoch [955], val_loss: 1357.0979\n",
      "gradient norm: 352.00853729248047, minimum ratio: 0.78125\n",
      "Epoch [956], val_loss: 1362.1473\n",
      "gradient norm: 352.8941230773926, minimum ratio: 0.76171875\n",
      "Epoch [957], val_loss: 1367.2107\n",
      "gradient norm: 353.7853088378906, minimum ratio: 0.7890625\n",
      "Epoch [958], val_loss: 1372.2881\n",
      "gradient norm: 354.67892837524414, minimum ratio: 0.765625\n",
      "Epoch [959], val_loss: 1377.3794\n",
      "gradient norm: 355.5683784484863, minimum ratio: 0.7734375\n",
      "Epoch [960], val_loss: 1382.4835\n",
      "gradient norm: 356.4562568664551, minimum ratio: 0.7421875\n",
      "Epoch [961], val_loss: 1387.6024\n",
      "gradient norm: 357.3481216430664, minimum ratio: 0.765625\n",
      "Epoch [962], val_loss: 1392.7355\n",
      "gradient norm: 358.23917388916016, minimum ratio: 0.7734375\n",
      "Epoch [963], val_loss: 1397.8835\n",
      "gradient norm: 359.1345100402832, minimum ratio: 0.765625\n",
      "Epoch [964], val_loss: 1403.0452\n",
      "gradient norm: 360.027587890625, minimum ratio: 0.7734375\n",
      "Epoch [965], val_loss: 1408.2208\n",
      "gradient norm: 360.9185371398926, minimum ratio: 0.7734375\n",
      "Epoch [966], val_loss: 1413.4109\n",
      "gradient norm: 361.82149505615234, minimum ratio: 0.75\n",
      "Epoch [967], val_loss: 1418.6149\n",
      "gradient norm: 362.72475814819336, minimum ratio: 0.76953125\n",
      "Epoch [968], val_loss: 1423.8330\n",
      "gradient norm: 363.62622451782227, minimum ratio: 0.7734375\n",
      "Epoch [969], val_loss: 1429.0664\n",
      "gradient norm: 364.5277290344238, minimum ratio: 0.765625\n",
      "Epoch [970], val_loss: 1434.3136\n",
      "gradient norm: 365.4373588562012, minimum ratio: 0.77734375\n",
      "Epoch [971], val_loss: 1439.5757\n",
      "gradient norm: 366.3485641479492, minimum ratio: 0.76953125\n",
      "Epoch [972], val_loss: 1444.8514\n",
      "gradient norm: 367.264892578125, minimum ratio: 0.76953125\n",
      "Epoch [973], val_loss: 1450.1412\n",
      "gradient norm: 368.1758155822754, minimum ratio: 0.74609375\n",
      "Epoch [974], val_loss: 1455.4457\n",
      "gradient norm: 369.09536361694336, minimum ratio: 0.765625\n",
      "Epoch [975], val_loss: 1460.7649\n",
      "gradient norm: 370.0023536682129, minimum ratio: 0.78125\n",
      "Epoch [976], val_loss: 1466.0989\n",
      "gradient norm: 370.915470123291, minimum ratio: 0.76953125\n",
      "Epoch [977], val_loss: 1471.4464\n",
      "gradient norm: 371.829833984375, minimum ratio: 0.765625\n",
      "Epoch [978], val_loss: 1476.8081\n",
      "gradient norm: 372.7421531677246, minimum ratio: 0.75\n",
      "Epoch [979], val_loss: 1482.1829\n",
      "gradient norm: 373.64623260498047, minimum ratio: 0.7734375\n",
      "Epoch [980], val_loss: 1487.5715\n",
      "gradient norm: 374.5691680908203, minimum ratio: 0.76171875\n",
      "Epoch [981], val_loss: 1492.9747\n",
      "gradient norm: 375.48779296875, minimum ratio: 0.734375\n",
      "Epoch [982], val_loss: 1498.3921\n",
      "gradient norm: 376.41205978393555, minimum ratio: 0.7734375\n",
      "Epoch [983], val_loss: 1503.8239\n",
      "gradient norm: 377.33166122436523, minimum ratio: 0.76171875\n",
      "Epoch [984], val_loss: 1509.2700\n",
      "gradient norm: 378.2616310119629, minimum ratio: 0.78125\n",
      "Epoch [985], val_loss: 1514.7317\n",
      "gradient norm: 379.1910209655762, minimum ratio: 0.75390625\n",
      "Epoch [986], val_loss: 1520.2070\n",
      "gradient norm: 380.1252670288086, minimum ratio: 0.7578125\n",
      "Epoch [987], val_loss: 1525.6969\n",
      "gradient norm: 381.05605697631836, minimum ratio: 0.75390625\n",
      "Epoch [988], val_loss: 1531.2007\n",
      "gradient norm: 381.9882164001465, minimum ratio: 0.78515625\n",
      "Epoch [989], val_loss: 1536.7179\n",
      "gradient norm: 382.9197425842285, minimum ratio: 0.76171875\n",
      "Epoch [990], val_loss: 1542.2509\n",
      "gradient norm: 383.85595321655273, minimum ratio: 0.78125\n",
      "Epoch [991], val_loss: 1547.7985\n",
      "gradient norm: 384.7872543334961, minimum ratio: 0.77734375\n",
      "Epoch [992], val_loss: 1553.3616\n",
      "gradient norm: 385.7270050048828, minimum ratio: 0.77734375\n",
      "Epoch [993], val_loss: 1558.9399\n",
      "gradient norm: 386.67148208618164, minimum ratio: 0.765625\n",
      "Epoch [994], val_loss: 1564.5327\n",
      "gradient norm: 387.6026153564453, minimum ratio: 0.7734375\n",
      "Epoch [995], val_loss: 1570.1393\n",
      "gradient norm: 388.54345321655273, minimum ratio: 0.765625\n",
      "Epoch [996], val_loss: 1575.7617\n",
      "gradient norm: 389.4865188598633, minimum ratio: 0.76953125\n",
      "Epoch [997], val_loss: 1581.3988\n",
      "gradient norm: 390.4400520324707, minimum ratio: 0.78515625\n",
      "Epoch [998], val_loss: 1587.0505\n",
      "gradient norm: 391.38323974609375, minimum ratio: 0.7734375\n",
      "Epoch [999], val_loss: 1592.7168\n",
      "gradient norm: 392.32178497314453, minimum ratio: 0.77734375\n",
      "Epoch [1000], val_loss: 1598.3977\n",
      "gradient norm: 393.2727470397949, minimum ratio: 0.77734375\n",
      "Epoch [1001], val_loss: 1604.0927\n",
      "gradient norm: 394.2168846130371, minimum ratio: 0.75390625\n",
      "Epoch [1002], val_loss: 1609.8030\n",
      "gradient norm: 395.1619529724121, minimum ratio: 0.76171875\n",
      "Epoch [1003], val_loss: 1615.5275\n",
      "gradient norm: 396.1197204589844, minimum ratio: 0.76171875\n",
      "Epoch [1004], val_loss: 1621.2671\n",
      "gradient norm: 397.07575607299805, minimum ratio: 0.78515625\n",
      "Epoch [1005], val_loss: 1627.0210\n",
      "gradient norm: 398.0335464477539, minimum ratio: 0.77734375\n",
      "Epoch [1006], val_loss: 1632.7905\n",
      "gradient norm: 398.99828338623047, minimum ratio: 0.78125\n",
      "Epoch [1007], val_loss: 1638.5762\n",
      "gradient norm: 399.950740814209, minimum ratio: 0.78125\n",
      "Epoch [1008], val_loss: 1644.3768\n",
      "gradient norm: 400.9094352722168, minimum ratio: 0.796875\n",
      "Epoch [1009], val_loss: 1650.1927\n",
      "gradient norm: 401.8655586242676, minimum ratio: 0.78515625\n",
      "Epoch [1010], val_loss: 1656.0236\n",
      "gradient norm: 402.82308197021484, minimum ratio: 0.76171875\n",
      "Epoch [1011], val_loss: 1661.8684\n",
      "gradient norm: 403.7838134765625, minimum ratio: 0.76171875\n",
      "Epoch [1012], val_loss: 1667.7294\n",
      "gradient norm: 404.75304412841797, minimum ratio: 0.7734375\n",
      "Epoch [1013], val_loss: 1673.6061\n",
      "gradient norm: 405.7164535522461, minimum ratio: 0.7734375\n",
      "Epoch [1014], val_loss: 1679.4973\n",
      "gradient norm: 406.68791580200195, minimum ratio: 0.7578125\n",
      "Epoch [1015], val_loss: 1685.4038\n",
      "gradient norm: 407.66476821899414, minimum ratio: 0.7734375\n",
      "Epoch [1016], val_loss: 1691.3251\n",
      "gradient norm: 408.6421546936035, minimum ratio: 0.76953125\n",
      "Epoch [1017], val_loss: 1697.2609\n",
      "gradient norm: 409.61217880249023, minimum ratio: 0.79296875\n",
      "Epoch [1018], val_loss: 1703.2125\n",
      "gradient norm: 410.59474182128906, minimum ratio: 0.7421875\n",
      "Epoch [1019], val_loss: 1709.1801\n",
      "gradient norm: 411.5639228820801, minimum ratio: 0.78125\n",
      "Epoch [1020], val_loss: 1715.1626\n",
      "gradient norm: 412.54520416259766, minimum ratio: 0.76953125\n",
      "Epoch [1021], val_loss: 1721.1603\n",
      "gradient norm: 413.5309257507324, minimum ratio: 0.75390625\n",
      "Epoch [1022], val_loss: 1727.1735\n",
      "gradient norm: 414.50682067871094, minimum ratio: 0.7890625\n",
      "Epoch [1023], val_loss: 1733.2013\n",
      "gradient norm: 415.4779281616211, minimum ratio: 0.76171875\n",
      "Epoch [1024], val_loss: 1739.2433\n",
      "gradient norm: 416.4453125, minimum ratio: 0.7578125\n",
      "Epoch [1025], val_loss: 1745.3009\n",
      "gradient norm: 417.42432022094727, minimum ratio: 0.77734375\n",
      "Epoch [1026], val_loss: 1751.3744\n",
      "gradient norm: 418.41189193725586, minimum ratio: 0.7734375\n",
      "Epoch [1027], val_loss: 1757.4625\n",
      "gradient norm: 419.4082946777344, minimum ratio: 0.75\n",
      "Epoch [1028], val_loss: 1763.5663\n",
      "gradient norm: 420.3916244506836, minimum ratio: 0.76171875\n",
      "Epoch [1029], val_loss: 1769.6848\n",
      "gradient norm: 421.3808250427246, minimum ratio: 0.79296875\n",
      "Epoch [1030], val_loss: 1775.8190\n",
      "gradient norm: 422.37480545043945, minimum ratio: 0.76171875\n",
      "Epoch [1031], val_loss: 1781.9688\n",
      "gradient norm: 423.3664093017578, minimum ratio: 0.765625\n",
      "Epoch [1032], val_loss: 1788.1342\n",
      "gradient norm: 424.35738372802734, minimum ratio: 0.7734375\n",
      "Epoch [1033], val_loss: 1794.3143\n",
      "gradient norm: 425.3573341369629, minimum ratio: 0.75390625\n",
      "Epoch [1034], val_loss: 1800.5109\n",
      "gradient norm: 426.3523979187012, minimum ratio: 0.7421875\n",
      "Epoch [1035], val_loss: 1806.7246\n",
      "gradient norm: 427.3565483093262, minimum ratio: 0.76171875\n",
      "Epoch [1036], val_loss: 1812.9514\n",
      "gradient norm: 428.3563766479492, minimum ratio: 0.78515625\n",
      "Epoch [1037], val_loss: 1819.1937\n",
      "gradient norm: 429.35774993896484, minimum ratio: 0.75390625\n",
      "Epoch [1038], val_loss: 1825.4525\n",
      "gradient norm: 430.3495216369629, minimum ratio: 0.7734375\n",
      "Epoch [1039], val_loss: 1831.7275\n",
      "gradient norm: 431.34549713134766, minimum ratio: 0.76171875\n",
      "Epoch [1040], val_loss: 1838.0186\n",
      "gradient norm: 432.3484802246094, minimum ratio: 0.73828125\n",
      "Epoch [1041], val_loss: 1844.3267\n",
      "gradient norm: 433.35750579833984, minimum ratio: 0.7734375\n",
      "Epoch [1042], val_loss: 1850.6503\n",
      "gradient norm: 434.35851669311523, minimum ratio: 0.7734375\n",
      "Epoch [1043], val_loss: 1856.9911\n",
      "gradient norm: 435.3754692077637, minimum ratio: 0.7734375\n",
      "Epoch [1044], val_loss: 1863.3485\n",
      "gradient norm: 436.39223861694336, minimum ratio: 0.78125\n",
      "Epoch [1045], val_loss: 1869.7220\n",
      "gradient norm: 437.3990020751953, minimum ratio: 0.7890625\n",
      "Epoch [1046], val_loss: 1876.1096\n",
      "gradient norm: 438.41425704956055, minimum ratio: 0.796875\n",
      "Epoch [1047], val_loss: 1882.5123\n",
      "gradient norm: 439.4364128112793, minimum ratio: 0.77734375\n",
      "Epoch [1048], val_loss: 1888.9296\n",
      "gradient norm: 440.45860290527344, minimum ratio: 0.7890625\n",
      "Epoch [1049], val_loss: 1895.3617\n",
      "gradient norm: 441.4858741760254, minimum ratio: 0.78125\n",
      "Epoch [1050], val_loss: 1901.8083\n",
      "gradient norm: 442.50939559936523, minimum ratio: 0.76953125\n",
      "Epoch [1051], val_loss: 1908.2721\n",
      "gradient norm: 443.53536224365234, minimum ratio: 0.765625\n",
      "Epoch [1052], val_loss: 1914.7518\n",
      "gradient norm: 444.5648193359375, minimum ratio: 0.76953125\n",
      "Epoch [1053], val_loss: 1921.2483\n",
      "gradient norm: 445.5890655517578, minimum ratio: 0.76953125\n",
      "Epoch [1054], val_loss: 1927.7629\n",
      "gradient norm: 446.5851745605469, minimum ratio: 0.77734375\n",
      "Epoch [1055], val_loss: 1934.2950\n",
      "gradient norm: 447.61539459228516, minimum ratio: 0.7734375\n",
      "Epoch [1056], val_loss: 1940.8434\n",
      "gradient norm: 448.63329315185547, minimum ratio: 0.765625\n",
      "Epoch [1057], val_loss: 1947.4080\n",
      "gradient norm: 449.6614570617676, minimum ratio: 0.7734375\n",
      "Epoch [1058], val_loss: 1953.9888\n",
      "gradient norm: 450.6948356628418, minimum ratio: 0.7421875\n",
      "Epoch [1059], val_loss: 1960.5852\n",
      "gradient norm: 451.7335090637207, minimum ratio: 0.79296875\n",
      "Epoch [1060], val_loss: 1967.1978\n",
      "gradient norm: 452.77157974243164, minimum ratio: 0.7578125\n",
      "Epoch [1061], val_loss: 1973.8254\n",
      "gradient norm: 453.8091049194336, minimum ratio: 0.76171875\n",
      "Epoch [1062], val_loss: 1980.4700\n",
      "gradient norm: 454.8411674499512, minimum ratio: 0.7734375\n",
      "Epoch [1063], val_loss: 1987.1309\n",
      "gradient norm: 455.8786926269531, minimum ratio: 0.796875\n",
      "Epoch [1064], val_loss: 1993.8079\n",
      "gradient norm: 456.9212341308594, minimum ratio: 0.7890625\n",
      "Epoch [1065], val_loss: 2000.5006\n",
      "gradient norm: 457.9666175842285, minimum ratio: 0.7734375\n",
      "Epoch [1066], val_loss: 2007.2098\n",
      "gradient norm: 459.02112197875977, minimum ratio: 0.76953125\n",
      "Epoch [1067], val_loss: 2013.9366\n",
      "gradient norm: 460.07090759277344, minimum ratio: 0.79296875\n",
      "Epoch [1068], val_loss: 2020.6801\n",
      "gradient norm: 461.11872482299805, minimum ratio: 0.7734375\n",
      "Epoch [1069], val_loss: 2027.4398\n",
      "gradient norm: 462.17255783081055, minimum ratio: 0.78515625\n",
      "Epoch [1070], val_loss: 2034.2170\n",
      "gradient norm: 463.21338272094727, minimum ratio: 0.7734375\n",
      "Epoch [1071], val_loss: 2041.0110\n",
      "gradient norm: 464.25865936279297, minimum ratio: 0.77734375\n",
      "Epoch [1072], val_loss: 2047.8221\n",
      "gradient norm: 465.29966735839844, minimum ratio: 0.7890625\n",
      "Epoch [1073], val_loss: 2054.6501\n",
      "gradient norm: 466.3525924682617, minimum ratio: 0.7734375\n",
      "Epoch [1074], val_loss: 2061.4968\n",
      "gradient norm: 467.41943359375, minimum ratio: 0.75390625\n",
      "Epoch [1075], val_loss: 2068.3599\n",
      "gradient norm: 468.47254943847656, minimum ratio: 0.76953125\n",
      "Epoch [1076], val_loss: 2075.2395\n",
      "gradient norm: 469.52671813964844, minimum ratio: 0.7578125\n",
      "Epoch [1077], val_loss: 2082.1343\n",
      "gradient norm: 470.5919990539551, minimum ratio: 0.7734375\n",
      "Epoch [1078], val_loss: 2089.0454\n",
      "gradient norm: 471.6522674560547, minimum ratio: 0.77734375\n",
      "Epoch [1079], val_loss: 2095.9736\n",
      "gradient norm: 472.7106628417969, minimum ratio: 0.796875\n",
      "Epoch [1080], val_loss: 2102.9204\n",
      "gradient norm: 473.7832946777344, minimum ratio: 0.78125\n",
      "Epoch [1081], val_loss: 2109.8850\n",
      "gradient norm: 474.85351943969727, minimum ratio: 0.78515625\n",
      "Epoch [1082], val_loss: 2116.8665\n",
      "gradient norm: 475.92533111572266, minimum ratio: 0.765625\n",
      "Epoch [1083], val_loss: 2123.8640\n",
      "gradient norm: 476.99551010131836, minimum ratio: 0.78125\n",
      "Epoch [1084], val_loss: 2130.8799\n",
      "gradient norm: 478.06980895996094, minimum ratio: 0.796875\n",
      "Epoch [1085], val_loss: 2137.9126\n",
      "gradient norm: 479.14595794677734, minimum ratio: 0.78125\n",
      "Epoch [1086], val_loss: 2144.9617\n",
      "gradient norm: 480.21473693847656, minimum ratio: 0.78515625\n",
      "Epoch [1087], val_loss: 2152.0278\n",
      "gradient norm: 481.2938003540039, minimum ratio: 0.7890625\n",
      "Epoch [1088], val_loss: 2159.1116\n",
      "gradient norm: 482.37425231933594, minimum ratio: 0.765625\n",
      "Epoch [1089], val_loss: 2166.2134\n",
      "gradient norm: 483.4453544616699, minimum ratio: 0.78515625\n",
      "Epoch [1090], val_loss: 2173.3308\n",
      "gradient norm: 484.51839447021484, minimum ratio: 0.765625\n",
      "Epoch [1091], val_loss: 2180.4658\n",
      "gradient norm: 485.6000518798828, minimum ratio: 0.7421875\n",
      "Epoch [1092], val_loss: 2187.6150\n",
      "gradient norm: 486.68714141845703, minimum ratio: 0.78515625\n",
      "Epoch [1093], val_loss: 2194.7808\n",
      "gradient norm: 487.7788848876953, minimum ratio: 0.7734375\n",
      "Epoch [1094], val_loss: 2201.9639\n",
      "gradient norm: 488.8723831176758, minimum ratio: 0.76953125\n",
      "Epoch [1095], val_loss: 2209.1653\n",
      "gradient norm: 489.96578216552734, minimum ratio: 0.79296875\n",
      "Epoch [1096], val_loss: 2216.3835\n",
      "gradient norm: 491.0636558532715, minimum ratio: 0.765625\n",
      "Epoch [1097], val_loss: 2223.6196\n",
      "gradient norm: 492.15247344970703, minimum ratio: 0.76171875\n",
      "Epoch [1098], val_loss: 2230.8721\n",
      "gradient norm: 493.255916595459, minimum ratio: 0.76171875\n",
      "Epoch [1099], val_loss: 2238.1428\n",
      "gradient norm: 494.33897399902344, minimum ratio: 0.78125\n",
      "Epoch [1100], val_loss: 2245.4307\n",
      "gradient norm: 495.42320251464844, minimum ratio: 0.75\n",
      "Epoch [1101], val_loss: 2252.7366\n",
      "gradient norm: 496.52635955810547, minimum ratio: 0.78515625\n",
      "Epoch [1102], val_loss: 2260.0588\n",
      "gradient norm: 497.61681747436523, minimum ratio: 0.7734375\n",
      "Epoch [1103], val_loss: 2267.3982\n",
      "gradient norm: 498.71489334106445, minimum ratio: 0.7734375\n",
      "Epoch [1104], val_loss: 2274.7551\n",
      "gradient norm: 499.8149719238281, minimum ratio: 0.76953125\n",
      "Epoch [1105], val_loss: 2282.1294\n",
      "gradient norm: 500.9115982055664, minimum ratio: 0.74609375\n",
      "Epoch [1106], val_loss: 2289.5220\n",
      "gradient norm: 501.99691009521484, minimum ratio: 0.75\n",
      "Epoch [1107], val_loss: 2296.9329\n",
      "gradient norm: 503.1147041320801, minimum ratio: 0.78515625\n",
      "Epoch [1108], val_loss: 2304.3618\n",
      "gradient norm: 504.21963119506836, minimum ratio: 0.7734375\n",
      "Epoch [1109], val_loss: 2311.8088\n",
      "gradient norm: 505.3356018066406, minimum ratio: 0.78125\n",
      "Epoch [1110], val_loss: 2319.2720\n",
      "gradient norm: 506.4501647949219, minimum ratio: 0.765625\n",
      "Epoch [1111], val_loss: 2326.7512\n",
      "gradient norm: 507.5735206604004, minimum ratio: 0.78515625\n",
      "Epoch [1112], val_loss: 2334.2473\n",
      "gradient norm: 508.69430923461914, minimum ratio: 0.78125\n",
      "Epoch [1113], val_loss: 2341.7600\n",
      "gradient norm: 509.8203239440918, minimum ratio: 0.7578125\n",
      "Epoch [1114], val_loss: 2349.2881\n",
      "gradient norm: 510.94321060180664, minimum ratio: 0.78125\n",
      "Epoch [1115], val_loss: 2356.8320\n",
      "gradient norm: 512.0636329650879, minimum ratio: 0.78125\n",
      "Epoch [1116], val_loss: 2364.3950\n",
      "gradient norm: 513.1765518188477, minimum ratio: 0.7734375\n",
      "Epoch [1117], val_loss: 2371.9766\n",
      "gradient norm: 514.3010292053223, minimum ratio: 0.7734375\n",
      "Epoch [1118], val_loss: 2379.5774\n",
      "gradient norm: 515.4138450622559, minimum ratio: 0.765625\n",
      "Epoch [1119], val_loss: 2387.1941\n",
      "gradient norm: 516.5373916625977, minimum ratio: 0.796875\n",
      "Epoch [1120], val_loss: 2394.8274\n",
      "gradient norm: 517.6526679992676, minimum ratio: 0.75390625\n",
      "Epoch [1121], val_loss: 2402.4773\n",
      "gradient norm: 518.7769393920898, minimum ratio: 0.78515625\n",
      "Epoch [1122], val_loss: 2410.1460\n",
      "gradient norm: 519.892017364502, minimum ratio: 0.76953125\n",
      "Epoch [1123], val_loss: 2417.8345\n",
      "gradient norm: 521.0068321228027, minimum ratio: 0.78515625\n",
      "Epoch [1124], val_loss: 2425.5408\n",
      "gradient norm: 522.1491508483887, minimum ratio: 0.7890625\n",
      "Epoch [1125], val_loss: 2433.2642\n",
      "gradient norm: 523.2903747558594, minimum ratio: 0.7578125\n",
      "Epoch [1126], val_loss: 2441.0066\n",
      "gradient norm: 524.4190330505371, minimum ratio: 0.7734375\n",
      "Epoch [1127], val_loss: 2448.7678\n",
      "gradient norm: 525.5586585998535, minimum ratio: 0.78515625\n",
      "Epoch [1128], val_loss: 2456.5469\n",
      "gradient norm: 526.6964645385742, minimum ratio: 0.78515625\n",
      "Epoch [1129], val_loss: 2464.3440\n",
      "gradient norm: 527.8391647338867, minimum ratio: 0.74609375\n",
      "Epoch [1130], val_loss: 2472.1606\n",
      "gradient norm: 528.9762687683105, minimum ratio: 0.76953125\n",
      "Epoch [1131], val_loss: 2479.9963\n",
      "gradient norm: 530.1308975219727, minimum ratio: 0.765625\n",
      "Epoch [1132], val_loss: 2487.8516\n",
      "gradient norm: 531.2766990661621, minimum ratio: 0.80078125\n",
      "Epoch [1133], val_loss: 2495.7234\n",
      "gradient norm: 532.4268112182617, minimum ratio: 0.7890625\n",
      "Epoch [1134], val_loss: 2503.6128\n",
      "gradient norm: 533.5607147216797, minimum ratio: 0.765625\n",
      "Epoch [1135], val_loss: 2511.5212\n",
      "gradient norm: 534.7049255371094, minimum ratio: 0.7578125\n",
      "Epoch [1136], val_loss: 2519.4468\n",
      "gradient norm: 535.8513031005859, minimum ratio: 0.7421875\n",
      "Epoch [1137], val_loss: 2527.3901\n",
      "gradient norm: 537.0148696899414, minimum ratio: 0.79296875\n",
      "Epoch [1138], val_loss: 2535.3508\n",
      "gradient norm: 538.1673736572266, minimum ratio: 0.76953125\n",
      "Epoch [1139], val_loss: 2543.3293\n",
      "gradient norm: 539.3291778564453, minimum ratio: 0.75390625\n",
      "Epoch [1140], val_loss: 2551.3254\n",
      "gradient norm: 540.4772109985352, minimum ratio: 0.75\n",
      "Epoch [1141], val_loss: 2559.3394\n",
      "gradient norm: 541.63037109375, minimum ratio: 0.7578125\n",
      "Epoch [1142], val_loss: 2567.3711\n",
      "gradient norm: 542.7786140441895, minimum ratio: 0.76171875\n",
      "Epoch [1143], val_loss: 2575.4207\n",
      "gradient norm: 543.9358978271484, minimum ratio: 0.76171875\n",
      "Epoch [1144], val_loss: 2583.4890\n",
      "gradient norm: 545.1060180664062, minimum ratio: 0.74609375\n",
      "Epoch [1145], val_loss: 2591.5759\n",
      "gradient norm: 546.2737655639648, minimum ratio: 0.7578125\n",
      "Epoch [1146], val_loss: 2599.6797\n",
      "gradient norm: 547.4378967285156, minimum ratio: 0.765625\n",
      "Epoch [1147], val_loss: 2607.8047\n",
      "gradient norm: 548.6057624816895, minimum ratio: 0.77734375\n",
      "Epoch [1148], val_loss: 2615.9482\n",
      "gradient norm: 549.764965057373, minimum ratio: 0.7734375\n",
      "Epoch [1149], val_loss: 2624.1116\n",
      "gradient norm: 550.9322776794434, minimum ratio: 0.7578125\n",
      "Epoch [1150], val_loss: 2632.2947\n",
      "gradient norm: 552.0939979553223, minimum ratio: 0.76171875\n",
      "Epoch [1151], val_loss: 2640.4961\n",
      "gradient norm: 553.2712287902832, minimum ratio: 0.78125\n",
      "Epoch [1152], val_loss: 2648.7151\n",
      "gradient norm: 554.4580726623535, minimum ratio: 0.78125\n",
      "Epoch [1153], val_loss: 2656.9529\n",
      "gradient norm: 555.6300201416016, minimum ratio: 0.79296875\n",
      "Epoch [1154], val_loss: 2665.2095\n",
      "gradient norm: 556.8097991943359, minimum ratio: 0.75\n",
      "Epoch [1155], val_loss: 2673.4858\n",
      "gradient norm: 557.9957008361816, minimum ratio: 0.765625\n",
      "Epoch [1156], val_loss: 2681.7805\n",
      "gradient norm: 559.1795463562012, minimum ratio: 0.7734375\n",
      "Epoch [1157], val_loss: 2690.0945\n",
      "gradient norm: 560.3697166442871, minimum ratio: 0.77734375\n",
      "Epoch [1158], val_loss: 2698.4277\n",
      "gradient norm: 561.5431289672852, minimum ratio: 0.76953125\n",
      "Epoch [1159], val_loss: 2706.7800\n",
      "gradient norm: 562.741886138916, minimum ratio: 0.76171875\n",
      "Epoch [1160], val_loss: 2715.1519\n",
      "gradient norm: 563.9212684631348, minimum ratio: 0.76953125\n",
      "Epoch [1161], val_loss: 2723.5408\n",
      "gradient norm: 565.122802734375, minimum ratio: 0.75\n",
      "Epoch [1162], val_loss: 2731.9485\n",
      "gradient norm: 566.3022613525391, minimum ratio: 0.7578125\n",
      "Epoch [1163], val_loss: 2740.3748\n",
      "gradient norm: 567.490421295166, minimum ratio: 0.76171875\n",
      "Epoch [1164], val_loss: 2748.8232\n",
      "gradient norm: 568.6879272460938, minimum ratio: 0.77734375\n",
      "Epoch [1165], val_loss: 2757.2891\n",
      "gradient norm: 569.8714065551758, minimum ratio: 0.765625\n",
      "Epoch [1166], val_loss: 2765.7732\n",
      "gradient norm: 571.0757713317871, minimum ratio: 0.75\n",
      "Epoch [1167], val_loss: 2774.2759\n",
      "gradient norm: 572.2778358459473, minimum ratio: 0.76171875\n",
      "Epoch [1168], val_loss: 2782.7966\n",
      "gradient norm: 573.4783248901367, minimum ratio: 0.765625\n",
      "Epoch [1169], val_loss: 2791.3372\n",
      "gradient norm: 574.6854858398438, minimum ratio: 0.76171875\n",
      "Epoch [1170], val_loss: 2799.8967\n",
      "gradient norm: 575.8773231506348, minimum ratio: 0.7734375\n",
      "Epoch [1171], val_loss: 2808.4749\n",
      "gradient norm: 577.0881156921387, minimum ratio: 0.7734375\n",
      "Epoch [1172], val_loss: 2817.0715\n",
      "gradient norm: 578.2828674316406, minimum ratio: 0.76953125\n",
      "Epoch [1173], val_loss: 2825.6880\n",
      "gradient norm: 579.4669151306152, minimum ratio: 0.78515625\n",
      "Epoch [1174], val_loss: 2834.3232\n",
      "gradient norm: 580.6886825561523, minimum ratio: 0.77734375\n",
      "Epoch [1175], val_loss: 2842.9763\n",
      "gradient norm: 581.9008636474609, minimum ratio: 0.75\n",
      "Epoch [1176], val_loss: 2851.6477\n",
      "gradient norm: 583.1201667785645, minimum ratio: 0.765625\n",
      "Epoch [1177], val_loss: 2860.3374\n",
      "gradient norm: 584.3299179077148, minimum ratio: 0.75\n",
      "Epoch [1178], val_loss: 2869.0461\n",
      "gradient norm: 585.5408096313477, minimum ratio: 0.765625\n",
      "Epoch [1179], val_loss: 2877.7749\n",
      "gradient norm: 586.7481117248535, minimum ratio: 0.77734375\n",
      "Epoch [1180], val_loss: 2886.5234\n",
      "gradient norm: 587.978759765625, minimum ratio: 0.76171875\n",
      "Epoch [1181], val_loss: 2895.2908\n",
      "gradient norm: 589.1985969543457, minimum ratio: 0.7734375\n",
      "Epoch [1182], val_loss: 2904.0762\n",
      "gradient norm: 590.4137268066406, minimum ratio: 0.76953125\n",
      "Epoch [1183], val_loss: 2912.8823\n",
      "gradient norm: 591.6241874694824, minimum ratio: 0.78125\n",
      "Epoch [1184], val_loss: 2921.7097\n",
      "gradient norm: 592.8528137207031, minimum ratio: 0.75390625\n",
      "Epoch [1185], val_loss: 2930.5564\n",
      "gradient norm: 594.0781440734863, minimum ratio: 0.765625\n",
      "Epoch [1186], val_loss: 2939.4226\n",
      "gradient norm: 595.3110847473145, minimum ratio: 0.78515625\n",
      "Epoch [1187], val_loss: 2948.3071\n",
      "gradient norm: 596.5404624938965, minimum ratio: 0.79296875\n",
      "Epoch [1188], val_loss: 2957.2129\n",
      "gradient norm: 597.7800369262695, minimum ratio: 0.75\n",
      "Epoch [1189], val_loss: 2966.1367\n",
      "gradient norm: 599.0214042663574, minimum ratio: 0.76171875\n",
      "Epoch [1190], val_loss: 2975.0791\n",
      "gradient norm: 600.247730255127, minimum ratio: 0.7578125\n",
      "Epoch [1191], val_loss: 2984.0430\n",
      "gradient norm: 601.4781684875488, minimum ratio: 0.76171875\n",
      "Epoch [1192], val_loss: 2993.0234\n",
      "gradient norm: 602.7207145690918, minimum ratio: 0.765625\n",
      "Epoch [1193], val_loss: 3002.0247\n",
      "gradient norm: 603.9620513916016, minimum ratio: 0.75390625\n",
      "Epoch [1194], val_loss: 3011.0447\n",
      "gradient norm: 605.2048645019531, minimum ratio: 0.75\n",
      "Epoch [1195], val_loss: 3020.0859\n",
      "gradient norm: 606.4104804992676, minimum ratio: 0.765625\n",
      "Epoch [1196], val_loss: 3029.1487\n",
      "gradient norm: 607.6672172546387, minimum ratio: 0.75\n",
      "Epoch [1197], val_loss: 3038.2310\n",
      "gradient norm: 608.9020919799805, minimum ratio: 0.765625\n",
      "Epoch [1198], val_loss: 3047.3311\n",
      "gradient norm: 610.1613426208496, minimum ratio: 0.74609375\n",
      "Epoch [1199], val_loss: 3056.4495\n",
      "gradient norm: 611.4147186279297, minimum ratio: 0.78515625\n",
      "Epoch [1200], val_loss: 3065.5903\n",
      "gradient norm: 612.664134979248, minimum ratio: 0.7578125\n",
      "Epoch [1201], val_loss: 3074.7510\n",
      "gradient norm: 613.9232025146484, minimum ratio: 0.765625\n",
      "Epoch [1202], val_loss: 3083.9312\n",
      "gradient norm: 615.1691246032715, minimum ratio: 0.77734375\n",
      "Epoch [1203], val_loss: 3093.1304\n",
      "gradient norm: 616.4342994689941, minimum ratio: 0.7578125\n",
      "Epoch [1204], val_loss: 3102.3525\n",
      "gradient norm: 617.6838836669922, minimum ratio: 0.765625\n",
      "Epoch [1205], val_loss: 3111.5928\n",
      "gradient norm: 618.936580657959, minimum ratio: 0.7734375\n",
      "Epoch [1206], val_loss: 3120.8518\n",
      "gradient norm: 620.1964378356934, minimum ratio: 0.76171875\n",
      "Epoch [1207], val_loss: 3130.1321\n",
      "gradient norm: 621.4638671875, minimum ratio: 0.74609375\n",
      "Epoch [1208], val_loss: 3139.4333\n",
      "gradient norm: 622.7285499572754, minimum ratio: 0.77734375\n",
      "Epoch [1209], val_loss: 3148.7534\n",
      "gradient norm: 623.9809684753418, minimum ratio: 0.77734375\n",
      "Epoch [1210], val_loss: 3158.0938\n",
      "gradient norm: 625.2533111572266, minimum ratio: 0.76953125\n",
      "Epoch [1211], val_loss: 3167.4526\n",
      "gradient norm: 626.502311706543, minimum ratio: 0.76953125\n",
      "Epoch [1212], val_loss: 3176.8320\n",
      "gradient norm: 627.7784423828125, minimum ratio: 0.75390625\n",
      "Epoch [1213], val_loss: 3186.2305\n",
      "gradient norm: 629.039867401123, minimum ratio: 0.76953125\n",
      "Epoch [1214], val_loss: 3195.6521\n",
      "gradient norm: 630.294189453125, minimum ratio: 0.765625\n",
      "Epoch [1215], val_loss: 3205.0947\n",
      "gradient norm: 631.5761489868164, minimum ratio: 0.765625\n",
      "Epoch [1216], val_loss: 3214.5579\n",
      "gradient norm: 632.8654403686523, minimum ratio: 0.76171875\n",
      "Epoch [1217], val_loss: 3224.0420\n",
      "gradient norm: 634.1389923095703, minimum ratio: 0.76953125\n",
      "Epoch [1218], val_loss: 3233.5466\n",
      "gradient norm: 635.4318580627441, minimum ratio: 0.74609375\n",
      "Epoch [1219], val_loss: 3243.0740\n",
      "gradient norm: 636.7113952636719, minimum ratio: 0.7421875\n",
      "Epoch [1220], val_loss: 3252.6208\n",
      "gradient norm: 637.9985961914062, minimum ratio: 0.7578125\n",
      "Epoch [1221], val_loss: 3262.1895\n",
      "gradient norm: 639.2509651184082, minimum ratio: 0.78125\n",
      "Epoch [1222], val_loss: 3271.7795\n",
      "gradient norm: 640.5420188903809, minimum ratio: 0.78125\n",
      "Epoch [1223], val_loss: 3281.3887\n",
      "gradient norm: 641.8238716125488, minimum ratio: 0.7734375\n",
      "Epoch [1224], val_loss: 3291.0171\n",
      "gradient norm: 643.1266326904297, minimum ratio: 0.7734375\n",
      "Epoch [1225], val_loss: 3300.6655\n",
      "gradient norm: 644.3943481445312, minimum ratio: 0.76171875\n",
      "Epoch [1226], val_loss: 3310.3318\n",
      "gradient norm: 645.6857872009277, minimum ratio: 0.78515625\n",
      "Epoch [1227], val_loss: 3320.0205\n",
      "gradient norm: 646.9931297302246, minimum ratio: 0.75\n",
      "Epoch [1228], val_loss: 3329.7324\n",
      "gradient norm: 648.2701187133789, minimum ratio: 0.76171875\n",
      "Epoch [1229], val_loss: 3339.4685\n",
      "gradient norm: 649.582405090332, minimum ratio: 0.7578125\n",
      "Epoch [1230], val_loss: 3349.2241\n",
      "gradient norm: 650.8818244934082, minimum ratio: 0.77734375\n",
      "Epoch [1231], val_loss: 3358.9993\n",
      "gradient norm: 652.1853218078613, minimum ratio: 0.78125\n",
      "Epoch [1232], val_loss: 3368.7952\n",
      "gradient norm: 653.4982528686523, minimum ratio: 0.765625\n",
      "Epoch [1233], val_loss: 3378.6108\n",
      "gradient norm: 654.7932395935059, minimum ratio: 0.76171875\n",
      "Epoch [1234], val_loss: 3388.4468\n",
      "gradient norm: 656.0809516906738, minimum ratio: 0.796875\n",
      "Epoch [1235], val_loss: 3398.2983\n",
      "gradient norm: 657.3785209655762, minimum ratio: 0.7734375\n",
      "Epoch [1236], val_loss: 3408.1726\n",
      "gradient norm: 658.7001113891602, minimum ratio: 0.7734375\n",
      "Epoch [1237], val_loss: 3418.0676\n",
      "gradient norm: 660.0236587524414, minimum ratio: 0.76953125\n",
      "Epoch [1238], val_loss: 3427.9822\n",
      "gradient norm: 661.3483428955078, minimum ratio: 0.75390625\n",
      "Epoch [1239], val_loss: 3437.9172\n",
      "gradient norm: 662.6662216186523, minimum ratio: 0.75390625\n",
      "Epoch [1240], val_loss: 3447.8716\n",
      "gradient norm: 663.9908142089844, minimum ratio: 0.765625\n",
      "Epoch [1241], val_loss: 3457.8459\n",
      "gradient norm: 665.3098678588867, minimum ratio: 0.78125\n",
      "Epoch [1242], val_loss: 3467.8408\n",
      "gradient norm: 666.6404647827148, minimum ratio: 0.78515625\n",
      "Epoch [1243], val_loss: 3477.8555\n",
      "gradient norm: 667.94775390625, minimum ratio: 0.765625\n",
      "Epoch [1244], val_loss: 3487.8943\n",
      "gradient norm: 669.2579498291016, minimum ratio: 0.78515625\n",
      "Epoch [1245], val_loss: 3497.9553\n",
      "gradient norm: 670.5619201660156, minimum ratio: 0.7734375\n",
      "Epoch [1246], val_loss: 3508.0383\n",
      "gradient norm: 671.8833770751953, minimum ratio: 0.7734375\n",
      "Epoch [1247], val_loss: 3518.1416\n",
      "gradient norm: 673.1854095458984, minimum ratio: 0.765625\n",
      "Epoch [1248], val_loss: 3528.2659\n",
      "gradient norm: 674.5264739990234, minimum ratio: 0.7734375\n",
      "Epoch [1249], val_loss: 3538.4099\n",
      "gradient norm: 675.8346405029297, minimum ratio: 0.76171875\n",
      "Epoch [1250], val_loss: 3548.5759\n",
      "gradient norm: 677.165168762207, minimum ratio: 0.7734375\n",
      "Epoch [1251], val_loss: 3558.7634\n",
      "gradient norm: 678.4936981201172, minimum ratio: 0.7734375\n",
      "Epoch [1252], val_loss: 3568.9712\n",
      "gradient norm: 679.8263320922852, minimum ratio: 0.7578125\n",
      "Epoch [1253], val_loss: 3579.2009\n",
      "gradient norm: 681.1544799804688, minimum ratio: 0.76953125\n",
      "Epoch [1254], val_loss: 3589.4524\n",
      "gradient norm: 682.4845504760742, minimum ratio: 0.765625\n",
      "Epoch [1255], val_loss: 3599.7234\n",
      "gradient norm: 683.8067855834961, minimum ratio: 0.78125\n",
      "Epoch [1256], val_loss: 3610.0154\n",
      "gradient norm: 685.1542892456055, minimum ratio: 0.77734375\n",
      "Epoch [1257], val_loss: 3620.3284\n",
      "gradient norm: 686.5061492919922, minimum ratio: 0.765625\n",
      "Epoch [1258], val_loss: 3630.6616\n",
      "gradient norm: 687.8546447753906, minimum ratio: 0.796875\n",
      "Epoch [1259], val_loss: 3641.0164\n",
      "gradient norm: 689.2095718383789, minimum ratio: 0.76953125\n",
      "Epoch [1260], val_loss: 3651.3926\n",
      "gradient norm: 690.5607070922852, minimum ratio: 0.75\n",
      "Epoch [1261], val_loss: 3661.7883\n",
      "gradient norm: 691.9111175537109, minimum ratio: 0.7578125\n",
      "Epoch [1262], val_loss: 3672.2058\n",
      "gradient norm: 693.2642059326172, minimum ratio: 0.73828125\n",
      "Epoch [1263], val_loss: 3682.6462\n",
      "gradient norm: 694.6186752319336, minimum ratio: 0.76171875\n",
      "Epoch [1264], val_loss: 3693.1074\n",
      "gradient norm: 695.9775924682617, minimum ratio: 0.76171875\n",
      "Epoch [1265], val_loss: 3703.5891\n",
      "gradient norm: 697.3399810791016, minimum ratio: 0.80078125\n",
      "Epoch [1266], val_loss: 3714.0916\n",
      "gradient norm: 698.6785736083984, minimum ratio: 0.76953125\n",
      "Epoch [1267], val_loss: 3724.6174\n",
      "gradient norm: 700.0295333862305, minimum ratio: 0.76171875\n",
      "Epoch [1268], val_loss: 3735.1682\n",
      "gradient norm: 701.3849716186523, minimum ratio: 0.78125\n",
      "Epoch [1269], val_loss: 3745.7417\n",
      "gradient norm: 702.7363510131836, minimum ratio: 0.77734375\n",
      "Epoch [1270], val_loss: 3756.3367\n",
      "gradient norm: 704.0900726318359, minimum ratio: 0.76953125\n",
      "Epoch [1271], val_loss: 3766.9534\n",
      "gradient norm: 705.4693069458008, minimum ratio: 0.765625\n",
      "Epoch [1272], val_loss: 3777.5933\n",
      "gradient norm: 706.818359375, minimum ratio: 0.78125\n",
      "Epoch [1273], val_loss: 3788.2534\n",
      "gradient norm: 708.1709976196289, minimum ratio: 0.77734375\n",
      "Epoch [1274], val_loss: 3798.9368\n",
      "gradient norm: 709.5377502441406, minimum ratio: 0.76953125\n",
      "Epoch [1275], val_loss: 3809.6426\n",
      "gradient norm: 710.9116821289062, minimum ratio: 0.7578125\n",
      "Epoch [1276], val_loss: 3820.3696\n",
      "gradient norm: 712.2814102172852, minimum ratio: 0.765625\n",
      "Epoch [1277], val_loss: 3831.1201\n",
      "gradient norm: 713.6511840820312, minimum ratio: 0.7421875\n",
      "Epoch [1278], val_loss: 3841.8884\n",
      "gradient norm: 715.0231170654297, minimum ratio: 0.75\n",
      "Epoch [1279], val_loss: 3852.6821\n",
      "gradient norm: 716.4080352783203, minimum ratio: 0.77734375\n",
      "Epoch [1280], val_loss: 3863.4978\n",
      "gradient norm: 717.7822113037109, minimum ratio: 0.78125\n",
      "Epoch [1281], val_loss: 3874.3367\n",
      "gradient norm: 719.1698150634766, minimum ratio: 0.77734375\n",
      "Epoch [1282], val_loss: 3885.1980\n",
      "gradient norm: 720.5472183227539, minimum ratio: 0.76953125\n",
      "Epoch [1283], val_loss: 3896.0825\n",
      "gradient norm: 721.9390182495117, minimum ratio: 0.78125\n",
      "Epoch [1284], val_loss: 3906.9895\n",
      "gradient norm: 723.3339538574219, minimum ratio: 0.77734375\n",
      "Epoch [1285], val_loss: 3917.9211\n",
      "gradient norm: 724.710823059082, minimum ratio: 0.77734375\n",
      "Epoch [1286], val_loss: 3928.8726\n",
      "gradient norm: 726.1082458496094, minimum ratio: 0.75\n",
      "Epoch [1287], val_loss: 3939.8442\n",
      "gradient norm: 727.48974609375, minimum ratio: 0.77734375\n",
      "Epoch [1288], val_loss: 3950.8374\n",
      "gradient norm: 728.8980407714844, minimum ratio: 0.7890625\n",
      "Epoch [1289], val_loss: 3961.8513\n",
      "gradient norm: 730.2706909179688, minimum ratio: 0.76171875\n",
      "Epoch [1290], val_loss: 3972.8862\n",
      "gradient norm: 731.6667709350586, minimum ratio: 0.73828125\n",
      "Epoch [1291], val_loss: 3983.9446\n",
      "gradient norm: 733.0647354125977, minimum ratio: 0.78125\n",
      "Epoch [1292], val_loss: 3995.0237\n",
      "gradient norm: 734.471435546875, minimum ratio: 0.76171875\n",
      "Epoch [1293], val_loss: 4006.1262\n",
      "gradient norm: 735.8796920776367, minimum ratio: 0.7734375\n",
      "Epoch [1294], val_loss: 4017.2512\n",
      "gradient norm: 737.2976989746094, minimum ratio: 0.77734375\n",
      "Epoch [1295], val_loss: 4028.3987\n",
      "gradient norm: 738.6936187744141, minimum ratio: 0.77734375\n",
      "Epoch [1296], val_loss: 4039.5696\n",
      "gradient norm: 740.1153259277344, minimum ratio: 0.78125\n",
      "Epoch [1297], val_loss: 4050.7617\n",
      "gradient norm: 741.5175170898438, minimum ratio: 0.75\n",
      "Epoch [1298], val_loss: 4061.9751\n",
      "gradient norm: 742.8884582519531, minimum ratio: 0.7421875\n",
      "Epoch [1299], val_loss: 4073.2109\n",
      "gradient norm: 744.2954940795898, minimum ratio: 0.79296875\n",
      "Epoch [1300], val_loss: 4084.4692\n",
      "gradient norm: 745.7069625854492, minimum ratio: 0.77734375\n",
      "Epoch [1301], val_loss: 4095.7493\n",
      "gradient norm: 747.1235275268555, minimum ratio: 0.765625\n",
      "Epoch [1302], val_loss: 4107.0508\n",
      "gradient norm: 748.5539398193359, minimum ratio: 0.80078125\n",
      "Epoch [1303], val_loss: 4118.3735\n",
      "gradient norm: 749.9678955078125, minimum ratio: 0.765625\n",
      "Epoch [1304], val_loss: 4129.7202\n",
      "gradient norm: 751.3921966552734, minimum ratio: 0.78515625\n",
      "Epoch [1305], val_loss: 4141.0913\n",
      "gradient norm: 752.8103561401367, minimum ratio: 0.78125\n",
      "Epoch [1306], val_loss: 4152.4854\n",
      "gradient norm: 754.2182464599609, minimum ratio: 0.7578125\n",
      "Epoch [1307], val_loss: 4163.9043\n",
      "gradient norm: 755.6522521972656, minimum ratio: 0.76953125\n",
      "Epoch [1308], val_loss: 4175.3467\n",
      "gradient norm: 757.0530624389648, minimum ratio: 0.76953125\n",
      "Epoch [1309], val_loss: 4186.8125\n",
      "gradient norm: 758.4761199951172, minimum ratio: 0.78125\n",
      "Epoch [1310], val_loss: 4198.2998\n",
      "gradient norm: 759.8996353149414, minimum ratio: 0.75\n",
      "Epoch [1311], val_loss: 4209.8081\n",
      "gradient norm: 761.3461837768555, minimum ratio: 0.7890625\n",
      "Epoch [1312], val_loss: 4221.3394\n",
      "gradient norm: 762.7944412231445, minimum ratio: 0.7734375\n",
      "Epoch [1313], val_loss: 4232.8916\n",
      "gradient norm: 764.2154998779297, minimum ratio: 0.7578125\n",
      "Epoch [1314], val_loss: 4244.4658\n",
      "gradient norm: 765.6579437255859, minimum ratio: 0.74609375\n",
      "Epoch [1315], val_loss: 4256.0620\n",
      "gradient norm: 767.0937728881836, minimum ratio: 0.78515625\n",
      "Epoch [1316], val_loss: 4267.6841\n",
      "gradient norm: 768.5172348022461, minimum ratio: 0.7734375\n",
      "Epoch [1317], val_loss: 4279.3296\n",
      "gradient norm: 769.9622573852539, minimum ratio: 0.7890625\n",
      "Epoch [1318], val_loss: 4290.9985\n",
      "gradient norm: 771.4076232910156, minimum ratio: 0.76953125\n",
      "Epoch [1319], val_loss: 4302.6929\n",
      "gradient norm: 772.8683776855469, minimum ratio: 0.76171875\n",
      "Epoch [1320], val_loss: 4314.4087\n",
      "gradient norm: 774.3293991088867, minimum ratio: 0.78515625\n",
      "Epoch [1321], val_loss: 4326.1489\n",
      "gradient norm: 775.7299880981445, minimum ratio: 0.77734375\n",
      "Epoch [1322], val_loss: 4337.9106\n",
      "gradient norm: 777.1844482421875, minimum ratio: 0.76953125\n",
      "Epoch [1323], val_loss: 4349.6958\n",
      "gradient norm: 778.6431503295898, minimum ratio: 0.76953125\n",
      "Epoch [1324], val_loss: 4361.5044\n",
      "gradient norm: 780.0998687744141, minimum ratio: 0.76171875\n",
      "Epoch [1325], val_loss: 4373.3330\n",
      "gradient norm: 781.5339508056641, minimum ratio: 0.77734375\n",
      "Epoch [1326], val_loss: 4385.1841\n",
      "gradient norm: 782.9901351928711, minimum ratio: 0.76171875\n",
      "Epoch [1327], val_loss: 4397.0601\n",
      "gradient norm: 784.4327621459961, minimum ratio: 0.76171875\n",
      "Epoch [1328], val_loss: 4408.9600\n",
      "gradient norm: 785.8866500854492, minimum ratio: 0.75\n",
      "Epoch [1329], val_loss: 4420.8818\n",
      "gradient norm: 787.3256683349609, minimum ratio: 0.7578125\n",
      "Epoch [1330], val_loss: 4432.8267\n",
      "gradient norm: 788.7857971191406, minimum ratio: 0.7890625\n",
      "Epoch [1331], val_loss: 4444.7915\n",
      "gradient norm: 790.2326583862305, minimum ratio: 0.7421875\n",
      "Epoch [1332], val_loss: 4456.7798\n",
      "gradient norm: 791.7028732299805, minimum ratio: 0.7421875\n",
      "Epoch [1333], val_loss: 4468.7905\n",
      "gradient norm: 793.1849517822266, minimum ratio: 0.796875\n",
      "Epoch [1334], val_loss: 4480.8223\n",
      "gradient norm: 794.6579360961914, minimum ratio: 0.78515625\n",
      "Epoch [1335], val_loss: 4492.8765\n",
      "gradient norm: 796.1142578125, minimum ratio: 0.77734375\n",
      "Epoch [1336], val_loss: 4504.9561\n",
      "gradient norm: 797.5887756347656, minimum ratio: 0.75390625\n",
      "Epoch [1337], val_loss: 4517.0610\n",
      "gradient norm: 799.0694046020508, minimum ratio: 0.7578125\n",
      "Epoch [1338], val_loss: 4529.1885\n",
      "gradient norm: 800.5232467651367, minimum ratio: 0.76171875\n",
      "Epoch [1339], val_loss: 4541.3379\n",
      "gradient norm: 802.0152282714844, minimum ratio: 0.7734375\n",
      "Epoch [1340], val_loss: 4553.5127\n",
      "gradient norm: 803.5074920654297, minimum ratio: 0.75\n",
      "Epoch [1341], val_loss: 4565.7114\n",
      "gradient norm: 805.0034637451172, minimum ratio: 0.77734375\n",
      "Epoch [1342], val_loss: 4577.9341\n",
      "gradient norm: 806.4936294555664, minimum ratio: 0.76171875\n",
      "Epoch [1343], val_loss: 4590.1748\n",
      "gradient norm: 807.9772872924805, minimum ratio: 0.78125\n",
      "Epoch [1344], val_loss: 4602.4360\n",
      "gradient norm: 809.4541778564453, minimum ratio: 0.7734375\n",
      "Epoch [1345], val_loss: 4614.7227\n",
      "gradient norm: 810.9320297241211, minimum ratio: 0.76171875\n",
      "Epoch [1346], val_loss: 4627.0352\n",
      "gradient norm: 812.41748046875, minimum ratio: 0.77734375\n",
      "Epoch [1347], val_loss: 4639.3755\n",
      "gradient norm: 813.8945541381836, minimum ratio: 0.7578125\n",
      "Epoch [1348], val_loss: 4651.7397\n",
      "gradient norm: 815.3595504760742, minimum ratio: 0.765625\n",
      "Epoch [1349], val_loss: 4664.1260\n",
      "gradient norm: 816.85546875, minimum ratio: 0.73828125\n",
      "Epoch [1350], val_loss: 4676.5356\n",
      "gradient norm: 818.3567352294922, minimum ratio: 0.7578125\n",
      "Epoch [1351], val_loss: 4688.9658\n",
      "gradient norm: 819.8368835449219, minimum ratio: 0.76171875\n",
      "Epoch [1352], val_loss: 4701.4204\n",
      "gradient norm: 821.3158950805664, minimum ratio: 0.78515625\n",
      "Epoch [1353], val_loss: 4713.8989\n",
      "gradient norm: 822.8306655883789, minimum ratio: 0.76171875\n",
      "Epoch [1354], val_loss: 4726.4009\n",
      "gradient norm: 824.32421875, minimum ratio: 0.765625\n",
      "Epoch [1355], val_loss: 4738.9268\n",
      "gradient norm: 825.8162841796875, minimum ratio: 0.76171875\n",
      "Epoch [1356], val_loss: 4751.4785\n",
      "gradient norm: 827.3265609741211, minimum ratio: 0.74609375\n",
      "Epoch [1357], val_loss: 4764.0532\n",
      "gradient norm: 828.8381042480469, minimum ratio: 0.77734375\n",
      "Epoch [1358], val_loss: 4776.6533\n",
      "gradient norm: 830.3536605834961, minimum ratio: 0.765625\n",
      "Epoch [1359], val_loss: 4789.2744\n",
      "gradient norm: 831.862548828125, minimum ratio: 0.78125\n",
      "Epoch [1360], val_loss: 4801.9209\n",
      "gradient norm: 833.3707809448242, minimum ratio: 0.76171875\n",
      "Epoch [1361], val_loss: 4814.5894\n",
      "gradient norm: 834.8527603149414, minimum ratio: 0.75\n",
      "Epoch [1362], val_loss: 4827.2827\n",
      "gradient norm: 836.3617248535156, minimum ratio: 0.76171875\n",
      "Epoch [1363], val_loss: 4840.0010\n",
      "gradient norm: 837.883674621582, minimum ratio: 0.7578125\n",
      "Epoch [1364], val_loss: 4852.7427\n",
      "gradient norm: 839.4168167114258, minimum ratio: 0.76953125\n",
      "Epoch [1365], val_loss: 4865.5107\n",
      "gradient norm: 840.9431610107422, minimum ratio: 0.7734375\n",
      "Epoch [1366], val_loss: 4878.3047\n",
      "gradient norm: 842.4357376098633, minimum ratio: 0.75\n",
      "Epoch [1367], val_loss: 4891.1211\n",
      "gradient norm: 843.958984375, minimum ratio: 0.77734375\n",
      "Epoch [1368], val_loss: 4903.9653\n",
      "gradient norm: 845.4929733276367, minimum ratio: 0.77734375\n",
      "Epoch [1369], val_loss: 4916.8350\n",
      "gradient norm: 847.0066299438477, minimum ratio: 0.765625\n",
      "Epoch [1370], val_loss: 4929.7310\n",
      "gradient norm: 848.5524368286133, minimum ratio: 0.765625\n",
      "Epoch [1371], val_loss: 4942.6489\n",
      "gradient norm: 850.082275390625, minimum ratio: 0.7734375\n",
      "Epoch [1372], val_loss: 4955.5918\n",
      "gradient norm: 851.6188201904297, minimum ratio: 0.78515625\n",
      "Epoch [1373], val_loss: 4968.5547\n",
      "gradient norm: 853.1133651733398, minimum ratio: 0.75390625\n",
      "Epoch [1374], val_loss: 4981.5425\n",
      "gradient norm: 854.6104049682617, minimum ratio: 0.78125\n",
      "Epoch [1375], val_loss: 4994.5581\n",
      "gradient norm: 856.1503372192383, minimum ratio: 0.78125\n",
      "Epoch [1376], val_loss: 5007.5981\n",
      "gradient norm: 857.7046585083008, minimum ratio: 0.76953125\n",
      "Epoch [1377], val_loss: 5020.6626\n",
      "gradient norm: 859.2484283447266, minimum ratio: 0.7734375\n",
      "Epoch [1378], val_loss: 5033.7510\n",
      "gradient norm: 860.7827224731445, minimum ratio: 0.76953125\n",
      "Epoch [1379], val_loss: 5046.8677\n",
      "gradient norm: 862.3144607543945, minimum ratio: 0.7734375\n",
      "Epoch [1380], val_loss: 5060.0073\n",
      "gradient norm: 863.8498153686523, minimum ratio: 0.76953125\n",
      "Epoch [1381], val_loss: 5073.1685\n",
      "gradient norm: 865.411018371582, minimum ratio: 0.7734375\n",
      "Epoch [1382], val_loss: 5086.3560\n",
      "gradient norm: 866.9591751098633, minimum ratio: 0.78515625\n",
      "Epoch [1383], val_loss: 5099.5669\n",
      "gradient norm: 868.515380859375, minimum ratio: 0.74609375\n",
      "Epoch [1384], val_loss: 5112.8027\n",
      "gradient norm: 870.0596237182617, minimum ratio: 0.74609375\n",
      "Epoch [1385], val_loss: 5126.0610\n",
      "gradient norm: 871.6200561523438, minimum ratio: 0.765625\n",
      "Epoch [1386], val_loss: 5139.3467\n",
      "gradient norm: 873.1715850830078, minimum ratio: 0.76953125\n",
      "Epoch [1387], val_loss: 5152.6582\n",
      "gradient norm: 874.7274780273438, minimum ratio: 0.77734375\n",
      "Epoch [1388], val_loss: 5165.9937\n",
      "gradient norm: 876.2796249389648, minimum ratio: 0.76171875\n",
      "Epoch [1389], val_loss: 5179.3560\n",
      "gradient norm: 877.8575057983398, minimum ratio: 0.78125\n",
      "Epoch [1390], val_loss: 5192.7407\n",
      "gradient norm: 879.4151458740234, minimum ratio: 0.78515625\n",
      "Epoch [1391], val_loss: 5206.1543\n",
      "gradient norm: 880.9826431274414, minimum ratio: 0.76953125\n",
      "Epoch [1392], val_loss: 5219.5908\n",
      "gradient norm: 882.5308227539062, minimum ratio: 0.7578125\n",
      "Epoch [1393], val_loss: 5233.0552\n",
      "gradient norm: 884.1164855957031, minimum ratio: 0.77734375\n",
      "Epoch [1394], val_loss: 5246.5425\n",
      "gradient norm: 885.7034149169922, minimum ratio: 0.76171875\n",
      "Epoch [1395], val_loss: 5260.0552\n",
      "gradient norm: 887.2581634521484, minimum ratio: 0.765625\n",
      "Epoch [1396], val_loss: 5273.5933\n",
      "gradient norm: 888.821044921875, minimum ratio: 0.74609375\n",
      "Epoch [1397], val_loss: 5287.1558\n",
      "gradient norm: 890.389778137207, minimum ratio: 0.75\n",
      "Epoch [1398], val_loss: 5300.7480\n",
      "gradient norm: 891.9448928833008, minimum ratio: 0.78515625\n",
      "Epoch [1399], val_loss: 5314.3633\n",
      "gradient norm: 893.521484375, minimum ratio: 0.76953125\n",
      "Epoch [1400], val_loss: 5328.0049\n",
      "gradient norm: 895.1124877929688, minimum ratio: 0.78515625\n",
      "Epoch [1401], val_loss: 5341.6689\n",
      "gradient norm: 896.6556167602539, minimum ratio: 0.7734375\n",
      "Epoch [1402], val_loss: 5355.3564\n",
      "gradient norm: 898.2446670532227, minimum ratio: 0.7734375\n",
      "Epoch [1403], val_loss: 5369.0708\n",
      "gradient norm: 899.8347396850586, minimum ratio: 0.77734375\n",
      "Epoch [1404], val_loss: 5382.8125\n",
      "gradient norm: 901.4297714233398, minimum ratio: 0.78515625\n",
      "Epoch [1405], val_loss: 5396.5811\n",
      "gradient norm: 903.0185546875, minimum ratio: 0.7734375\n",
      "Epoch [1406], val_loss: 5410.3794\n",
      "gradient norm: 904.6172790527344, minimum ratio: 0.76953125\n",
      "Epoch [1407], val_loss: 5424.2026\n",
      "gradient norm: 906.2046813964844, minimum ratio: 0.77734375\n",
      "Epoch [1408], val_loss: 5438.0498\n",
      "gradient norm: 907.8076629638672, minimum ratio: 0.78125\n",
      "Epoch [1409], val_loss: 5451.9258\n",
      "gradient norm: 909.3801345825195, minimum ratio: 0.76171875\n",
      "Epoch [1410], val_loss: 5465.8271\n",
      "gradient norm: 910.9755477905273, minimum ratio: 0.78125\n",
      "Epoch [1411], val_loss: 5479.7544\n",
      "gradient norm: 912.5584030151367, minimum ratio: 0.7578125\n",
      "Epoch [1412], val_loss: 5493.7051\n",
      "gradient norm: 914.1675033569336, minimum ratio: 0.77734375\n",
      "Epoch [1413], val_loss: 5507.6821\n",
      "gradient norm: 915.7897567749023, minimum ratio: 0.76171875\n",
      "Epoch [1414], val_loss: 5521.6826\n",
      "gradient norm: 917.3929290771484, minimum ratio: 0.75\n",
      "Epoch [1415], val_loss: 5535.7041\n",
      "gradient norm: 919.01708984375, minimum ratio: 0.76171875\n",
      "Epoch [1416], val_loss: 5549.7529\n",
      "gradient norm: 920.5850982666016, minimum ratio: 0.7578125\n",
      "Epoch [1417], val_loss: 5563.8281\n",
      "gradient norm: 922.1942367553711, minimum ratio: 0.7734375\n",
      "Epoch [1418], val_loss: 5577.9272\n",
      "gradient norm: 923.7961196899414, minimum ratio: 0.75390625\n",
      "Epoch [1419], val_loss: 5592.0562\n",
      "gradient norm: 925.4038009643555, minimum ratio: 0.78125\n",
      "Epoch [1420], val_loss: 5606.2090\n",
      "gradient norm: 927.0233764648438, minimum ratio: 0.76953125\n",
      "Epoch [1421], val_loss: 5620.3857\n",
      "gradient norm: 928.6452713012695, minimum ratio: 0.75\n",
      "Epoch [1422], val_loss: 5634.5864\n",
      "gradient norm: 930.2619323730469, minimum ratio: 0.78125\n",
      "Epoch [1423], val_loss: 5648.8154\n",
      "gradient norm: 931.8886337280273, minimum ratio: 0.7578125\n",
      "Epoch [1424], val_loss: 5663.0703\n",
      "gradient norm: 933.4853363037109, minimum ratio: 0.78515625\n",
      "Epoch [1425], val_loss: 5677.3496\n",
      "gradient norm: 935.1043701171875, minimum ratio: 0.76171875\n",
      "Epoch [1426], val_loss: 5691.6548\n",
      "gradient norm: 936.7179870605469, minimum ratio: 0.7734375\n",
      "Epoch [1427], val_loss: 5705.9844\n",
      "gradient norm: 938.3383865356445, minimum ratio: 0.76171875\n",
      "Epoch [1428], val_loss: 5720.3457\n",
      "gradient norm: 939.9567794799805, minimum ratio: 0.7734375\n",
      "Epoch [1429], val_loss: 5734.7285\n",
      "gradient norm: 941.5729370117188, minimum ratio: 0.7578125\n",
      "Epoch [1430], val_loss: 5749.1401\n",
      "gradient norm: 943.2158279418945, minimum ratio: 0.76953125\n",
      "Epoch [1431], val_loss: 5763.5728\n",
      "gradient norm: 944.8538131713867, minimum ratio: 0.74609375\n",
      "Epoch [1432], val_loss: 5778.0337\n",
      "gradient norm: 946.4909820556641, minimum ratio: 0.7734375\n",
      "Epoch [1433], val_loss: 5792.5259\n",
      "gradient norm: 948.1388092041016, minimum ratio: 0.765625\n",
      "Epoch [1434], val_loss: 5807.0459\n",
      "gradient norm: 949.7978439331055, minimum ratio: 0.77734375\n",
      "Epoch [1435], val_loss: 5821.5894\n",
      "gradient norm: 951.4501571655273, minimum ratio: 0.75390625\n",
      "Epoch [1436], val_loss: 5836.1602\n",
      "gradient norm: 953.1098403930664, minimum ratio: 0.76953125\n",
      "Epoch [1437], val_loss: 5850.7573\n",
      "gradient norm: 954.7161102294922, minimum ratio: 0.77734375\n",
      "Epoch [1438], val_loss: 5865.3828\n",
      "gradient norm: 956.3528594970703, minimum ratio: 0.74609375\n",
      "Epoch [1439], val_loss: 5880.0308\n",
      "gradient norm: 958.0196685791016, minimum ratio: 0.78515625\n",
      "Epoch [1440], val_loss: 5894.7026\n",
      "gradient norm: 959.6878433227539, minimum ratio: 0.76953125\n",
      "Epoch [1441], val_loss: 5909.4033\n",
      "gradient norm: 961.3395614624023, minimum ratio: 0.73828125\n",
      "Epoch [1442], val_loss: 5924.1318\n",
      "gradient norm: 962.9401550292969, minimum ratio: 0.765625\n",
      "Epoch [1443], val_loss: 5938.8843\n",
      "gradient norm: 964.6022567749023, minimum ratio: 0.78125\n",
      "Epoch [1444], val_loss: 5953.6602\n",
      "gradient norm: 966.243522644043, minimum ratio: 0.75\n",
      "Epoch [1445], val_loss: 5968.4590\n",
      "gradient norm: 967.8859405517578, minimum ratio: 0.7734375\n",
      "Epoch [1446], val_loss: 5983.2866\n",
      "gradient norm: 969.5328598022461, minimum ratio: 0.75390625\n",
      "Epoch [1447], val_loss: 5998.1396\n",
      "gradient norm: 971.1972427368164, minimum ratio: 0.76953125\n",
      "Epoch [1448], val_loss: 6013.0181\n",
      "gradient norm: 972.8782272338867, minimum ratio: 0.76171875\n",
      "Epoch [1449], val_loss: 6027.9219\n",
      "gradient norm: 974.5348968505859, minimum ratio: 0.78125\n",
      "Epoch [1450], val_loss: 6042.8477\n",
      "gradient norm: 976.1901397705078, minimum ratio: 0.7734375\n",
      "Epoch [1451], val_loss: 6057.8022\n",
      "gradient norm: 977.8492050170898, minimum ratio: 0.75390625\n",
      "Epoch [1452], val_loss: 6072.7856\n",
      "gradient norm: 979.4783935546875, minimum ratio: 0.7890625\n",
      "Epoch [1453], val_loss: 6087.7944\n",
      "gradient norm: 981.1569442749023, minimum ratio: 0.79296875\n",
      "Epoch [1454], val_loss: 6102.8350\n",
      "gradient norm: 982.8019027709961, minimum ratio: 0.79296875\n",
      "Epoch [1455], val_loss: 6117.9009\n",
      "gradient norm: 984.4900665283203, minimum ratio: 0.7578125\n",
      "Epoch [1456], val_loss: 6132.9956\n",
      "gradient norm: 986.1728668212891, minimum ratio: 0.75\n",
      "Epoch [1457], val_loss: 6148.1138\n",
      "gradient norm: 987.8585815429688, minimum ratio: 0.8046875\n",
      "Epoch [1458], val_loss: 6163.2598\n",
      "gradient norm: 989.5315093994141, minimum ratio: 0.76171875\n",
      "Epoch [1459], val_loss: 6178.4307\n",
      "gradient norm: 991.2064361572266, minimum ratio: 0.77734375\n",
      "Epoch [1460], val_loss: 6193.6323\n",
      "gradient norm: 992.9092864990234, minimum ratio: 0.76171875\n",
      "Epoch [1461], val_loss: 6208.8633\n",
      "gradient norm: 994.58642578125, minimum ratio: 0.75390625\n",
      "Epoch [1462], val_loss: 6224.1172\n",
      "gradient norm: 996.2927780151367, minimum ratio: 0.77734375\n",
      "Epoch [1463], val_loss: 6239.3984\n",
      "gradient norm: 997.9878616333008, minimum ratio: 0.75\n",
      "Epoch [1464], val_loss: 6254.7051\n",
      "gradient norm: 999.6862030029297, minimum ratio: 0.79296875\n",
      "Epoch [1465], val_loss: 6270.0405\n",
      "gradient norm: 1001.3977584838867, minimum ratio: 0.765625\n",
      "Epoch [1466], val_loss: 6285.4019\n",
      "gradient norm: 1003.0555572509766, minimum ratio: 0.7734375\n",
      "Epoch [1467], val_loss: 6300.7915\n",
      "gradient norm: 1004.7299194335938, minimum ratio: 0.78125\n",
      "Epoch [1468], val_loss: 6316.2100\n",
      "gradient norm: 1006.4183197021484, minimum ratio: 0.77734375\n",
      "Epoch [1469], val_loss: 6331.6533\n",
      "gradient norm: 1008.1257171630859, minimum ratio: 0.7890625\n",
      "Epoch [1470], val_loss: 6347.1240\n",
      "gradient norm: 1009.8461380004883, minimum ratio: 0.7578125\n",
      "Epoch [1471], val_loss: 6362.6230\n",
      "gradient norm: 1011.4967269897461, minimum ratio: 0.7734375\n",
      "Epoch [1472], val_loss: 6378.1484\n",
      "gradient norm: 1013.2161712646484, minimum ratio: 0.75390625\n",
      "Epoch [1473], val_loss: 6393.7002\n",
      "gradient norm: 1014.9136047363281, minimum ratio: 0.765625\n",
      "Epoch [1474], val_loss: 6409.2783\n",
      "gradient norm: 1016.6408309936523, minimum ratio: 0.7578125\n",
      "Epoch [1475], val_loss: 6424.8823\n",
      "gradient norm: 1018.3516845703125, minimum ratio: 0.76953125\n",
      "Epoch [1476], val_loss: 6440.5127\n",
      "gradient norm: 1020.0532836914062, minimum ratio: 0.76953125\n",
      "Epoch [1477], val_loss: 6456.1748\n",
      "gradient norm: 1021.6980590820312, minimum ratio: 0.7734375\n",
      "Epoch [1478], val_loss: 6471.8594\n",
      "gradient norm: 1023.3922500610352, minimum ratio: 0.7578125\n",
      "Epoch [1479], val_loss: 6487.5718\n",
      "gradient norm: 1025.116600036621, minimum ratio: 0.76953125\n",
      "Epoch [1480], val_loss: 6503.3140\n",
      "gradient norm: 1026.833122253418, minimum ratio: 0.77734375\n",
      "Epoch [1481], val_loss: 6519.0825\n",
      "gradient norm: 1028.5453338623047, minimum ratio: 0.76953125\n",
      "Epoch [1482], val_loss: 6534.8804\n",
      "gradient norm: 1030.2705841064453, minimum ratio: 0.765625\n",
      "Epoch [1483], val_loss: 6550.7056\n",
      "gradient norm: 1032.0138244628906, minimum ratio: 0.77734375\n",
      "Epoch [1484], val_loss: 6566.5581\n",
      "gradient norm: 1033.7436904907227, minimum ratio: 0.78515625\n",
      "Epoch [1485], val_loss: 6582.4360\n",
      "gradient norm: 1035.4900512695312, minimum ratio: 0.75390625\n",
      "Epoch [1486], val_loss: 6598.3423\n",
      "gradient norm: 1037.2268524169922, minimum ratio: 0.76953125\n",
      "Epoch [1487], val_loss: 6614.2773\n",
      "gradient norm: 1038.962760925293, minimum ratio: 0.7734375\n",
      "Epoch [1488], val_loss: 6630.2393\n",
      "gradient norm: 1040.7020263671875, minimum ratio: 0.765625\n",
      "Epoch [1489], val_loss: 6646.2305\n",
      "gradient norm: 1042.4163970947266, minimum ratio: 0.78515625\n",
      "Epoch [1490], val_loss: 6662.2485\n",
      "gradient norm: 1044.146110534668, minimum ratio: 0.765625\n",
      "Epoch [1491], val_loss: 6678.2925\n",
      "gradient norm: 1045.8538436889648, minimum ratio: 0.80078125\n",
      "Epoch [1492], val_loss: 6694.3618\n",
      "gradient norm: 1047.6121139526367, minimum ratio: 0.765625\n",
      "Epoch [1493], val_loss: 6710.4634\n",
      "gradient norm: 1049.335838317871, minimum ratio: 0.76171875\n",
      "Epoch [1494], val_loss: 6726.5884\n",
      "gradient norm: 1051.068130493164, minimum ratio: 0.7734375\n",
      "Epoch [1495], val_loss: 6742.7471\n",
      "gradient norm: 1052.8072128295898, minimum ratio: 0.79296875\n",
      "Epoch [1496], val_loss: 6758.9331\n",
      "gradient norm: 1054.545036315918, minimum ratio: 0.765625\n",
      "Epoch [1497], val_loss: 6775.1440\n",
      "gradient norm: 1056.2701721191406, minimum ratio: 0.81640625\n",
      "Epoch [1498], val_loss: 6791.3843\n",
      "gradient norm: 1058.0272750854492, minimum ratio: 0.75390625\n",
      "Epoch [1499], val_loss: 6807.6543\n",
      "gradient norm: 1059.7504119873047, minimum ratio: 0.7578125\n",
      "Epoch [1500], val_loss: 6823.9507\n",
      "gradient norm: 1061.5037841796875, minimum ratio: 0.77734375\n",
      "Epoch [1501], val_loss: 6840.2759\n",
      "gradient norm: 1063.2661437988281, minimum ratio: 0.76171875\n",
      "Epoch [1502], val_loss: 6856.6309\n",
      "gradient norm: 1065.022087097168, minimum ratio: 0.78125\n",
      "Epoch [1503], val_loss: 6873.0103\n",
      "gradient norm: 1066.7664947509766, minimum ratio: 0.76953125\n",
      "Epoch [1504], val_loss: 6889.4185\n",
      "gradient norm: 1068.5090103149414, minimum ratio: 0.7421875\n",
      "Epoch [1505], val_loss: 6905.8540\n",
      "gradient norm: 1070.2760467529297, minimum ratio: 0.76953125\n",
      "Epoch [1506], val_loss: 6922.3169\n",
      "gradient norm: 1072.0452041625977, minimum ratio: 0.77734375\n",
      "Epoch [1507], val_loss: 6938.8091\n",
      "gradient norm: 1073.8123016357422, minimum ratio: 0.78125\n",
      "Epoch [1508], val_loss: 6955.3286\n",
      "gradient norm: 1075.5764541625977, minimum ratio: 0.77734375\n",
      "Epoch [1509], val_loss: 6971.8750\n",
      "gradient norm: 1077.3486251831055, minimum ratio: 0.7734375\n",
      "Epoch [1510], val_loss: 6988.4507\n",
      "gradient norm: 1079.1209182739258, minimum ratio: 0.76171875\n",
      "Epoch [1511], val_loss: 7005.0552\n",
      "gradient norm: 1080.896957397461, minimum ratio: 0.75\n",
      "Epoch [1512], val_loss: 7021.6851\n",
      "gradient norm: 1082.691032409668, minimum ratio: 0.765625\n",
      "Epoch [1513], val_loss: 7038.3423\n",
      "gradient norm: 1084.4767684936523, minimum ratio: 0.77734375\n",
      "Epoch [1514], val_loss: 7055.0327\n",
      "gradient norm: 1086.2575607299805, minimum ratio: 0.7734375\n",
      "Epoch [1515], val_loss: 7071.7500\n",
      "gradient norm: 1088.0319595336914, minimum ratio: 0.76171875\n",
      "Epoch [1516], val_loss: 7088.4956\n",
      "gradient norm: 1089.7686004638672, minimum ratio: 0.75\n",
      "Epoch [1517], val_loss: 7105.2710\n",
      "gradient norm: 1091.5564498901367, minimum ratio: 0.76953125\n",
      "Epoch [1518], val_loss: 7122.0718\n",
      "gradient norm: 1093.3373718261719, minimum ratio: 0.76953125\n",
      "Epoch [1519], val_loss: 7138.9067\n",
      "gradient norm: 1095.1327285766602, minimum ratio: 0.765625\n",
      "Epoch [1520], val_loss: 7155.7690\n",
      "gradient norm: 1096.866325378418, minimum ratio: 0.76953125\n",
      "Epoch [1521], val_loss: 7172.6626\n",
      "gradient norm: 1098.6740036010742, minimum ratio: 0.76953125\n",
      "Epoch [1522], val_loss: 7189.5859\n",
      "gradient norm: 1100.467414855957, minimum ratio: 0.76171875\n",
      "Epoch [1523], val_loss: 7206.5317\n",
      "gradient norm: 1102.2437286376953, minimum ratio: 0.76953125\n",
      "Epoch [1524], val_loss: 7223.5117\n",
      "gradient norm: 1104.0101928710938, minimum ratio: 0.7734375\n",
      "Epoch [1525], val_loss: 7240.5176\n",
      "gradient norm: 1105.8288040161133, minimum ratio: 0.78125\n",
      "Epoch [1526], val_loss: 7257.5532\n",
      "gradient norm: 1107.6355209350586, minimum ratio: 0.78125\n",
      "Epoch [1527], val_loss: 7274.6157\n",
      "gradient norm: 1109.4144744873047, minimum ratio: 0.77734375\n",
      "Epoch [1528], val_loss: 7291.7065\n",
      "gradient norm: 1111.208641052246, minimum ratio: 0.76171875\n",
      "Epoch [1529], val_loss: 7308.8198\n",
      "gradient norm: 1113.0087890625, minimum ratio: 0.76171875\n",
      "Epoch [1530], val_loss: 7325.9644\n",
      "gradient norm: 1114.815658569336, minimum ratio: 0.76953125\n",
      "Epoch [1531], val_loss: 7343.1392\n",
      "gradient norm: 1116.6229629516602, minimum ratio: 0.75\n",
      "Epoch [1532], val_loss: 7360.3433\n",
      "gradient norm: 1118.4063720703125, minimum ratio: 0.7734375\n",
      "Epoch [1533], val_loss: 7377.5776\n",
      "gradient norm: 1120.2335739135742, minimum ratio: 0.796875\n",
      "Epoch [1534], val_loss: 7394.8394\n",
      "gradient norm: 1122.0343399047852, minimum ratio: 0.77734375\n",
      "Epoch [1535], val_loss: 7412.1318\n",
      "gradient norm: 1123.8581466674805, minimum ratio: 0.76953125\n",
      "Epoch [1536], val_loss: 7429.4507\n",
      "gradient norm: 1125.696029663086, minimum ratio: 0.77734375\n",
      "Epoch [1537], val_loss: 7446.7998\n",
      "gradient norm: 1127.512336730957, minimum ratio: 0.7734375\n",
      "Epoch [1538], val_loss: 7464.1768\n",
      "gradient norm: 1129.3353576660156, minimum ratio: 0.75390625\n",
      "Epoch [1539], val_loss: 7481.5825\n",
      "gradient norm: 1131.1394348144531, minimum ratio: 0.765625\n",
      "Epoch [1540], val_loss: 7499.0176\n",
      "gradient norm: 1132.952880859375, minimum ratio: 0.75\n",
      "Epoch [1541], val_loss: 7516.4800\n",
      "gradient norm: 1134.7818603515625, minimum ratio: 0.76953125\n",
      "Epoch [1542], val_loss: 7533.9707\n",
      "gradient norm: 1136.602653503418, minimum ratio: 0.765625\n",
      "Epoch [1543], val_loss: 7551.4917\n",
      "gradient norm: 1138.4203033447266, minimum ratio: 0.7578125\n",
      "Epoch [1544], val_loss: 7569.0366\n",
      "gradient norm: 1140.2549362182617, minimum ratio: 0.765625\n",
      "Epoch [1545], val_loss: 7586.6099\n",
      "gradient norm: 1142.0756607055664, minimum ratio: 0.7890625\n",
      "Epoch [1546], val_loss: 7604.2134\n",
      "gradient norm: 1143.890640258789, minimum ratio: 0.76171875\n",
      "Epoch [1547], val_loss: 7621.8481\n",
      "gradient norm: 1145.72998046875, minimum ratio: 0.78515625\n",
      "Epoch [1548], val_loss: 7639.5107\n",
      "gradient norm: 1147.518196105957, minimum ratio: 0.765625\n",
      "Epoch [1549], val_loss: 7657.2041\n",
      "gradient norm: 1149.3478393554688, minimum ratio: 0.765625\n",
      "Epoch [1550], val_loss: 7674.9224\n",
      "gradient norm: 1151.1863250732422, minimum ratio: 0.76171875\n",
      "Epoch [1551], val_loss: 7692.6719\n",
      "gradient norm: 1153.0075225830078, minimum ratio: 0.765625\n",
      "Epoch [1552], val_loss: 7710.4482\n",
      "gradient norm: 1154.8717803955078, minimum ratio: 0.76953125\n",
      "Epoch [1553], val_loss: 7728.2568\n",
      "gradient norm: 1156.7145385742188, minimum ratio: 0.7734375\n",
      "Epoch [1554], val_loss: 7746.0967\n",
      "gradient norm: 1158.56689453125, minimum ratio: 0.76171875\n",
      "Epoch [1555], val_loss: 7763.9644\n",
      "gradient norm: 1160.4230041503906, minimum ratio: 0.765625\n",
      "Epoch [1556], val_loss: 7781.8633\n",
      "gradient norm: 1162.2727355957031, minimum ratio: 0.78125\n",
      "Epoch [1557], val_loss: 7799.7876\n",
      "gradient norm: 1164.1067276000977, minimum ratio: 0.78125\n",
      "Epoch [1558], val_loss: 7817.7456\n",
      "gradient norm: 1165.966667175293, minimum ratio: 0.765625\n",
      "Epoch [1559], val_loss: 7835.7368\n",
      "gradient norm: 1167.845718383789, minimum ratio: 0.79296875\n",
      "Epoch [1560], val_loss: 7853.7534\n",
      "gradient norm: 1169.6903915405273, minimum ratio: 0.75390625\n",
      "Epoch [1561], val_loss: 7871.7993\n",
      "gradient norm: 1171.5576934814453, minimum ratio: 0.78125\n",
      "Epoch [1562], val_loss: 7889.8750\n",
      "gradient norm: 1173.3914489746094, minimum ratio: 0.77734375\n",
      "Epoch [1563], val_loss: 7907.9824\n",
      "gradient norm: 1175.2214736938477, minimum ratio: 0.76953125\n",
      "Epoch [1564], val_loss: 7926.1216\n",
      "gradient norm: 1177.1091079711914, minimum ratio: 0.78125\n",
      "Epoch [1565], val_loss: 7944.2935\n",
      "gradient norm: 1178.9610137939453, minimum ratio: 0.78125\n",
      "Epoch [1566], val_loss: 7962.4932\n",
      "gradient norm: 1180.8471603393555, minimum ratio: 0.75\n",
      "Epoch [1567], val_loss: 7980.7231\n",
      "gradient norm: 1182.7273483276367, minimum ratio: 0.7734375\n",
      "Epoch [1568], val_loss: 7998.9844\n",
      "gradient norm: 1184.5945663452148, minimum ratio: 0.7578125\n",
      "Epoch [1569], val_loss: 8017.2773\n",
      "gradient norm: 1186.483169555664, minimum ratio: 0.77734375\n",
      "Epoch [1570], val_loss: 8035.6016\n",
      "gradient norm: 1188.3420639038086, minimum ratio: 0.75390625\n",
      "Epoch [1571], val_loss: 8053.9517\n",
      "gradient norm: 1190.2062606811523, minimum ratio: 0.765625\n",
      "Epoch [1572], val_loss: 8072.3335\n",
      "gradient norm: 1192.0921173095703, minimum ratio: 0.78125\n",
      "Epoch [1573], val_loss: 8090.7417\n",
      "gradient norm: 1193.9552764892578, minimum ratio: 0.77734375\n",
      "Epoch [1574], val_loss: 8109.1792\n",
      "gradient norm: 1195.8561477661133, minimum ratio: 0.80078125\n",
      "Epoch [1575], val_loss: 8127.6475\n",
      "gradient norm: 1197.7498245239258, minimum ratio: 0.765625\n",
      "Epoch [1576], val_loss: 8146.1440\n",
      "gradient norm: 1199.6104202270508, minimum ratio: 0.76171875\n",
      "Epoch [1577], val_loss: 8164.6719\n",
      "gradient norm: 1201.4534759521484, minimum ratio: 0.76953125\n",
      "Epoch [1578], val_loss: 8183.2285\n",
      "gradient norm: 1203.3335876464844, minimum ratio: 0.7734375\n",
      "Epoch [1579], val_loss: 8201.8154\n",
      "gradient norm: 1205.2030792236328, minimum ratio: 0.75390625\n",
      "Epoch [1580], val_loss: 8220.4326\n",
      "gradient norm: 1207.074691772461, minimum ratio: 0.7421875\n",
      "Epoch [1581], val_loss: 8239.0781\n",
      "gradient norm: 1208.9584350585938, minimum ratio: 0.75390625\n",
      "Epoch [1582], val_loss: 8257.7549\n",
      "gradient norm: 1210.8777923583984, minimum ratio: 0.8125\n",
      "Epoch [1583], val_loss: 8276.4570\n",
      "gradient norm: 1212.779525756836, minimum ratio: 0.76953125\n",
      "Epoch [1584], val_loss: 8295.1943\n",
      "gradient norm: 1214.7026901245117, minimum ratio: 0.78515625\n",
      "Epoch [1585], val_loss: 8313.9619\n",
      "gradient norm: 1216.5679931640625, minimum ratio: 0.79296875\n",
      "Epoch [1586], val_loss: 8332.7617\n",
      "gradient norm: 1218.4323959350586, minimum ratio: 0.7578125\n",
      "Epoch [1587], val_loss: 8351.5898\n",
      "gradient norm: 1220.3614807128906, minimum ratio: 0.74609375\n",
      "Epoch [1588], val_loss: 8370.4502\n",
      "gradient norm: 1222.2923431396484, minimum ratio: 0.7734375\n",
      "Epoch [1589], val_loss: 8389.3418\n",
      "gradient norm: 1224.181785583496, minimum ratio: 0.765625\n",
      "Epoch [1590], val_loss: 8408.2637\n",
      "gradient norm: 1226.1167831420898, minimum ratio: 0.76953125\n",
      "Epoch [1591], val_loss: 8427.2168\n",
      "gradient norm: 1228.0322036743164, minimum ratio: 0.7734375\n",
      "Epoch [1592], val_loss: 8446.1992\n",
      "gradient norm: 1229.970962524414, minimum ratio: 0.7734375\n",
      "Epoch [1593], val_loss: 8465.2158\n",
      "gradient norm: 1231.8789291381836, minimum ratio: 0.81640625\n",
      "Epoch [1594], val_loss: 8484.2617\n",
      "gradient norm: 1233.796859741211, minimum ratio: 0.76953125\n",
      "Epoch [1595], val_loss: 8503.3418\n",
      "gradient norm: 1235.7120971679688, minimum ratio: 0.7578125\n",
      "Epoch [1596], val_loss: 8522.4561\n",
      "gradient norm: 1237.6015090942383, minimum ratio: 0.76171875\n",
      "Epoch [1597], val_loss: 8541.5986\n",
      "gradient norm: 1239.5312957763672, minimum ratio: 0.75\n",
      "Epoch [1598], val_loss: 8560.7676\n",
      "gradient norm: 1241.4105758666992, minimum ratio: 0.765625\n",
      "Epoch [1599], val_loss: 8579.9668\n",
      "gradient norm: 1243.3396911621094, minimum ratio: 0.76953125\n",
      "Epoch [1600], val_loss: 8599.1943\n",
      "gradient norm: 1245.2928924560547, minimum ratio: 0.76171875\n",
      "Epoch [1601], val_loss: 8618.4570\n",
      "gradient norm: 1247.2060317993164, minimum ratio: 0.765625\n",
      "Epoch [1602], val_loss: 8637.7510\n",
      "gradient norm: 1249.1436157226562, minimum ratio: 0.75\n",
      "Epoch [1603], val_loss: 8657.0771\n",
      "gradient norm: 1251.1036071777344, minimum ratio: 0.75390625\n",
      "Epoch [1604], val_loss: 8676.4336\n",
      "gradient norm: 1253.0425872802734, minimum ratio: 0.7578125\n",
      "Epoch [1605], val_loss: 8695.8213\n",
      "gradient norm: 1254.954002380371, minimum ratio: 0.77734375\n",
      "Epoch [1606], val_loss: 8715.2383\n",
      "gradient norm: 1256.9051895141602, minimum ratio: 0.76953125\n",
      "Epoch [1607], val_loss: 8734.6865\n",
      "gradient norm: 1258.8073272705078, minimum ratio: 0.78515625\n",
      "Epoch [1608], val_loss: 8754.1680\n",
      "gradient norm: 1260.741439819336, minimum ratio: 0.765625\n",
      "Epoch [1609], val_loss: 8773.6797\n",
      "gradient norm: 1262.6983032226562, minimum ratio: 0.74609375\n",
      "Epoch [1610], val_loss: 8793.2285\n",
      "gradient norm: 1264.6163940429688, minimum ratio: 0.7890625\n",
      "Epoch [1611], val_loss: 8812.8066\n",
      "gradient norm: 1266.5584030151367, minimum ratio: 0.76171875\n",
      "Epoch [1612], val_loss: 8832.4219\n",
      "gradient norm: 1268.5094451904297, minimum ratio: 0.78125\n",
      "Epoch [1613], val_loss: 8852.0615\n",
      "gradient norm: 1270.4888610839844, minimum ratio: 0.77734375\n",
      "Epoch [1614], val_loss: 8871.7354\n",
      "gradient norm: 1272.4466857910156, minimum ratio: 0.76953125\n",
      "Epoch [1615], val_loss: 8891.4453\n",
      "gradient norm: 1274.431640625, minimum ratio: 0.7578125\n",
      "Epoch [1616], val_loss: 8911.1855\n",
      "gradient norm: 1276.4182357788086, minimum ratio: 0.765625\n",
      "Epoch [1617], val_loss: 8930.9580\n",
      "gradient norm: 1278.3762283325195, minimum ratio: 0.7578125\n",
      "Epoch [1618], val_loss: 8950.7588\n",
      "gradient norm: 1280.2851028442383, minimum ratio: 0.75\n",
      "Epoch [1619], val_loss: 8970.5938\n",
      "gradient norm: 1282.2312088012695, minimum ratio: 0.76171875\n",
      "Epoch [1620], val_loss: 8990.4600\n",
      "gradient norm: 1284.2249069213867, minimum ratio: 0.74609375\n",
      "Epoch [1621], val_loss: 9010.3604\n",
      "gradient norm: 1286.2210388183594, minimum ratio: 0.76953125\n",
      "Epoch [1622], val_loss: 9030.2871\n",
      "gradient norm: 1288.1867370605469, minimum ratio: 0.7734375\n",
      "Epoch [1623], val_loss: 9050.2471\n",
      "gradient norm: 1290.166000366211, minimum ratio: 0.74609375\n",
      "Epoch [1624], val_loss: 9070.2412\n",
      "gradient norm: 1292.1451263427734, minimum ratio: 0.78125\n",
      "Epoch [1625], val_loss: 9090.2666\n",
      "gradient norm: 1294.1234130859375, minimum ratio: 0.76953125\n",
      "Epoch [1626], val_loss: 9110.3242\n",
      "gradient norm: 1296.0886917114258, minimum ratio: 0.73828125\n",
      "Epoch [1627], val_loss: 9130.4121\n",
      "gradient norm: 1298.0739822387695, minimum ratio: 0.75\n",
      "Epoch [1628], val_loss: 9150.5312\n",
      "gradient norm: 1300.0633544921875, minimum ratio: 0.76171875\n",
      "Epoch [1629], val_loss: 9170.6816\n",
      "gradient norm: 1302.05126953125, minimum ratio: 0.78125\n",
      "Epoch [1630], val_loss: 9190.8652\n",
      "gradient norm: 1303.9645919799805, minimum ratio: 0.75\n",
      "Epoch [1631], val_loss: 9211.0703\n",
      "gradient norm: 1305.957389831543, minimum ratio: 0.77734375\n",
      "Epoch [1632], val_loss: 9231.3057\n",
      "gradient norm: 1307.9243698120117, minimum ratio: 0.77734375\n",
      "Epoch [1633], val_loss: 9251.5781\n",
      "gradient norm: 1309.9191360473633, minimum ratio: 0.75\n",
      "Epoch [1634], val_loss: 9271.8760\n",
      "gradient norm: 1311.8651504516602, minimum ratio: 0.765625\n",
      "Epoch [1635], val_loss: 9292.2070\n",
      "gradient norm: 1313.8026123046875, minimum ratio: 0.7578125\n",
      "Epoch [1636], val_loss: 9312.5674\n",
      "gradient norm: 1315.7751922607422, minimum ratio: 0.75390625\n",
      "Epoch [1637], val_loss: 9332.9609\n",
      "gradient norm: 1317.7738876342773, minimum ratio: 0.7421875\n",
      "Epoch [1638], val_loss: 9353.3828\n",
      "gradient norm: 1319.763412475586, minimum ratio: 0.7890625\n",
      "Epoch [1639], val_loss: 9373.8389\n",
      "gradient norm: 1321.7793350219727, minimum ratio: 0.765625\n",
      "Epoch [1640], val_loss: 9394.3291\n",
      "gradient norm: 1323.8074493408203, minimum ratio: 0.76171875\n",
      "Epoch [1641], val_loss: 9414.8467\n",
      "gradient norm: 1325.8367156982422, minimum ratio: 0.7578125\n",
      "Epoch [1642], val_loss: 9435.3975\n",
      "gradient norm: 1327.8431243896484, minimum ratio: 0.75390625\n",
      "Epoch [1643], val_loss: 9455.9766\n",
      "gradient norm: 1329.8548202514648, minimum ratio: 0.76171875\n",
      "Epoch [1644], val_loss: 9476.5879\n",
      "gradient norm: 1331.863914489746, minimum ratio: 0.7734375\n",
      "Epoch [1645], val_loss: 9497.2305\n",
      "gradient norm: 1333.847557067871, minimum ratio: 0.77734375\n",
      "Epoch [1646], val_loss: 9517.9062\n",
      "gradient norm: 1335.8643493652344, minimum ratio: 0.7734375\n",
      "Epoch [1647], val_loss: 9538.6113\n",
      "gradient norm: 1337.9035568237305, minimum ratio: 0.78125\n",
      "Epoch [1648], val_loss: 9559.3467\n",
      "gradient norm: 1339.9230728149414, minimum ratio: 0.796875\n",
      "Epoch [1649], val_loss: 9580.1123\n",
      "gradient norm: 1341.9256896972656, minimum ratio: 0.76171875\n",
      "Epoch [1650], val_loss: 9600.9072\n",
      "gradient norm: 1343.9244384765625, minimum ratio: 0.78125\n",
      "Epoch [1651], val_loss: 9621.7402\n",
      "gradient norm: 1345.9373474121094, minimum ratio: 0.73828125\n",
      "Epoch [1652], val_loss: 9642.6055\n",
      "gradient norm: 1347.936538696289, minimum ratio: 0.75\n",
      "Epoch [1653], val_loss: 9663.5049\n",
      "gradient norm: 1349.987319946289, minimum ratio: 0.75390625\n",
      "Epoch [1654], val_loss: 9684.4365\n",
      "gradient norm: 1352.008529663086, minimum ratio: 0.75390625\n",
      "Epoch [1655], val_loss: 9705.4004\n",
      "gradient norm: 1354.0438385009766, minimum ratio: 0.74609375\n",
      "Epoch [1656], val_loss: 9726.4004\n",
      "gradient norm: 1356.0843200683594, minimum ratio: 0.77734375\n",
      "Epoch [1657], val_loss: 9747.4248\n",
      "gradient norm: 1358.126937866211, minimum ratio: 0.78125\n",
      "Epoch [1658], val_loss: 9768.4814\n",
      "gradient norm: 1360.1761016845703, minimum ratio: 0.76953125\n",
      "Epoch [1659], val_loss: 9789.5742\n",
      "gradient norm: 1362.2286987304688, minimum ratio: 0.7734375\n",
      "Epoch [1660], val_loss: 9810.6953\n",
      "gradient norm: 1364.2452697753906, minimum ratio: 0.78125\n",
      "Epoch [1661], val_loss: 9831.8555\n",
      "gradient norm: 1366.3011627197266, minimum ratio: 0.77734375\n",
      "Epoch [1662], val_loss: 9853.0479\n",
      "gradient norm: 1368.3041687011719, minimum ratio: 0.77734375\n",
      "Epoch [1663], val_loss: 9874.2754\n",
      "gradient norm: 1370.2952270507812, minimum ratio: 0.75390625\n",
      "Epoch [1664], val_loss: 9895.5312\n",
      "gradient norm: 1372.3392181396484, minimum ratio: 0.7890625\n",
      "Epoch [1665], val_loss: 9916.8213\n",
      "gradient norm: 1374.3665771484375, minimum ratio: 0.79296875\n",
      "Epoch [1666], val_loss: 9938.1465\n",
      "gradient norm: 1376.381362915039, minimum ratio: 0.7734375\n",
      "Epoch [1667], val_loss: 9959.5029\n",
      "gradient norm: 1378.4588928222656, minimum ratio: 0.74609375\n",
      "Epoch [1668], val_loss: 9980.8936\n",
      "gradient norm: 1380.5124206542969, minimum ratio: 0.76171875\n",
      "Epoch [1669], val_loss: 10002.3115\n",
      "gradient norm: 1382.5745849609375, minimum ratio: 0.80859375\n",
      "Epoch [1670], val_loss: 10023.7686\n",
      "gradient norm: 1384.6393127441406, minimum ratio: 0.78125\n",
      "Epoch [1671], val_loss: 10045.2559\n",
      "gradient norm: 1386.694076538086, minimum ratio: 0.7578125\n",
      "Epoch [1672], val_loss: 10066.7764\n",
      "gradient norm: 1388.7808227539062, minimum ratio: 0.78125\n",
      "Epoch [1673], val_loss: 10088.3271\n",
      "gradient norm: 1390.8266296386719, minimum ratio: 0.76953125\n",
      "Epoch [1674], val_loss: 10109.9121\n",
      "gradient norm: 1392.8453369140625, minimum ratio: 0.765625\n",
      "Epoch [1675], val_loss: 10131.5293\n",
      "gradient norm: 1394.93701171875, minimum ratio: 0.76171875\n",
      "Epoch [1676], val_loss: 10153.1807\n",
      "gradient norm: 1397.0089416503906, minimum ratio: 0.77734375\n",
      "Epoch [1677], val_loss: 10174.8652\n",
      "gradient norm: 1399.053466796875, minimum ratio: 0.7421875\n",
      "Epoch [1678], val_loss: 10196.5801\n",
      "gradient norm: 1401.0941772460938, minimum ratio: 0.765625\n",
      "Epoch [1679], val_loss: 10218.3350\n",
      "gradient norm: 1403.0845489501953, minimum ratio: 0.76953125\n",
      "Epoch [1680], val_loss: 10240.1201\n",
      "gradient norm: 1405.1251678466797, minimum ratio: 0.78515625\n",
      "Epoch [1681], val_loss: 10261.9434\n",
      "gradient norm: 1407.180191040039, minimum ratio: 0.7578125\n",
      "Epoch [1682], val_loss: 10283.7998\n",
      "gradient norm: 1409.2648315429688, minimum ratio: 0.76171875\n",
      "Epoch [1683], val_loss: 10305.6875\n",
      "gradient norm: 1411.372787475586, minimum ratio: 0.7578125\n",
      "Epoch [1684], val_loss: 10327.6104\n",
      "gradient norm: 1413.4592895507812, minimum ratio: 0.78125\n",
      "Epoch [1685], val_loss: 10349.5654\n",
      "gradient norm: 1415.5711822509766, minimum ratio: 0.78125\n",
      "Epoch [1686], val_loss: 10371.5557\n",
      "gradient norm: 1417.6480865478516, minimum ratio: 0.76171875\n",
      "Epoch [1687], val_loss: 10393.5732\n",
      "gradient norm: 1419.7488708496094, minimum ratio: 0.78515625\n",
      "Epoch [1688], val_loss: 10415.6250\n",
      "gradient norm: 1421.8478393554688, minimum ratio: 0.75\n",
      "Epoch [1689], val_loss: 10437.7129\n",
      "gradient norm: 1423.966796875, minimum ratio: 0.7578125\n",
      "Epoch [1690], val_loss: 10459.8350\n",
      "gradient norm: 1426.0880432128906, minimum ratio: 0.79296875\n",
      "Epoch [1691], val_loss: 10481.9912\n",
      "gradient norm: 1428.1294403076172, minimum ratio: 0.7734375\n",
      "Epoch [1692], val_loss: 10504.1807\n",
      "gradient norm: 1430.2064361572266, minimum ratio: 0.76953125\n",
      "Epoch [1693], val_loss: 10526.4082\n",
      "gradient norm: 1432.3037109375, minimum ratio: 0.73828125\n",
      "Epoch [1694], val_loss: 10548.6689\n",
      "gradient norm: 1434.41259765625, minimum ratio: 0.76953125\n",
      "Epoch [1695], val_loss: 10570.9648\n",
      "gradient norm: 1436.4945068359375, minimum ratio: 0.76953125\n",
      "Epoch [1696], val_loss: 10593.2939\n",
      "gradient norm: 1438.628677368164, minimum ratio: 0.7734375\n",
      "Epoch [1697], val_loss: 10615.6523\n",
      "gradient norm: 1440.7317352294922, minimum ratio: 0.78125\n",
      "Epoch [1698], val_loss: 10638.0479\n",
      "gradient norm: 1442.8444061279297, minimum ratio: 0.734375\n",
      "Epoch [1699], val_loss: 10660.4717\n",
      "gradient norm: 1444.9154510498047, minimum ratio: 0.765625\n",
      "Epoch [1700], val_loss: 10682.9277\n",
      "gradient norm: 1447.0155487060547, minimum ratio: 0.76171875\n",
      "Epoch [1701], val_loss: 10705.4160\n",
      "gradient norm: 1449.1571350097656, minimum ratio: 0.78515625\n",
      "Epoch [1702], val_loss: 10727.9385\n",
      "gradient norm: 1451.2539825439453, minimum ratio: 0.77734375\n",
      "Epoch [1703], val_loss: 10750.4941\n",
      "gradient norm: 1453.3791809082031, minimum ratio: 0.765625\n",
      "Epoch [1704], val_loss: 10773.0820\n",
      "gradient norm: 1455.5257415771484, minimum ratio: 0.77734375\n",
      "Epoch [1705], val_loss: 10795.7109\n",
      "gradient norm: 1457.6192321777344, minimum ratio: 0.76953125\n",
      "Epoch [1706], val_loss: 10818.3730\n",
      "gradient norm: 1459.7186279296875, minimum ratio: 0.77734375\n",
      "Epoch [1707], val_loss: 10841.0703\n",
      "gradient norm: 1461.841781616211, minimum ratio: 0.7890625\n",
      "Epoch [1708], val_loss: 10863.7979\n",
      "gradient norm: 1463.9973754882812, minimum ratio: 0.76171875\n",
      "Epoch [1709], val_loss: 10886.5605\n",
      "gradient norm: 1466.0812530517578, minimum ratio: 0.7734375\n",
      "Epoch [1710], val_loss: 10909.3604\n",
      "gradient norm: 1468.1616516113281, minimum ratio: 0.76953125\n",
      "Epoch [1711], val_loss: 10932.1943\n",
      "gradient norm: 1470.3022766113281, minimum ratio: 0.7890625\n",
      "Epoch [1712], val_loss: 10955.0625\n",
      "gradient norm: 1472.3992309570312, minimum ratio: 0.77734375\n",
      "Epoch [1713], val_loss: 10977.9619\n",
      "gradient norm: 1474.5433959960938, minimum ratio: 0.7578125\n",
      "Epoch [1714], val_loss: 11000.8965\n",
      "gradient norm: 1476.6815948486328, minimum ratio: 0.7890625\n",
      "Epoch [1715], val_loss: 11023.8662\n",
      "gradient norm: 1478.7847747802734, minimum ratio: 0.7890625\n",
      "Epoch [1716], val_loss: 11046.8662\n",
      "gradient norm: 1480.9549560546875, minimum ratio: 0.78515625\n",
      "Epoch [1717], val_loss: 11069.9004\n",
      "gradient norm: 1483.126480102539, minimum ratio: 0.76953125\n",
      "Epoch [1718], val_loss: 11092.9688\n",
      "gradient norm: 1485.2901000976562, minimum ratio: 0.7890625\n",
      "Epoch [1719], val_loss: 11116.0723\n",
      "gradient norm: 1487.4659576416016, minimum ratio: 0.77734375\n",
      "Epoch [1720], val_loss: 11139.2080\n",
      "gradient norm: 1489.6270599365234, minimum ratio: 0.765625\n",
      "Epoch [1721], val_loss: 11162.3818\n",
      "gradient norm: 1491.8073425292969, minimum ratio: 0.7578125\n",
      "Epoch [1722], val_loss: 11185.5820\n",
      "gradient norm: 1493.9329833984375, minimum ratio: 0.8046875\n",
      "Epoch [1723], val_loss: 11208.8164\n",
      "gradient norm: 1496.0730743408203, minimum ratio: 0.7734375\n",
      "Epoch [1724], val_loss: 11232.0830\n",
      "gradient norm: 1498.1907196044922, minimum ratio: 0.78125\n",
      "Epoch [1725], val_loss: 11255.3838\n",
      "gradient norm: 1500.3763732910156, minimum ratio: 0.75390625\n",
      "Epoch [1726], val_loss: 11278.7207\n",
      "gradient norm: 1502.5640411376953, minimum ratio: 0.78515625\n",
      "Epoch [1727], val_loss: 11302.0869\n",
      "gradient norm: 1504.6876983642578, minimum ratio: 0.7890625\n",
      "Epoch [1728], val_loss: 11325.4863\n",
      "gradient norm: 1506.8515930175781, minimum ratio: 0.7734375\n",
      "Epoch [1729], val_loss: 11348.9150\n",
      "gradient norm: 1509.0179138183594, minimum ratio: 0.78125\n",
      "Epoch [1730], val_loss: 11372.3867\n",
      "gradient norm: 1511.1446990966797, minimum ratio: 0.765625\n",
      "Epoch [1731], val_loss: 11395.8896\n",
      "gradient norm: 1513.2815704345703, minimum ratio: 0.78515625\n",
      "Epoch [1732], val_loss: 11419.4248\n",
      "gradient norm: 1515.4653015136719, minimum ratio: 0.74609375\n",
      "Epoch [1733], val_loss: 11443.0010\n",
      "gradient norm: 1517.5537414550781, minimum ratio: 0.765625\n",
      "Epoch [1734], val_loss: 11466.6133\n",
      "gradient norm: 1519.7113342285156, minimum ratio: 0.7890625\n",
      "Epoch [1735], val_loss: 11490.2627\n",
      "gradient norm: 1521.8262329101562, minimum ratio: 0.7421875\n",
      "Epoch [1736], val_loss: 11513.9502\n",
      "gradient norm: 1524.0021667480469, minimum ratio: 0.75\n",
      "Epoch [1737], val_loss: 11537.6738\n",
      "gradient norm: 1526.2118835449219, minimum ratio: 0.77734375\n",
      "Epoch [1738], val_loss: 11561.4375\n",
      "gradient norm: 1528.3668060302734, minimum ratio: 0.76953125\n",
      "Epoch [1739], val_loss: 11585.2314\n",
      "gradient norm: 1530.4956512451172, minimum ratio: 0.78125\n",
      "Epoch [1740], val_loss: 11609.0625\n",
      "gradient norm: 1532.6898651123047, minimum ratio: 0.78125\n",
      "Epoch [1741], val_loss: 11632.9268\n",
      "gradient norm: 1534.9071807861328, minimum ratio: 0.76171875\n",
      "Epoch [1742], val_loss: 11656.8232\n",
      "gradient norm: 1537.1050720214844, minimum ratio: 0.7734375\n",
      "Epoch [1743], val_loss: 11680.7559\n",
      "gradient norm: 1539.2782745361328, minimum ratio: 0.765625\n",
      "Epoch [1744], val_loss: 11704.7207\n",
      "gradient norm: 1541.4806671142578, minimum ratio: 0.7890625\n",
      "Epoch [1745], val_loss: 11728.7197\n",
      "gradient norm: 1543.6790008544922, minimum ratio: 0.78125\n",
      "Epoch [1746], val_loss: 11752.7539\n",
      "gradient norm: 1545.9048614501953, minimum ratio: 0.7578125\n",
      "Epoch [1747], val_loss: 11776.8271\n",
      "gradient norm: 1548.1333312988281, minimum ratio: 0.76171875\n",
      "Epoch [1748], val_loss: 11800.9277\n",
      "gradient norm: 1550.362548828125, minimum ratio: 0.7734375\n",
      "Epoch [1749], val_loss: 11825.0713\n",
      "gradient norm: 1552.5516510009766, minimum ratio: 0.76171875\n",
      "Epoch [1750], val_loss: 11849.2490\n",
      "gradient norm: 1554.7513732910156, minimum ratio: 0.77734375\n",
      "Epoch [1751], val_loss: 11873.4570\n",
      "gradient norm: 1556.9867248535156, minimum ratio: 0.76171875\n",
      "Epoch [1752], val_loss: 11897.6982\n",
      "gradient norm: 1559.095718383789, minimum ratio: 0.75\n",
      "Epoch [1753], val_loss: 11921.9736\n",
      "gradient norm: 1561.3119506835938, minimum ratio: 0.77734375\n",
      "Epoch [1754], val_loss: 11946.2871\n",
      "gradient norm: 1563.4892272949219, minimum ratio: 0.765625\n",
      "Epoch [1755], val_loss: 11970.6338\n",
      "gradient norm: 1565.6971130371094, minimum ratio: 0.78125\n",
      "Epoch [1756], val_loss: 11995.0176\n",
      "gradient norm: 1567.9205474853516, minimum ratio: 0.765625\n",
      "Epoch [1757], val_loss: 12019.4443\n",
      "gradient norm: 1570.1686248779297, minimum ratio: 0.76953125\n",
      "Epoch [1758], val_loss: 12043.9033\n",
      "gradient norm: 1572.4050750732422, minimum ratio: 0.76953125\n",
      "Epoch [1759], val_loss: 12068.4014\n",
      "gradient norm: 1574.6434783935547, minimum ratio: 0.76171875\n",
      "Epoch [1760], val_loss: 12092.9346\n",
      "gradient norm: 1576.86083984375, minimum ratio: 0.7734375\n",
      "Epoch [1761], val_loss: 12117.5029\n",
      "gradient norm: 1579.0553283691406, minimum ratio: 0.765625\n",
      "Epoch [1762], val_loss: 12142.1016\n",
      "gradient norm: 1581.3123168945312, minimum ratio: 0.75390625\n",
      "Epoch [1763], val_loss: 12166.7373\n",
      "gradient norm: 1583.5227355957031, minimum ratio: 0.75\n",
      "Epoch [1764], val_loss: 12191.4102\n",
      "gradient norm: 1585.7491302490234, minimum ratio: 0.75390625\n",
      "Epoch [1765], val_loss: 12216.1201\n",
      "gradient norm: 1587.9469299316406, minimum ratio: 0.7578125\n",
      "Epoch [1766], val_loss: 12240.8643\n",
      "gradient norm: 1590.161849975586, minimum ratio: 0.76171875\n",
      "Epoch [1767], val_loss: 12265.6416\n",
      "gradient norm: 1592.3997955322266, minimum ratio: 0.796875\n",
      "Epoch [1768], val_loss: 12290.4551\n",
      "gradient norm: 1594.6114349365234, minimum ratio: 0.78515625\n",
      "Epoch [1769], val_loss: 12315.3037\n",
      "gradient norm: 1596.83056640625, minimum ratio: 0.77734375\n",
      "Epoch [1770], val_loss: 12340.1963\n",
      "gradient norm: 1599.0072631835938, minimum ratio: 0.7578125\n",
      "Epoch [1771], val_loss: 12365.1211\n",
      "gradient norm: 1601.2821502685547, minimum ratio: 0.78515625\n",
      "Epoch [1772], val_loss: 12390.0850\n",
      "gradient norm: 1603.5595703125, minimum ratio: 0.76953125\n",
      "Epoch [1773], val_loss: 12415.0850\n",
      "gradient norm: 1605.8050537109375, minimum ratio: 0.76953125\n",
      "Epoch [1774], val_loss: 12440.1182\n",
      "gradient norm: 1608.0714874267578, minimum ratio: 0.7578125\n",
      "Epoch [1775], val_loss: 12465.1895\n",
      "gradient norm: 1610.3073120117188, minimum ratio: 0.796875\n",
      "Epoch [1776], val_loss: 12490.2920\n",
      "gradient norm: 1612.5552215576172, minimum ratio: 0.7578125\n",
      "Epoch [1777], val_loss: 12515.4316\n",
      "gradient norm: 1614.8411407470703, minimum ratio: 0.76171875\n",
      "Epoch [1778], val_loss: 12540.6035\n",
      "gradient norm: 1617.1282806396484, minimum ratio: 0.8125\n",
      "Epoch [1779], val_loss: 12565.8086\n",
      "gradient norm: 1619.3684997558594, minimum ratio: 0.76953125\n",
      "Epoch [1780], val_loss: 12591.0527\n",
      "gradient norm: 1621.6213073730469, minimum ratio: 0.7734375\n",
      "Epoch [1781], val_loss: 12616.3330\n",
      "gradient norm: 1623.8829956054688, minimum ratio: 0.7578125\n",
      "Epoch [1782], val_loss: 12641.6533\n",
      "gradient norm: 1626.1113739013672, minimum ratio: 0.75\n",
      "Epoch [1783], val_loss: 12667.0049\n",
      "gradient norm: 1628.3757781982422, minimum ratio: 0.75\n",
      "Epoch [1784], val_loss: 12692.3945\n",
      "gradient norm: 1630.6488189697266, minimum ratio: 0.765625\n",
      "Epoch [1785], val_loss: 12717.8184\n",
      "gradient norm: 1632.8939819335938, minimum ratio: 0.76953125\n",
      "Epoch [1786], val_loss: 12743.2754\n",
      "gradient norm: 1635.1434326171875, minimum ratio: 0.75\n",
      "Epoch [1787], val_loss: 12768.7666\n",
      "gradient norm: 1637.446792602539, minimum ratio: 0.765625\n",
      "Epoch [1788], val_loss: 12794.2969\n",
      "gradient norm: 1639.7008666992188, minimum ratio: 0.79296875\n",
      "Epoch [1789], val_loss: 12819.8672\n",
      "gradient norm: 1642.009292602539, minimum ratio: 0.765625\n",
      "Epoch [1790], val_loss: 12845.4727\n",
      "gradient norm: 1644.2642517089844, minimum ratio: 0.7578125\n",
      "Epoch [1791], val_loss: 12871.1191\n",
      "gradient norm: 1646.5118713378906, minimum ratio: 0.765625\n",
      "Epoch [1792], val_loss: 12896.7998\n",
      "gradient norm: 1648.7807159423828, minimum ratio: 0.7578125\n",
      "Epoch [1793], val_loss: 12922.5146\n",
      "gradient norm: 1651.0213317871094, minimum ratio: 0.765625\n",
      "Epoch [1794], val_loss: 12948.2686\n",
      "gradient norm: 1653.28173828125, minimum ratio: 0.78125\n",
      "Epoch [1795], val_loss: 12974.0547\n",
      "gradient norm: 1655.601303100586, minimum ratio: 0.77734375\n",
      "Epoch [1796], val_loss: 12999.8799\n",
      "gradient norm: 1657.8058624267578, minimum ratio: 0.7421875\n",
      "Epoch [1797], val_loss: 13025.7383\n",
      "gradient norm: 1660.1285552978516, minimum ratio: 0.75\n",
      "Epoch [1798], val_loss: 13051.6367\n",
      "gradient norm: 1662.4299926757812, minimum ratio: 0.76171875\n",
      "Epoch [1799], val_loss: 13077.5713\n",
      "gradient norm: 1664.727066040039, minimum ratio: 0.7890625\n",
      "Epoch [1800], val_loss: 13103.5479\n",
      "gradient norm: 1667.0570373535156, minimum ratio: 0.7578125\n",
      "Epoch [1801], val_loss: 13129.5566\n",
      "gradient norm: 1669.3665313720703, minimum ratio: 0.7734375\n",
      "Epoch [1802], val_loss: 13155.6064\n",
      "gradient norm: 1671.656753540039, minimum ratio: 0.765625\n",
      "Epoch [1803], val_loss: 13181.6924\n",
      "gradient norm: 1673.9929809570312, minimum ratio: 0.7734375\n",
      "Epoch [1804], val_loss: 13207.8174\n",
      "gradient norm: 1676.2437286376953, minimum ratio: 0.765625\n",
      "Epoch [1805], val_loss: 13233.9766\n",
      "gradient norm: 1678.5837860107422, minimum ratio: 0.765625\n",
      "Epoch [1806], val_loss: 13260.1748\n",
      "gradient norm: 1680.8659057617188, minimum ratio: 0.7734375\n",
      "Epoch [1807], val_loss: 13286.4170\n",
      "gradient norm: 1683.2115173339844, minimum ratio: 0.76953125\n",
      "Epoch [1808], val_loss: 13312.6934\n",
      "gradient norm: 1685.5150756835938, minimum ratio: 0.76953125\n",
      "Epoch [1809], val_loss: 13339.0049\n",
      "gradient norm: 1687.7987976074219, minimum ratio: 0.7734375\n",
      "Epoch [1810], val_loss: 13365.3516\n",
      "gradient norm: 1690.1056365966797, minimum ratio: 0.7734375\n",
      "Epoch [1811], val_loss: 13391.7305\n",
      "gradient norm: 1692.4210662841797, minimum ratio: 0.78515625\n",
      "Epoch [1812], val_loss: 13418.1465\n",
      "gradient norm: 1694.7576293945312, minimum ratio: 0.78515625\n",
      "Epoch [1813], val_loss: 13444.5928\n",
      "gradient norm: 1697.0762023925781, minimum ratio: 0.76953125\n",
      "Epoch [1814], val_loss: 13471.0742\n",
      "gradient norm: 1699.3971405029297, minimum ratio: 0.75\n",
      "Epoch [1815], val_loss: 13497.5908\n",
      "gradient norm: 1701.7541809082031, minimum ratio: 0.765625\n",
      "Epoch [1816], val_loss: 13524.1396\n",
      "gradient norm: 1704.112060546875, minimum ratio: 0.7734375\n",
      "Epoch [1817], val_loss: 13550.7285\n",
      "gradient norm: 1706.3892974853516, minimum ratio: 0.78125\n",
      "Epoch [1818], val_loss: 13577.3525\n",
      "gradient norm: 1708.7510833740234, minimum ratio: 0.765625\n",
      "Epoch [1819], val_loss: 13604.0176\n",
      "gradient norm: 1711.0454711914062, minimum ratio: 0.75390625\n",
      "Epoch [1820], val_loss: 13630.7227\n",
      "gradient norm: 1713.3088989257812, minimum ratio: 0.77734375\n",
      "Epoch [1821], val_loss: 13657.4639\n",
      "gradient norm: 1715.6492614746094, minimum ratio: 0.77734375\n",
      "Epoch [1822], val_loss: 13684.2402\n",
      "gradient norm: 1717.9488220214844, minimum ratio: 0.75390625\n",
      "Epoch [1823], val_loss: 13711.0479\n",
      "gradient norm: 1720.2809600830078, minimum ratio: 0.76171875\n",
      "Epoch [1824], val_loss: 13737.8906\n",
      "gradient norm: 1722.6529388427734, minimum ratio: 0.75390625\n",
      "Epoch [1825], val_loss: 13764.7656\n",
      "gradient norm: 1725.0260925292969, minimum ratio: 0.76171875\n",
      "Epoch [1826], val_loss: 13791.6797\n",
      "gradient norm: 1727.375473022461, minimum ratio: 0.78125\n",
      "Epoch [1827], val_loss: 13818.6367\n",
      "gradient norm: 1729.6562194824219, minimum ratio: 0.76171875\n",
      "Epoch [1828], val_loss: 13845.6338\n",
      "gradient norm: 1732.0368957519531, minimum ratio: 0.75\n",
      "Epoch [1829], val_loss: 13872.6670\n",
      "gradient norm: 1734.4195861816406, minimum ratio: 0.7421875\n",
      "Epoch [1830], val_loss: 13899.7393\n",
      "gradient norm: 1736.7017059326172, minimum ratio: 0.77734375\n",
      "Epoch [1831], val_loss: 13926.8447\n",
      "gradient norm: 1739.0882415771484, minimum ratio: 0.765625\n",
      "Epoch [1832], val_loss: 13953.9883\n",
      "gradient norm: 1741.4485931396484, minimum ratio: 0.7421875\n",
      "Epoch [1833], val_loss: 13981.1719\n",
      "gradient norm: 1743.8350219726562, minimum ratio: 0.7890625\n",
      "Epoch [1834], val_loss: 14008.3916\n",
      "gradient norm: 1746.1262664794922, minimum ratio: 0.75\n",
      "Epoch [1835], val_loss: 14035.6504\n",
      "gradient norm: 1748.5204162597656, minimum ratio: 0.7578125\n",
      "Epoch [1836], val_loss: 14062.9414\n",
      "gradient norm: 1750.916244506836, minimum ratio: 0.72265625\n",
      "Epoch [1837], val_loss: 14090.2783\n",
      "gradient norm: 1753.2632598876953, minimum ratio: 0.77734375\n",
      "Epoch [1838], val_loss: 14117.6621\n",
      "gradient norm: 1755.6051635742188, minimum ratio: 0.74609375\n",
      "Epoch [1839], val_loss: 14145.0850\n",
      "gradient norm: 1757.9496307373047, minimum ratio: 0.78515625\n",
      "Epoch [1840], val_loss: 14172.5479\n",
      "gradient norm: 1760.3307342529297, minimum ratio: 0.76953125\n",
      "Epoch [1841], val_loss: 14200.0469\n",
      "gradient norm: 1762.6917266845703, minimum ratio: 0.78125\n",
      "Epoch [1842], val_loss: 14227.5850\n",
      "gradient norm: 1765.0523986816406, minimum ratio: 0.7578125\n",
      "Epoch [1843], val_loss: 14255.1602\n",
      "gradient norm: 1767.4654541015625, minimum ratio: 0.75390625\n",
      "Epoch [1844], val_loss: 14282.7764\n",
      "gradient norm: 1769.8479766845703, minimum ratio: 0.76171875\n",
      "Epoch [1845], val_loss: 14310.4346\n",
      "gradient norm: 1772.2174530029297, minimum ratio: 0.78125\n",
      "Epoch [1846], val_loss: 14338.1279\n",
      "gradient norm: 1774.6019134521484, minimum ratio: 0.76171875\n",
      "Epoch [1847], val_loss: 14365.8613\n",
      "gradient norm: 1776.9690856933594, minimum ratio: 0.76171875\n",
      "Epoch [1848], val_loss: 14393.6348\n",
      "gradient norm: 1779.3934783935547, minimum ratio: 0.76171875\n",
      "Epoch [1849], val_loss: 14421.4404\n",
      "gradient norm: 1781.8184204101562, minimum ratio: 0.77734375\n",
      "Epoch [1850], val_loss: 14449.2803\n",
      "gradient norm: 1784.2085418701172, minimum ratio: 0.78125\n",
      "Epoch [1851], val_loss: 14477.1631\n",
      "gradient norm: 1786.5342254638672, minimum ratio: 0.7734375\n",
      "Epoch [1852], val_loss: 14505.0869\n",
      "gradient norm: 1788.937744140625, minimum ratio: 0.77734375\n",
      "Epoch [1853], val_loss: 14533.0498\n",
      "gradient norm: 1791.3716583251953, minimum ratio: 0.76953125\n",
      "Epoch [1854], val_loss: 14561.0547\n",
      "gradient norm: 1793.6135864257812, minimum ratio: 0.77734375\n",
      "Epoch [1855], val_loss: 14589.0938\n",
      "gradient norm: 1796.001205444336, minimum ratio: 0.7734375\n",
      "Epoch [1856], val_loss: 14617.1748\n",
      "gradient norm: 1798.4411315917969, minimum ratio: 0.77734375\n",
      "Epoch [1857], val_loss: 14645.2900\n",
      "gradient norm: 1800.8551330566406, minimum ratio: 0.78125\n",
      "Epoch [1858], val_loss: 14673.4414\n",
      "gradient norm: 1803.2674865722656, minimum ratio: 0.7578125\n",
      "Epoch [1859], val_loss: 14701.6338\n",
      "gradient norm: 1805.6918487548828, minimum ratio: 0.78125\n",
      "Epoch [1860], val_loss: 14729.8613\n",
      "gradient norm: 1808.0487365722656, minimum ratio: 0.75\n",
      "Epoch [1861], val_loss: 14758.1299\n",
      "gradient norm: 1810.4978942871094, minimum ratio: 0.7578125\n",
      "Epoch [1862], val_loss: 14786.4385\n",
      "gradient norm: 1812.870849609375, minimum ratio: 0.7890625\n",
      "Epoch [1863], val_loss: 14814.7900\n",
      "gradient norm: 1815.32470703125, minimum ratio: 0.74609375\n",
      "Epoch [1864], val_loss: 14843.1748\n",
      "gradient norm: 1817.780014038086, minimum ratio: 0.7578125\n",
      "Epoch [1865], val_loss: 14871.5947\n",
      "gradient norm: 1820.1710815429688, minimum ratio: 0.7734375\n",
      "Epoch [1866], val_loss: 14900.0518\n",
      "gradient norm: 1822.609848022461, minimum ratio: 0.7578125\n",
      "Epoch [1867], val_loss: 14928.5371\n",
      "gradient norm: 1825.018798828125, minimum ratio: 0.76171875\n",
      "Epoch [1868], val_loss: 14957.0635\n",
      "gradient norm: 1827.4560546875, minimum ratio: 0.76171875\n",
      "Epoch [1869], val_loss: 14985.6299\n",
      "gradient norm: 1829.8733367919922, minimum ratio: 0.79296875\n",
      "Epoch [1870], val_loss: 15014.2432\n",
      "gradient norm: 1832.3204650878906, minimum ratio: 0.765625\n",
      "Epoch [1871], val_loss: 15042.8965\n",
      "gradient norm: 1834.751220703125, minimum ratio: 0.7578125\n",
      "Epoch [1872], val_loss: 15071.5869\n",
      "gradient norm: 1837.2212982177734, minimum ratio: 0.74609375\n",
      "Epoch [1873], val_loss: 15100.3271\n",
      "gradient norm: 1839.6956939697266, minimum ratio: 0.765625\n",
      "Epoch [1874], val_loss: 15129.1104\n",
      "gradient norm: 1842.134017944336, minimum ratio: 0.75\n",
      "Epoch [1875], val_loss: 15157.9229\n",
      "gradient norm: 1844.5858612060547, minimum ratio: 0.73828125\n",
      "Epoch [1876], val_loss: 15186.7803\n",
      "gradient norm: 1846.9933776855469, minimum ratio: 0.75\n",
      "Epoch [1877], val_loss: 15215.6729\n",
      "gradient norm: 1849.4300994873047, minimum ratio: 0.75\n",
      "Epoch [1878], val_loss: 15244.6035\n",
      "gradient norm: 1851.913818359375, minimum ratio: 0.7421875\n",
      "Epoch [1879], val_loss: 15273.5684\n",
      "gradient norm: 1854.3295593261719, minimum ratio: 0.78125\n",
      "Epoch [1880], val_loss: 15302.5664\n",
      "gradient norm: 1856.786376953125, minimum ratio: 0.75\n",
      "Epoch [1881], val_loss: 15331.6055\n",
      "gradient norm: 1859.1599884033203, minimum ratio: 0.765625\n",
      "Epoch [1882], val_loss: 15360.6846\n",
      "gradient norm: 1861.6311950683594, minimum ratio: 0.74609375\n",
      "Epoch [1883], val_loss: 15389.8066\n",
      "gradient norm: 1864.0879974365234, minimum ratio: 0.7578125\n",
      "Epoch [1884], val_loss: 15418.9697\n",
      "gradient norm: 1866.55078125, minimum ratio: 0.78125\n",
      "Epoch [1885], val_loss: 15448.1748\n",
      "gradient norm: 1869.0503540039062, minimum ratio: 0.74609375\n",
      "Epoch [1886], val_loss: 15477.4199\n",
      "gradient norm: 1871.4459075927734, minimum ratio: 0.7890625\n",
      "Epoch [1887], val_loss: 15506.7021\n",
      "gradient norm: 1873.8782043457031, minimum ratio: 0.7890625\n",
      "Epoch [1888], val_loss: 15536.0254\n",
      "gradient norm: 1876.3230743408203, minimum ratio: 0.7734375\n",
      "Epoch [1889], val_loss: 15565.3916\n",
      "gradient norm: 1878.7503051757812, minimum ratio: 0.7578125\n",
      "Epoch [1890], val_loss: 15594.7979\n",
      "gradient norm: 1881.2576599121094, minimum ratio: 0.75390625\n",
      "Epoch [1891], val_loss: 15624.2451\n",
      "gradient norm: 1883.726821899414, minimum ratio: 0.7734375\n",
      "Epoch [1892], val_loss: 15653.7305\n",
      "gradient norm: 1886.2388610839844, minimum ratio: 0.7734375\n",
      "Epoch [1893], val_loss: 15683.2520\n",
      "gradient norm: 1888.6527862548828, minimum ratio: 0.7734375\n",
      "Epoch [1894], val_loss: 15712.8086\n",
      "gradient norm: 1891.1086120605469, minimum ratio: 0.78125\n",
      "Epoch [1895], val_loss: 15742.3984\n",
      "gradient norm: 1893.6234588623047, minimum ratio: 0.7421875\n",
      "Epoch [1896], val_loss: 15772.0254\n",
      "gradient norm: 1896.1070556640625, minimum ratio: 0.765625\n",
      "Epoch [1897], val_loss: 15801.6963\n",
      "gradient norm: 1898.5564422607422, minimum ratio: 0.765625\n",
      "Epoch [1898], val_loss: 15831.4102\n",
      "gradient norm: 1901.0778198242188, minimum ratio: 0.7734375\n",
      "Epoch [1899], val_loss: 15861.1562\n",
      "gradient norm: 1903.5313720703125, minimum ratio: 0.7578125\n",
      "Epoch [1900], val_loss: 15890.9385\n",
      "gradient norm: 1906.0552215576172, minimum ratio: 0.75\n",
      "Epoch [1901], val_loss: 15920.7637\n",
      "gradient norm: 1908.4899291992188, minimum ratio: 0.765625\n",
      "Epoch [1902], val_loss: 15950.6338\n",
      "gradient norm: 1910.9898223876953, minimum ratio: 0.76171875\n",
      "Epoch [1903], val_loss: 15980.5264\n",
      "gradient norm: 1913.5179595947266, minimum ratio: 0.7421875\n",
      "Epoch [1904], val_loss: 16010.4688\n",
      "gradient norm: 1916.001220703125, minimum ratio: 0.796875\n",
      "Epoch [1905], val_loss: 16040.4531\n",
      "gradient norm: 1918.4817962646484, minimum ratio: 0.7890625\n",
      "Epoch [1906], val_loss: 16070.4746\n",
      "gradient norm: 1921.0174102783203, minimum ratio: 0.7578125\n",
      "Epoch [1907], val_loss: 16100.5400\n",
      "gradient norm: 1923.5373992919922, minimum ratio: 0.76953125\n",
      "Epoch [1908], val_loss: 16130.6445\n",
      "gradient norm: 1926.0775451660156, minimum ratio: 0.75\n",
      "Epoch [1909], val_loss: 16160.7881\n",
      "gradient norm: 1928.6197814941406, minimum ratio: 0.73046875\n",
      "Epoch [1910], val_loss: 16190.9619\n",
      "gradient norm: 1931.0962371826172, minimum ratio: 0.7734375\n",
      "Epoch [1911], val_loss: 16221.1748\n",
      "gradient norm: 1933.5975646972656, minimum ratio: 0.76171875\n",
      "Epoch [1912], val_loss: 16251.4268\n",
      "gradient norm: 1936.0656433105469, minimum ratio: 0.74609375\n",
      "Epoch [1913], val_loss: 16281.7236\n",
      "gradient norm: 1938.5731048583984, minimum ratio: 0.74609375\n",
      "Epoch [1914], val_loss: 16312.0566\n",
      "gradient norm: 1941.0467376708984, minimum ratio: 0.76953125\n",
      "Epoch [1915], val_loss: 16342.4297\n",
      "gradient norm: 1943.5987701416016, minimum ratio: 0.7734375\n",
      "Epoch [1916], val_loss: 16372.8564\n",
      "gradient norm: 1946.1258087158203, minimum ratio: 0.7578125\n",
      "Epoch [1917], val_loss: 16403.3203\n",
      "gradient norm: 1948.61767578125, minimum ratio: 0.75\n",
      "Epoch [1918], val_loss: 16433.8262\n",
      "gradient norm: 1951.1407928466797, minimum ratio: 0.73828125\n",
      "Epoch [1919], val_loss: 16464.3711\n",
      "gradient norm: 1953.6572875976562, minimum ratio: 0.75390625\n",
      "Epoch [1920], val_loss: 16494.9531\n",
      "gradient norm: 1956.112289428711, minimum ratio: 0.7890625\n",
      "Epoch [1921], val_loss: 16525.5801\n",
      "gradient norm: 1958.6459197998047, minimum ratio: 0.7734375\n",
      "Epoch [1922], val_loss: 16556.2363\n",
      "gradient norm: 1961.1804809570312, minimum ratio: 0.7890625\n",
      "Epoch [1923], val_loss: 16586.9355\n",
      "gradient norm: 1963.6880950927734, minimum ratio: 0.7421875\n",
      "Epoch [1924], val_loss: 16617.6777\n",
      "gradient norm: 1966.1752166748047, minimum ratio: 0.76953125\n",
      "Epoch [1925], val_loss: 16648.4629\n",
      "gradient norm: 1968.7181701660156, minimum ratio: 0.765625\n",
      "Epoch [1926], val_loss: 16679.2832\n",
      "gradient norm: 1971.2930297851562, minimum ratio: 0.765625\n",
      "Epoch [1927], val_loss: 16710.1484\n",
      "gradient norm: 1973.83203125, minimum ratio: 0.78125\n",
      "Epoch [1928], val_loss: 16741.0469\n",
      "gradient norm: 1976.3449096679688, minimum ratio: 0.7578125\n",
      "Epoch [1929], val_loss: 16771.9863\n",
      "gradient norm: 1978.8943481445312, minimum ratio: 0.765625\n",
      "Epoch [1930], val_loss: 16802.9668\n",
      "gradient norm: 1981.4759521484375, minimum ratio: 0.76953125\n",
      "Epoch [1931], val_loss: 16833.9902\n",
      "gradient norm: 1983.9715728759766, minimum ratio: 0.7578125\n",
      "Epoch [1932], val_loss: 16865.0566\n",
      "gradient norm: 1986.4607391357422, minimum ratio: 0.78125\n",
      "Epoch [1933], val_loss: 16896.1680\n",
      "gradient norm: 1989.0534210205078, minimum ratio: 0.7734375\n",
      "Epoch [1934], val_loss: 16927.3125\n",
      "gradient norm: 1991.5727996826172, minimum ratio: 0.7734375\n",
      "Epoch [1935], val_loss: 16958.5000\n",
      "gradient norm: 1994.0732116699219, minimum ratio: 0.76953125\n",
      "Epoch [1936], val_loss: 16989.7324\n",
      "gradient norm: 1996.6683959960938, minimum ratio: 0.75390625\n",
      "Epoch [1937], val_loss: 17021.0078\n",
      "gradient norm: 1999.266098022461, minimum ratio: 0.7578125\n",
      "Epoch [1938], val_loss: 17052.3242\n",
      "gradient norm: 2001.8229522705078, minimum ratio: 0.78125\n",
      "Epoch [1939], val_loss: 17083.6777\n",
      "gradient norm: 2004.4244689941406, minimum ratio: 0.76171875\n",
      "Epoch [1940], val_loss: 17115.0820\n",
      "gradient norm: 2007.0292510986328, minimum ratio: 0.74609375\n",
      "Epoch [1941], val_loss: 17146.5215\n",
      "gradient norm: 2009.6356811523438, minimum ratio: 0.76171875\n",
      "Epoch [1942], val_loss: 17177.9980\n",
      "gradient norm: 2012.164291381836, minimum ratio: 0.76953125\n",
      "Epoch [1943], val_loss: 17209.5176\n",
      "gradient norm: 2014.7738647460938, minimum ratio: 0.765625\n",
      "Epoch [1944], val_loss: 17241.0762\n",
      "gradient norm: 2017.3614959716797, minimum ratio: 0.74609375\n",
      "Epoch [1945], val_loss: 17272.6836\n",
      "gradient norm: 2019.9261627197266, minimum ratio: 0.75390625\n",
      "Epoch [1946], val_loss: 17304.3262\n",
      "gradient norm: 2022.427993774414, minimum ratio: 0.74609375\n",
      "Epoch [1947], val_loss: 17336.0078\n",
      "gradient norm: 2025.0011444091797, minimum ratio: 0.76171875\n",
      "Epoch [1948], val_loss: 17367.7344\n",
      "gradient norm: 2027.591064453125, minimum ratio: 0.78125\n",
      "Epoch [1949], val_loss: 17399.5039\n",
      "gradient norm: 2030.1791687011719, minimum ratio: 0.765625\n",
      "Epoch [1950], val_loss: 17431.3184\n",
      "gradient norm: 2032.7813415527344, minimum ratio: 0.7578125\n",
      "Epoch [1951], val_loss: 17463.1719\n",
      "gradient norm: 2035.3740234375, minimum ratio: 0.75390625\n",
      "Epoch [1952], val_loss: 17495.0645\n",
      "gradient norm: 2037.8948669433594, minimum ratio: 0.75390625\n",
      "Epoch [1953], val_loss: 17526.9961\n",
      "gradient norm: 2040.4928894042969, minimum ratio: 0.75390625\n",
      "Epoch [1954], val_loss: 17558.9648\n",
      "gradient norm: 2043.0321502685547, minimum ratio: 0.7734375\n",
      "Epoch [1955], val_loss: 17590.9824\n",
      "gradient norm: 2045.6225280761719, minimum ratio: 0.76953125\n",
      "Epoch [1956], val_loss: 17623.0391\n",
      "gradient norm: 2048.1839904785156, minimum ratio: 0.76953125\n",
      "Epoch [1957], val_loss: 17655.1270\n",
      "gradient norm: 2050.7830963134766, minimum ratio: 0.75\n",
      "Epoch [1958], val_loss: 17687.2598\n",
      "gradient norm: 2053.421646118164, minimum ratio: 0.7578125\n",
      "Epoch [1959], val_loss: 17719.4316\n",
      "gradient norm: 2056.038330078125, minimum ratio: 0.75\n",
      "Epoch [1960], val_loss: 17751.6523\n",
      "gradient norm: 2058.632034301758, minimum ratio: 0.75390625\n",
      "Epoch [1961], val_loss: 17783.9062\n",
      "gradient norm: 2061.2359771728516, minimum ratio: 0.75390625\n",
      "Epoch [1962], val_loss: 17816.2031\n",
      "gradient norm: 2063.7994384765625, minimum ratio: 0.76171875\n",
      "Epoch [1963], val_loss: 17848.5508\n",
      "gradient norm: 2066.3941955566406, minimum ratio: 0.796875\n",
      "Epoch [1964], val_loss: 17880.9375\n",
      "gradient norm: 2069.0270233154297, minimum ratio: 0.75390625\n",
      "Epoch [1965], val_loss: 17913.3750\n",
      "gradient norm: 2071.598876953125, minimum ratio: 0.7578125\n",
      "Epoch [1966], val_loss: 17945.8496\n",
      "gradient norm: 2074.2552337646484, minimum ratio: 0.7734375\n",
      "Epoch [1967], val_loss: 17978.3730\n",
      "gradient norm: 2076.915542602539, minimum ratio: 0.7421875\n",
      "Epoch [1968], val_loss: 18010.9434\n",
      "gradient norm: 2079.5779876708984, minimum ratio: 0.7421875\n",
      "Epoch [1969], val_loss: 18043.5586\n",
      "gradient norm: 2082.2100982666016, minimum ratio: 0.78515625\n",
      "Epoch [1970], val_loss: 18076.2129\n",
      "gradient norm: 2084.8775634765625, minimum ratio: 0.76953125\n",
      "Epoch [1971], val_loss: 18108.9102\n",
      "gradient norm: 2087.405242919922, minimum ratio: 0.76953125\n",
      "Epoch [1972], val_loss: 18141.6426\n",
      "gradient norm: 2090.037078857422, minimum ratio: 0.76953125\n",
      "Epoch [1973], val_loss: 18174.4258\n",
      "gradient norm: 2092.7108764648438, minimum ratio: 0.78125\n",
      "Epoch [1974], val_loss: 18207.2559\n",
      "gradient norm: 2095.255401611328, minimum ratio: 0.75390625\n",
      "Epoch [1975], val_loss: 18240.1289\n",
      "gradient norm: 2097.893569946289, minimum ratio: 0.75390625\n",
      "Epoch [1976], val_loss: 18273.0410\n",
      "gradient norm: 2100.4894409179688, minimum ratio: 0.76953125\n",
      "Epoch [1977], val_loss: 18305.9980\n",
      "gradient norm: 2103.1725158691406, minimum ratio: 0.7734375\n",
      "Epoch [1978], val_loss: 18338.9941\n",
      "gradient norm: 2105.856887817383, minimum ratio: 0.75390625\n",
      "Epoch [1979], val_loss: 18372.0176\n",
      "gradient norm: 2108.462844848633, minimum ratio: 0.75\n",
      "Epoch [1980], val_loss: 18405.0859\n",
      "gradient norm: 2111.117660522461, minimum ratio: 0.75\n",
      "Epoch [1981], val_loss: 18438.1992\n",
      "gradient norm: 2113.713836669922, minimum ratio: 0.734375\n",
      "Epoch [1982], val_loss: 18471.3477\n",
      "gradient norm: 2116.394760131836, minimum ratio: 0.76171875\n",
      "Epoch [1983], val_loss: 18504.5312\n",
      "gradient norm: 2119.0474548339844, minimum ratio: 0.7734375\n",
      "Epoch [1984], val_loss: 18537.7539\n",
      "gradient norm: 2121.7381286621094, minimum ratio: 0.76953125\n",
      "Epoch [1985], val_loss: 18571.0215\n",
      "gradient norm: 2124.383544921875, minimum ratio: 0.77734375\n",
      "Epoch [1986], val_loss: 18604.3320\n",
      "gradient norm: 2127.027847290039, minimum ratio: 0.7578125\n",
      "Epoch [1987], val_loss: 18637.6934\n",
      "gradient norm: 2129.7279663085938, minimum ratio: 0.76953125\n",
      "Epoch [1988], val_loss: 18671.1055\n",
      "gradient norm: 2132.398956298828, minimum ratio: 0.76171875\n",
      "Epoch [1989], val_loss: 18704.5508\n",
      "gradient norm: 2135.0291442871094, minimum ratio: 0.76171875\n",
      "Epoch [1990], val_loss: 18738.0332\n",
      "gradient norm: 2137.6741333007812, minimum ratio: 0.7890625\n",
      "Epoch [1991], val_loss: 18771.5566\n",
      "gradient norm: 2140.380172729492, minimum ratio: 0.7578125\n",
      "Epoch [1992], val_loss: 18805.1211\n",
      "gradient norm: 2142.9900970458984, minimum ratio: 0.7578125\n",
      "Epoch [1993], val_loss: 18838.7285\n",
      "gradient norm: 2145.699920654297, minimum ratio: 0.75390625\n",
      "Epoch [1994], val_loss: 18872.3672\n",
      "gradient norm: 2148.362030029297, minimum ratio: 0.75390625\n",
      "Epoch [1995], val_loss: 18906.0469\n",
      "gradient norm: 2150.973587036133, minimum ratio: 0.77734375\n",
      "Epoch [1996], val_loss: 18939.7715\n",
      "gradient norm: 2153.637252807617, minimum ratio: 0.76171875\n",
      "Epoch [1997], val_loss: 18973.5469\n",
      "gradient norm: 2156.217041015625, minimum ratio: 0.75390625\n",
      "Epoch [1998], val_loss: 19007.3594\n",
      "gradient norm: 2158.811981201172, minimum ratio: 0.74609375\n",
      "Epoch [1999], val_loss: 19041.2168\n",
      "gradient norm: 2161.4493408203125, minimum ratio: 0.76953125\n",
      "Epoch [2000], val_loss: 19075.1191\n",
      "gradient norm: 2164.172378540039, minimum ratio: 0.75390625\n",
      "Epoch [2001], val_loss: 19109.0625\n",
      "gradient norm: 2166.8973693847656, minimum ratio: 0.7734375\n",
      "Epoch [2002], val_loss: 19143.0527\n",
      "gradient norm: 2169.597702026367, minimum ratio: 0.7734375\n",
      "Epoch [2003], val_loss: 19177.0840\n",
      "gradient norm: 2172.326644897461, minimum ratio: 0.77734375\n",
      "Epoch [2004], val_loss: 19211.1543\n",
      "gradient norm: 2175.0572967529297, minimum ratio: 0.76171875\n",
      "Epoch [2005], val_loss: 19245.2617\n",
      "gradient norm: 2177.789779663086, minimum ratio: 0.7578125\n",
      "Epoch [2006], val_loss: 19279.4238\n",
      "gradient norm: 2180.474395751953, minimum ratio: 0.75390625\n",
      "Epoch [2007], val_loss: 19313.6270\n",
      "gradient norm: 2183.107696533203, minimum ratio: 0.76953125\n",
      "Epoch [2008], val_loss: 19347.8750\n",
      "gradient norm: 2185.8478393554688, minimum ratio: 0.765625\n",
      "Epoch [2009], val_loss: 19382.1719\n",
      "gradient norm: 2188.5596313476562, minimum ratio: 0.75390625\n",
      "Epoch [2010], val_loss: 19416.5098\n",
      "gradient norm: 2191.249725341797, minimum ratio: 0.7734375\n",
      "Epoch [2011], val_loss: 19450.8926\n",
      "gradient norm: 2193.9227905273438, minimum ratio: 0.79296875\n",
      "Epoch [2012], val_loss: 19485.3086\n",
      "gradient norm: 2196.6204223632812, minimum ratio: 0.74609375\n",
      "Epoch [2013], val_loss: 19519.7637\n",
      "gradient norm: 2199.329605102539, minimum ratio: 0.76953125\n",
      "Epoch [2014], val_loss: 19554.2637\n",
      "gradient norm: 2202.000442504883, minimum ratio: 0.77734375\n",
      "Epoch [2015], val_loss: 19588.8047\n",
      "gradient norm: 2204.6463928222656, minimum ratio: 0.7734375\n",
      "Epoch [2016], val_loss: 19623.3867\n",
      "gradient norm: 2207.3326416015625, minimum ratio: 0.75\n",
      "Epoch [2017], val_loss: 19658.0059\n",
      "gradient norm: 2210.0379943847656, minimum ratio: 0.7734375\n",
      "Epoch [2018], val_loss: 19692.6641\n",
      "gradient norm: 2212.7589569091797, minimum ratio: 0.7578125\n",
      "Epoch [2019], val_loss: 19727.3809\n",
      "gradient norm: 2215.519500732422, minimum ratio: 0.7578125\n",
      "Epoch [2020], val_loss: 19762.1406\n",
      "gradient norm: 2218.2381744384766, minimum ratio: 0.75390625\n",
      "Epoch [2021], val_loss: 19796.9453\n",
      "gradient norm: 2220.9487915039062, minimum ratio: 0.765625\n",
      "Epoch [2022], val_loss: 19831.7891\n",
      "gradient norm: 2223.6876831054688, minimum ratio: 0.75390625\n",
      "Epoch [2023], val_loss: 19866.6836\n",
      "gradient norm: 2226.45751953125, minimum ratio: 0.765625\n",
      "Epoch [2024], val_loss: 19901.6191\n",
      "gradient norm: 2229.186477661133, minimum ratio: 0.75\n",
      "Epoch [2025], val_loss: 19936.6074\n",
      "gradient norm: 2231.911819458008, minimum ratio: 0.76171875\n",
      "Epoch [2026], val_loss: 19971.6328\n",
      "gradient norm: 2234.6882934570312, minimum ratio: 0.78125\n",
      "Epoch [2027], val_loss: 20006.7051\n",
      "gradient norm: 2237.446029663086, minimum ratio: 0.74609375\n",
      "Epoch [2028], val_loss: 20041.8145\n",
      "gradient norm: 2240.2256469726562, minimum ratio: 0.75\n",
      "Epoch [2029], val_loss: 20076.9668\n",
      "gradient norm: 2242.946533203125, minimum ratio: 0.75390625\n",
      "Epoch [2030], val_loss: 20112.1602\n",
      "gradient norm: 2245.689407348633, minimum ratio: 0.7421875\n",
      "Epoch [2031], val_loss: 20147.4023\n",
      "gradient norm: 2248.4754943847656, minimum ratio: 0.78515625\n",
      "Epoch [2032], val_loss: 20182.6836\n",
      "gradient norm: 2251.2149658203125, minimum ratio: 0.76953125\n",
      "Epoch [2033], val_loss: 20218.0137\n",
      "gradient norm: 2253.9683227539062, minimum ratio: 0.76953125\n",
      "Epoch [2034], val_loss: 20253.3867\n",
      "gradient norm: 2256.6683959960938, minimum ratio: 0.7734375\n",
      "Epoch [2035], val_loss: 20288.8008\n",
      "gradient norm: 2259.425247192383, minimum ratio: 0.78125\n",
      "Epoch [2036], val_loss: 20324.2559\n",
      "gradient norm: 2262.0736694335938, minimum ratio: 0.7734375\n",
      "Epoch [2037], val_loss: 20359.7559\n",
      "gradient norm: 2264.8179779052734, minimum ratio: 0.75390625\n",
      "Epoch [2038], val_loss: 20395.3008\n",
      "gradient norm: 2267.5506744384766, minimum ratio: 0.7578125\n",
      "Epoch [2039], val_loss: 20430.8848\n",
      "gradient norm: 2270.2554779052734, minimum ratio: 0.76171875\n",
      "Epoch [2040], val_loss: 20466.5176\n",
      "gradient norm: 2273.0371704101562, minimum ratio: 0.75390625\n",
      "Epoch [2041], val_loss: 20502.1895\n",
      "gradient norm: 2275.8421783447266, minimum ratio: 0.7421875\n",
      "Epoch [2042], val_loss: 20537.9043\n",
      "gradient norm: 2278.61767578125, minimum ratio: 0.7578125\n",
      "Epoch [2043], val_loss: 20573.6621\n",
      "gradient norm: 2281.426315307617, minimum ratio: 0.7421875\n",
      "Epoch [2044], val_loss: 20609.4707\n",
      "gradient norm: 2284.1238555908203, minimum ratio: 0.76171875\n",
      "Epoch [2045], val_loss: 20645.3242\n",
      "gradient norm: 2286.938491821289, minimum ratio: 0.76171875\n",
      "Epoch [2046], val_loss: 20681.2207\n",
      "gradient norm: 2289.628707885742, minimum ratio: 0.76953125\n",
      "Epoch [2047], val_loss: 20717.1543\n",
      "gradient norm: 2292.394073486328, minimum ratio: 0.765625\n",
      "Epoch [2048], val_loss: 20753.1504\n",
      "gradient norm: 2295.2155151367188, minimum ratio: 0.7578125\n",
      "Epoch [2049], val_loss: 20789.1914\n",
      "gradient norm: 2297.9817810058594, minimum ratio: 0.7578125\n",
      "Epoch [2050], val_loss: 20825.2832\n",
      "gradient norm: 2300.7328338623047, minimum ratio: 0.76953125\n",
      "Epoch [2051], val_loss: 20861.4199\n",
      "gradient norm: 2303.56201171875, minimum ratio: 0.75390625\n",
      "Epoch [2052], val_loss: 20897.5859\n",
      "gradient norm: 2306.348861694336, minimum ratio: 0.75390625\n",
      "Epoch [2053], val_loss: 20933.8008\n",
      "gradient norm: 2309.1546783447266, minimum ratio: 0.77734375\n",
      "Epoch [2054], val_loss: 20970.0605\n",
      "gradient norm: 2311.958526611328, minimum ratio: 0.75\n",
      "Epoch [2055], val_loss: 21006.3691\n",
      "gradient norm: 2314.744094848633, minimum ratio: 0.765625\n",
      "Epoch [2056], val_loss: 21042.7207\n",
      "gradient norm: 2317.5135498046875, minimum ratio: 0.76171875\n",
      "Epoch [2057], val_loss: 21079.1191\n",
      "gradient norm: 2320.2542419433594, minimum ratio: 0.79296875\n",
      "Epoch [2058], val_loss: 21115.5723\n",
      "gradient norm: 2322.975341796875, minimum ratio: 0.73828125\n",
      "Epoch [2059], val_loss: 21152.0625\n",
      "gradient norm: 2325.821090698242, minimum ratio: 0.78515625\n",
      "Epoch [2060], val_loss: 21188.5977\n",
      "gradient norm: 2328.668960571289, minimum ratio: 0.75390625\n",
      "Epoch [2061], val_loss: 21225.1719\n",
      "gradient norm: 2331.436477661133, minimum ratio: 0.76953125\n",
      "Epoch [2062], val_loss: 21261.7930\n",
      "gradient norm: 2334.2340545654297, minimum ratio: 0.76953125\n",
      "Epoch [2063], val_loss: 21298.4492\n",
      "gradient norm: 2337.0209350585938, minimum ratio: 0.7578125\n",
      "Epoch [2064], val_loss: 21335.1484\n",
      "gradient norm: 2339.8741455078125, minimum ratio: 0.76953125\n",
      "Epoch [2065], val_loss: 21371.8984\n",
      "gradient norm: 2342.6764526367188, minimum ratio: 0.7734375\n",
      "Epoch [2066], val_loss: 21408.6875\n",
      "gradient norm: 2345.4249420166016, minimum ratio: 0.78125\n",
      "Epoch [2067], val_loss: 21445.5273\n",
      "gradient norm: 2348.284439086914, minimum ratio: 0.76171875\n",
      "Epoch [2068], val_loss: 21482.4102\n",
      "gradient norm: 2351.103256225586, minimum ratio: 0.75\n",
      "Epoch [2069], val_loss: 21519.3359\n",
      "gradient norm: 2353.9149169921875, minimum ratio: 0.77734375\n",
      "Epoch [2070], val_loss: 21556.3066\n",
      "gradient norm: 2356.781005859375, minimum ratio: 0.765625\n",
      "Epoch [2071], val_loss: 21593.3340\n",
      "gradient norm: 2359.6512603759766, minimum ratio: 0.75390625\n",
      "Epoch [2072], val_loss: 21630.4043\n",
      "gradient norm: 2362.474151611328, minimum ratio: 0.7578125\n",
      "Epoch [2073], val_loss: 21667.5156\n",
      "gradient norm: 2365.255859375, minimum ratio: 0.76171875\n",
      "Epoch [2074], val_loss: 21704.6758\n",
      "gradient norm: 2368.131607055664, minimum ratio: 0.765625\n",
      "Epoch [2075], val_loss: 21741.8789\n",
      "gradient norm: 2370.812057495117, minimum ratio: 0.75\n",
      "Epoch [2076], val_loss: 21779.1348\n",
      "gradient norm: 2373.6919708251953, minimum ratio: 0.75\n",
      "Epoch [2077], val_loss: 21816.4297\n",
      "gradient norm: 2376.573760986328, minimum ratio: 0.77734375\n",
      "Epoch [2078], val_loss: 21853.7637\n",
      "gradient norm: 2379.4122924804688, minimum ratio: 0.73828125\n",
      "Epoch [2079], val_loss: 21891.1426\n",
      "gradient norm: 2382.269760131836, minimum ratio: 0.7734375\n",
      "Epoch [2080], val_loss: 21928.5664\n",
      "gradient norm: 2385.111618041992, minimum ratio: 0.7578125\n",
      "Epoch [2081], val_loss: 21966.0391\n",
      "gradient norm: 2387.935516357422, minimum ratio: 0.74609375\n",
      "Epoch [2082], val_loss: 22003.5508\n",
      "gradient norm: 2390.660125732422, minimum ratio: 0.7421875\n",
      "Epoch [2083], val_loss: 22041.0957\n",
      "gradient norm: 2393.5104217529297, minimum ratio: 0.7578125\n",
      "Epoch [2084], val_loss: 22078.6797\n",
      "gradient norm: 2396.401641845703, minimum ratio: 0.734375\n",
      "Epoch [2085], val_loss: 22116.3066\n",
      "gradient norm: 2399.2944946289062, minimum ratio: 0.76171875\n",
      "Epoch [2086], val_loss: 22153.9844\n",
      "gradient norm: 2402.1898345947266, minimum ratio: 0.75\n",
      "Epoch [2087], val_loss: 22191.7070\n",
      "gradient norm: 2405.0545349121094, minimum ratio: 0.76171875\n",
      "Epoch [2088], val_loss: 22229.4707\n",
      "gradient norm: 2407.9095458984375, minimum ratio: 0.7890625\n",
      "Epoch [2089], val_loss: 22267.2754\n",
      "gradient norm: 2410.722702026367, minimum ratio: 0.796875\n",
      "Epoch [2090], val_loss: 22305.1348\n",
      "gradient norm: 2413.563507080078, minimum ratio: 0.76953125\n",
      "Epoch [2091], val_loss: 22343.0371\n",
      "gradient norm: 2416.468795776367, minimum ratio: 0.77734375\n",
      "Epoch [2092], val_loss: 22380.9863\n",
      "gradient norm: 2419.3544158935547, minimum ratio: 0.765625\n",
      "Epoch [2093], val_loss: 22418.9746\n",
      "gradient norm: 2422.2294921875, minimum ratio: 0.73828125\n",
      "Epoch [2094], val_loss: 22457.0137\n",
      "gradient norm: 2425.0804443359375, minimum ratio: 0.7578125\n",
      "Epoch [2095], val_loss: 22495.1035\n",
      "gradient norm: 2427.94482421875, minimum ratio: 0.765625\n",
      "Epoch [2096], val_loss: 22533.2363\n",
      "gradient norm: 2430.761489868164, minimum ratio: 0.7421875\n",
      "Epoch [2097], val_loss: 22571.4082\n",
      "gradient norm: 2433.6791229248047, minimum ratio: 0.75390625\n",
      "Epoch [2098], val_loss: 22609.6309\n",
      "gradient norm: 2436.5301971435547, minimum ratio: 0.75\n",
      "Epoch [2099], val_loss: 22647.8887\n",
      "gradient norm: 2439.348373413086, minimum ratio: 0.734375\n",
      "Epoch [2100], val_loss: 22686.2109\n",
      "gradient norm: 2442.227584838867, minimum ratio: 0.765625\n",
      "Epoch [2101], val_loss: 22724.5723\n",
      "gradient norm: 2445.154251098633, minimum ratio: 0.734375\n",
      "Epoch [2102], val_loss: 22762.9805\n",
      "gradient norm: 2448.0833892822266, minimum ratio: 0.76953125\n",
      "Epoch [2103], val_loss: 22801.4453\n",
      "gradient norm: 2450.9755249023438, minimum ratio: 0.765625\n",
      "Epoch [2104], val_loss: 22839.9648\n",
      "gradient norm: 2453.8759765625, minimum ratio: 0.76953125\n",
      "Epoch [2105], val_loss: 22878.5293\n",
      "gradient norm: 2456.8246307373047, minimum ratio: 0.76953125\n",
      "Epoch [2106], val_loss: 22917.1309\n",
      "gradient norm: 2459.5293884277344, minimum ratio: 0.74609375\n",
      "Epoch [2107], val_loss: 22955.7695\n",
      "gradient norm: 2462.3512420654297, minimum ratio: 0.76171875\n",
      "Epoch [2108], val_loss: 22994.4570\n",
      "gradient norm: 2465.292495727539, minimum ratio: 0.7578125\n",
      "Epoch [2109], val_loss: 23033.2012\n",
      "gradient norm: 2468.2377014160156, minimum ratio: 0.76171875\n",
      "Epoch [2110], val_loss: 23071.9844\n",
      "gradient norm: 2471.1844482421875, minimum ratio: 0.765625\n",
      "Epoch [2111], val_loss: 23110.8125\n",
      "gradient norm: 2474.001190185547, minimum ratio: 0.76171875\n",
      "Epoch [2112], val_loss: 23149.6953\n",
      "gradient norm: 2476.9529418945312, minimum ratio: 0.76171875\n",
      "Epoch [2113], val_loss: 23188.6270\n",
      "gradient norm: 2479.8563690185547, minimum ratio: 0.75\n",
      "Epoch [2114], val_loss: 23227.5996\n",
      "gradient norm: 2482.763412475586, minimum ratio: 0.78515625\n",
      "Epoch [2115], val_loss: 23266.6211\n",
      "gradient norm: 2485.605438232422, minimum ratio: 0.7421875\n",
      "Epoch [2116], val_loss: 23305.6895\n",
      "gradient norm: 2488.5250396728516, minimum ratio: 0.765625\n",
      "Epoch [2117], val_loss: 23344.8105\n",
      "gradient norm: 2491.436553955078, minimum ratio: 0.7578125\n",
      "Epoch [2118], val_loss: 23383.9844\n",
      "gradient norm: 2494.317596435547, minimum ratio: 0.75390625\n",
      "Epoch [2119], val_loss: 23423.1934\n",
      "gradient norm: 2497.137252807617, minimum ratio: 0.75390625\n",
      "Epoch [2120], val_loss: 23462.4492\n",
      "gradient norm: 2500.07080078125, minimum ratio: 0.7265625\n",
      "Epoch [2121], val_loss: 23501.7578\n",
      "gradient norm: 2502.9398803710938, minimum ratio: 0.765625\n",
      "Epoch [2122], val_loss: 23541.1074\n",
      "gradient norm: 2505.883590698242, minimum ratio: 0.78125\n",
      "Epoch [2123], val_loss: 23580.5137\n",
      "gradient norm: 2508.8598022460938, minimum ratio: 0.75\n",
      "Epoch [2124], val_loss: 23619.9707\n",
      "gradient norm: 2511.8394622802734, minimum ratio: 0.76953125\n",
      "Epoch [2125], val_loss: 23659.4727\n",
      "gradient norm: 2514.7764892578125, minimum ratio: 0.7578125\n",
      "Epoch [2126], val_loss: 23699.0195\n",
      "gradient norm: 2517.6478271484375, minimum ratio: 0.76171875\n",
      "Epoch [2127], val_loss: 23738.6152\n",
      "gradient norm: 2520.6332244873047, minimum ratio: 0.75\n",
      "Epoch [2128], val_loss: 23778.2520\n",
      "gradient norm: 2523.6224365234375, minimum ratio: 0.7578125\n",
      "Epoch [2129], val_loss: 23817.9258\n",
      "gradient norm: 2526.6099700927734, minimum ratio: 0.76953125\n",
      "Epoch [2130], val_loss: 23857.6543\n",
      "gradient norm: 2529.4952392578125, minimum ratio: 0.7578125\n",
      "Epoch [2131], val_loss: 23897.4277\n",
      "gradient norm: 2532.4884643554688, minimum ratio: 0.74609375\n",
      "Epoch [2132], val_loss: 23937.2539\n",
      "gradient norm: 2535.3759002685547, minimum ratio: 0.75390625\n",
      "Epoch [2133], val_loss: 23977.1172\n",
      "gradient norm: 2538.371780395508, minimum ratio: 0.73046875\n",
      "Epoch [2134], val_loss: 24017.0293\n",
      "gradient norm: 2541.297576904297, minimum ratio: 0.765625\n",
      "Epoch [2135], val_loss: 24056.9902\n",
      "gradient norm: 2544.29833984375, minimum ratio: 0.7734375\n",
      "Epoch [2136], val_loss: 24097.0000\n",
      "gradient norm: 2547.2627716064453, minimum ratio: 0.7734375\n",
      "Epoch [2137], val_loss: 24137.0684\n",
      "gradient norm: 2550.210952758789, minimum ratio: 0.76953125\n",
      "Epoch [2138], val_loss: 24177.1797\n",
      "gradient norm: 2553.1034240722656, minimum ratio: 0.76953125\n",
      "Epoch [2139], val_loss: 24217.3398\n",
      "gradient norm: 2556.1141967773438, minimum ratio: 0.75390625\n",
      "Epoch [2140], val_loss: 24257.5527\n",
      "gradient norm: 2559.1287689208984, minimum ratio: 0.78125\n",
      "Epoch [2141], val_loss: 24297.8086\n",
      "gradient norm: 2562.0894317626953, minimum ratio: 0.7734375\n",
      "Epoch [2142], val_loss: 24338.1016\n",
      "gradient norm: 2565.0540771484375, minimum ratio: 0.77734375\n",
      "Epoch [2143], val_loss: 24378.4473\n",
      "gradient norm: 2568.0238342285156, minimum ratio: 0.78515625\n",
      "Epoch [2144], val_loss: 24418.8438\n",
      "gradient norm: 2570.9269104003906, minimum ratio: 0.7578125\n",
      "Epoch [2145], val_loss: 24459.2910\n",
      "gradient norm: 2573.8667602539062, minimum ratio: 0.7421875\n",
      "Epoch [2146], val_loss: 24499.8008\n",
      "gradient norm: 2576.845672607422, minimum ratio: 0.78125\n",
      "Epoch [2147], val_loss: 24540.3457\n",
      "gradient norm: 2579.826629638672, minimum ratio: 0.7578125\n",
      "Epoch [2148], val_loss: 24580.9375\n",
      "gradient norm: 2582.796157836914, minimum ratio: 0.74609375\n",
      "Epoch [2149], val_loss: 24621.5586\n",
      "gradient norm: 2585.8267974853516, minimum ratio: 0.78515625\n",
      "Epoch [2150], val_loss: 24662.2246\n",
      "gradient norm: 2588.8060607910156, minimum ratio: 0.77734375\n",
      "Epoch [2151], val_loss: 24702.9492\n",
      "gradient norm: 2591.784896850586, minimum ratio: 0.77734375\n",
      "Epoch [2152], val_loss: 24743.7070\n",
      "gradient norm: 2594.8201293945312, minimum ratio: 0.74609375\n",
      "Epoch [2153], val_loss: 24784.5137\n",
      "gradient norm: 2597.8580474853516, minimum ratio: 0.7734375\n",
      "Epoch [2154], val_loss: 24825.3750\n",
      "gradient norm: 2600.800308227539, minimum ratio: 0.765625\n",
      "Epoch [2155], val_loss: 24866.2871\n",
      "gradient norm: 2603.788345336914, minimum ratio: 0.7734375\n",
      "Epoch [2156], val_loss: 24907.2441\n",
      "gradient norm: 2606.646530151367, minimum ratio: 0.7578125\n",
      "Epoch [2157], val_loss: 24948.2441\n",
      "gradient norm: 2609.692657470703, minimum ratio: 0.74609375\n",
      "Epoch [2158], val_loss: 24989.2969\n",
      "gradient norm: 2612.713607788086, minimum ratio: 0.77734375\n",
      "Epoch [2159], val_loss: 25030.3965\n",
      "gradient norm: 2615.739776611328, minimum ratio: 0.765625\n",
      "Epoch [2160], val_loss: 25071.5410\n",
      "gradient norm: 2618.7173919677734, minimum ratio: 0.7578125\n",
      "Epoch [2161], val_loss: 25112.7285\n",
      "gradient norm: 2621.6854248046875, minimum ratio: 0.75\n",
      "Epoch [2162], val_loss: 25153.9629\n",
      "gradient norm: 2624.6415405273438, minimum ratio: 0.7578125\n",
      "Epoch [2163], val_loss: 25195.2461\n",
      "gradient norm: 2627.6278533935547, minimum ratio: 0.765625\n",
      "Epoch [2164], val_loss: 25236.5820\n",
      "gradient norm: 2630.6885681152344, minimum ratio: 0.7734375\n",
      "Epoch [2165], val_loss: 25277.9707\n",
      "gradient norm: 2633.7528381347656, minimum ratio: 0.75\n",
      "Epoch [2166], val_loss: 25319.4043\n",
      "gradient norm: 2636.8190155029297, minimum ratio: 0.76953125\n",
      "Epoch [2167], val_loss: 25360.8848\n",
      "gradient norm: 2639.8421783447266, minimum ratio: 0.765625\n",
      "Epoch [2168], val_loss: 25402.4102\n",
      "gradient norm: 2642.9114532470703, minimum ratio: 0.76171875\n",
      "Epoch [2169], val_loss: 25443.9883\n",
      "gradient norm: 2645.9074096679688, minimum ratio: 0.75\n",
      "Epoch [2170], val_loss: 25485.6152\n",
      "gradient norm: 2648.9821166992188, minimum ratio: 0.75\n",
      "Epoch [2171], val_loss: 25527.2734\n",
      "gradient norm: 2651.8262939453125, minimum ratio: 0.74609375\n",
      "Epoch [2172], val_loss: 25568.9785\n",
      "gradient norm: 2654.85888671875, minimum ratio: 0.76953125\n",
      "Epoch [2173], val_loss: 25610.7324\n",
      "gradient norm: 2657.845672607422, minimum ratio: 0.75390625\n",
      "Epoch [2174], val_loss: 25652.5430\n",
      "gradient norm: 2660.8802337646484, minimum ratio: 0.76171875\n",
      "Epoch [2175], val_loss: 25694.4023\n",
      "gradient norm: 2663.9632720947266, minimum ratio: 0.74609375\n",
      "Epoch [2176], val_loss: 25736.3125\n",
      "gradient norm: 2667.0030670166016, minimum ratio: 0.76171875\n",
      "Epoch [2177], val_loss: 25778.2617\n",
      "gradient norm: 2670.089141845703, minimum ratio: 0.74609375\n",
      "Epoch [2178], val_loss: 25820.2637\n",
      "gradient norm: 2673.1092987060547, minimum ratio: 0.7578125\n",
      "Epoch [2179], val_loss: 25862.3223\n",
      "gradient norm: 2676.2014923095703, minimum ratio: 0.75390625\n",
      "Epoch [2180], val_loss: 25904.4258\n",
      "gradient norm: 2679.1943969726562, minimum ratio: 0.734375\n",
      "Epoch [2181], val_loss: 25946.5762\n",
      "gradient norm: 2682.2278442382812, minimum ratio: 0.75390625\n",
      "Epoch [2182], val_loss: 25988.7754\n",
      "gradient norm: 2685.2801361083984, minimum ratio: 0.73046875\n",
      "Epoch [2183], val_loss: 26031.0176\n",
      "gradient norm: 2688.3154907226562, minimum ratio: 0.78515625\n",
      "Epoch [2184], val_loss: 26073.3047\n",
      "gradient norm: 2691.3531036376953, minimum ratio: 0.76171875\n",
      "Epoch [2185], val_loss: 26115.6465\n",
      "gradient norm: 2694.456817626953, minimum ratio: 0.734375\n",
      "Epoch [2186], val_loss: 26158.0410\n",
      "gradient norm: 2697.5081329345703, minimum ratio: 0.765625\n",
      "Epoch [2187], val_loss: 26200.4863\n",
      "gradient norm: 2700.501724243164, minimum ratio: 0.76953125\n",
      "Epoch [2188], val_loss: 26242.9922\n",
      "gradient norm: 2703.4623107910156, minimum ratio: 0.76953125\n",
      "Epoch [2189], val_loss: 26285.5449\n",
      "gradient norm: 2706.523681640625, minimum ratio: 0.7421875\n",
      "Epoch [2190], val_loss: 26328.1465\n",
      "gradient norm: 2709.6415100097656, minimum ratio: 0.7578125\n",
      "Epoch [2191], val_loss: 26370.8008\n",
      "gradient norm: 2712.7015075683594, minimum ratio: 0.7421875\n",
      "Epoch [2192], val_loss: 26413.5176\n",
      "gradient norm: 2715.727508544922, minimum ratio: 0.7734375\n",
      "Epoch [2193], val_loss: 26456.2832\n",
      "gradient norm: 2718.854019165039, minimum ratio: 0.77734375\n",
      "Epoch [2194], val_loss: 26499.0879\n",
      "gradient norm: 2721.9144744873047, minimum ratio: 0.76953125\n",
      "Epoch [2195], val_loss: 26541.9336\n",
      "gradient norm: 2725.0132598876953, minimum ratio: 0.75390625\n",
      "Epoch [2196], val_loss: 26584.8301\n",
      "gradient norm: 2728.143600463867, minimum ratio: 0.76171875\n",
      "Epoch [2197], val_loss: 26627.7891\n",
      "gradient norm: 2731.278091430664, minimum ratio: 0.7578125\n",
      "Epoch [2198], val_loss: 26670.7930\n",
      "gradient norm: 2734.2503051757812, minimum ratio: 0.765625\n",
      "Epoch [2199], val_loss: 26713.8457\n"
     ]
    }
   ],
   "source": [
    "history_1,gradient_norm_1,model_1  = fit(num_epochs, lr, model, data_loader, criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1cfd16e-4690-4920-83c9-ce79f10fbc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'gradient_norm': 1.3272387273609638, 'ratio': 0.71484375},\n",
       " 1: {'gradient_norm': 1.393603229895234, 'ratio': 0.6953125},\n",
       " 2: {'gradient_norm': 1.4600272439420223, 'ratio': 0.734375},\n",
       " 3: {'gradient_norm': 1.5265095680952072, 'ratio': 0.70703125},\n",
       " 4: {'gradient_norm': 1.5930377747863531, 'ratio': 0.7421875},\n",
       " 5: {'gradient_norm': 1.65960599668324, 'ratio': 0.70703125},\n",
       " 6: {'gradient_norm': 1.72620870731771, 'ratio': 0.75},\n",
       " 7: {'gradient_norm': 1.7928659599274397, 'ratio': 0.72265625},\n",
       " 8: {'gradient_norm': 1.8595834914594889, 'ratio': 0.71875},\n",
       " 9: {'gradient_norm': 1.9263307675719261, 'ratio': 0.71875},\n",
       " 10: {'gradient_norm': 1.9931526631116867, 'ratio': 0.76171875},\n",
       " 11: {'gradient_norm': 2.06000598333776, 'ratio': 0.71875},\n",
       " 12: {'gradient_norm': 2.1269011609256268, 'ratio': 0.70703125},\n",
       " 13: {'gradient_norm': 2.1938791386783123, 'ratio': 0.71484375},\n",
       " 14: {'gradient_norm': 2.2609098628163338, 'ratio': 0.7421875},\n",
       " 15: {'gradient_norm': 2.327999822795391, 'ratio': 0.7265625},\n",
       " 16: {'gradient_norm': 2.395144861191511, 'ratio': 0.70703125},\n",
       " 17: {'gradient_norm': 2.4623651318252087, 'ratio': 0.7421875},\n",
       " 18: {'gradient_norm': 2.5296516828238964, 'ratio': 0.75390625},\n",
       " 19: {'gradient_norm': 2.596999056637287, 'ratio': 0.73828125},\n",
       " 20: {'gradient_norm': 2.6644210927188396, 'ratio': 0.74609375},\n",
       " 21: {'gradient_norm': 2.7319148741662502, 'ratio': 0.7421875},\n",
       " 22: {'gradient_norm': 2.799479641020298, 'ratio': 0.74609375},\n",
       " 23: {'gradient_norm': 2.867116827517748, 'ratio': 0.72265625},\n",
       " 24: {'gradient_norm': 2.9348847232759, 'ratio': 0.73828125},\n",
       " 25: {'gradient_norm': 3.002690400928259, 'ratio': 0.71875},\n",
       " 26: {'gradient_norm': 3.0705968104302883, 'ratio': 0.74609375},\n",
       " 27: {'gradient_norm': 3.1385791450738907, 'ratio': 0.70703125},\n",
       " 28: {'gradient_norm': 3.2066348008811474, 'ratio': 0.73046875},\n",
       " 29: {'gradient_norm': 3.2747686244547367, 'ratio': 0.7109375},\n",
       " 30: {'gradient_norm': 3.3430255614221096, 'ratio': 0.734375},\n",
       " 31: {'gradient_norm': 3.4113754592835903, 'ratio': 0.75},\n",
       " 32: {'gradient_norm': 3.479815661907196, 'ratio': 0.74609375},\n",
       " 33: {'gradient_norm': 3.5483525954186916, 'ratio': 0.73828125},\n",
       " 34: {'gradient_norm': 3.617004584521055, 'ratio': 0.75390625},\n",
       " 35: {'gradient_norm': 3.6857659965753555, 'ratio': 0.734375},\n",
       " 36: {'gradient_norm': 3.754654549062252, 'ratio': 0.71484375},\n",
       " 37: {'gradient_norm': 3.8236809745430946, 'ratio': 0.7421875},\n",
       " 38: {'gradient_norm': 3.8927697613835335, 'ratio': 0.73046875},\n",
       " 39: {'gradient_norm': 3.9619515389204025, 'ratio': 0.75390625},\n",
       " 40: {'gradient_norm': 4.031282842159271, 'ratio': 0.71875},\n",
       " 41: {'gradient_norm': 4.100720211863518, 'ratio': 0.71875},\n",
       " 42: {'gradient_norm': 4.170297242701054, 'ratio': 0.7421875},\n",
       " 43: {'gradient_norm': 4.239966094493866, 'ratio': 0.7109375},\n",
       " 44: {'gradient_norm': 4.309758439660072, 'ratio': 0.75390625},\n",
       " 45: {'gradient_norm': 4.379709906876087, 'ratio': 0.76953125},\n",
       " 46: {'gradient_norm': 4.449781373143196, 'ratio': 0.7421875},\n",
       " 47: {'gradient_norm': 4.519995145499706, 'ratio': 0.7578125},\n",
       " 48: {'gradient_norm': 4.5903085842728615, 'ratio': 0.734375},\n",
       " 49: {'gradient_norm': 4.660805009305477, 'ratio': 0.74609375},\n",
       " 50: {'gradient_norm': 4.7314325496554375, 'ratio': 0.74609375},\n",
       " 51: {'gradient_norm': 4.80217520147562, 'ratio': 0.7421875},\n",
       " 52: {'gradient_norm': 4.873036079108715, 'ratio': 0.74609375},\n",
       " 53: {'gradient_norm': 4.944083958864212, 'ratio': 0.7265625},\n",
       " 54: {'gradient_norm': 5.0152300372719765, 'ratio': 0.75},\n",
       " 55: {'gradient_norm': 5.086542084813118, 'ratio': 0.7421875},\n",
       " 56: {'gradient_norm': 5.158000826835632, 'ratio': 0.75},\n",
       " 57: {'gradient_norm': 5.229651667177677, 'ratio': 0.74609375},\n",
       " 58: {'gradient_norm': 5.301430836319923, 'ratio': 0.74609375},\n",
       " 59: {'gradient_norm': 5.373354576528072, 'ratio': 0.7578125},\n",
       " 60: {'gradient_norm': 5.445473611354828, 'ratio': 0.7421875},\n",
       " 61: {'gradient_norm': 5.517693176865578, 'ratio': 0.7421875},\n",
       " 62: {'gradient_norm': 5.59007702767849, 'ratio': 0.734375},\n",
       " 63: {'gradient_norm': 5.6626390144228935, 'ratio': 0.7578125},\n",
       " 64: {'gradient_norm': 5.735304571688175, 'ratio': 0.7421875},\n",
       " 65: {'gradient_norm': 5.808258682489395, 'ratio': 0.76953125},\n",
       " 66: {'gradient_norm': 5.881314113736153, 'ratio': 0.7421875},\n",
       " 67: {'gradient_norm': 5.954502113163471, 'ratio': 0.73828125},\n",
       " 68: {'gradient_norm': 6.027887642383575, 'ratio': 0.74609375},\n",
       " 69: {'gradient_norm': 6.101530805230141, 'ratio': 0.76171875},\n",
       " 70: {'gradient_norm': 6.1753218322992325, 'ratio': 0.7578125},\n",
       " 71: {'gradient_norm': 6.249246880412102, 'ratio': 0.75},\n",
       " 72: {'gradient_norm': 6.3233955055475235, 'ratio': 0.7265625},\n",
       " 73: {'gradient_norm': 6.397708311676979, 'ratio': 0.73828125},\n",
       " 74: {'gradient_norm': 6.472240075469017, 'ratio': 0.734375},\n",
       " 75: {'gradient_norm': 6.546920523047447, 'ratio': 0.75390625},\n",
       " 76: {'gradient_norm': 6.621831014752388, 'ratio': 0.75},\n",
       " 77: {'gradient_norm': 6.696940362453461, 'ratio': 0.7734375},\n",
       " 78: {'gradient_norm': 6.772200435400009, 'ratio': 0.73828125},\n",
       " 79: {'gradient_norm': 6.847648486495018, 'ratio': 0.75},\n",
       " 80: {'gradient_norm': 6.923265367746353, 'ratio': 0.7421875},\n",
       " 81: {'gradient_norm': 6.999111518263817, 'ratio': 0.74609375},\n",
       " 82: {'gradient_norm': 7.075239941477776, 'ratio': 0.73828125},\n",
       " 83: {'gradient_norm': 7.151490807533264, 'ratio': 0.73828125},\n",
       " 84: {'gradient_norm': 7.228013515472412, 'ratio': 0.75390625},\n",
       " 85: {'gradient_norm': 7.30471833050251, 'ratio': 0.74609375},\n",
       " 86: {'gradient_norm': 7.381510838866234, 'ratio': 0.75390625},\n",
       " 87: {'gradient_norm': 7.458600416779518, 'ratio': 0.75390625},\n",
       " 88: {'gradient_norm': 7.535955041646957, 'ratio': 0.73828125},\n",
       " 89: {'gradient_norm': 7.613446548581123, 'ratio': 0.74609375},\n",
       " 90: {'gradient_norm': 7.691162332892418, 'ratio': 0.76171875},\n",
       " 91: {'gradient_norm': 7.769057944417, 'ratio': 0.75390625},\n",
       " 92: {'gradient_norm': 7.847233459353447, 'ratio': 0.7578125},\n",
       " 93: {'gradient_norm': 7.925640374422073, 'ratio': 0.765625},\n",
       " 94: {'gradient_norm': 8.00422140955925, 'ratio': 0.75390625},\n",
       " 95: {'gradient_norm': 8.083038315176964, 'ratio': 0.76953125},\n",
       " 96: {'gradient_norm': 8.162140935659409, 'ratio': 0.76953125},\n",
       " 97: {'gradient_norm': 8.241503208875656, 'ratio': 0.73828125},\n",
       " 98: {'gradient_norm': 8.320943683385849, 'ratio': 0.73828125},\n",
       " 99: {'gradient_norm': 8.400695115327835, 'ratio': 0.73828125},\n",
       " 100: {'gradient_norm': 8.480728149414062, 'ratio': 0.73828125},\n",
       " 101: {'gradient_norm': 8.561051845550537, 'ratio': 0.7421875},\n",
       " 102: {'gradient_norm': 8.6414086073637, 'ratio': 0.75390625},\n",
       " 103: {'gradient_norm': 8.722150832414627, 'ratio': 0.74609375},\n",
       " 104: {'gradient_norm': 8.8031345307827, 'ratio': 0.7578125},\n",
       " 105: {'gradient_norm': 8.884280383586884, 'ratio': 0.7421875},\n",
       " 106: {'gradient_norm': 8.965828150510788, 'ratio': 0.75},\n",
       " 107: {'gradient_norm': 9.047532394528389, 'ratio': 0.73828125},\n",
       " 108: {'gradient_norm': 9.129377946257591, 'ratio': 0.7421875},\n",
       " 109: {'gradient_norm': 9.211579039692879, 'ratio': 0.7578125},\n",
       " 110: {'gradient_norm': 9.294071346521378, 'ratio': 0.76171875},\n",
       " 111: {'gradient_norm': 9.376799657940865, 'ratio': 0.765625},\n",
       " 112: {'gradient_norm': 9.459779247641563, 'ratio': 0.76953125},\n",
       " 113: {'gradient_norm': 9.543135777115822, 'ratio': 0.75},\n",
       " 114: {'gradient_norm': 9.626532956957817, 'ratio': 0.7578125},\n",
       " 115: {'gradient_norm': 9.710454657673836, 'ratio': 0.78515625},\n",
       " 116: {'gradient_norm': 9.794511690735817, 'ratio': 0.7578125},\n",
       " 117: {'gradient_norm': 9.878753870725632, 'ratio': 0.76171875},\n",
       " 118: {'gradient_norm': 9.963281571865082, 'ratio': 0.7734375},\n",
       " 119: {'gradient_norm': 10.048309057950974, 'ratio': 0.7109375},\n",
       " 120: {'gradient_norm': 10.133461564779282, 'ratio': 0.74609375},\n",
       " 121: {'gradient_norm': 10.219005167484283, 'ratio': 0.74609375},\n",
       " 122: {'gradient_norm': 10.304569631814957, 'ratio': 0.75390625},\n",
       " 123: {'gradient_norm': 10.390750348567963, 'ratio': 0.75},\n",
       " 124: {'gradient_norm': 10.476885944604874, 'ratio': 0.734375},\n",
       " 125: {'gradient_norm': 10.563546687364578, 'ratio': 0.7734375},\n",
       " 126: {'gradient_norm': 10.650442332029343, 'ratio': 0.77734375},\n",
       " 127: {'gradient_norm': 10.737578839063644, 'ratio': 0.765625},\n",
       " 128: {'gradient_norm': 10.825143039226532, 'ratio': 0.73828125},\n",
       " 129: {'gradient_norm': 10.912828415632248, 'ratio': 0.7578125},\n",
       " 130: {'gradient_norm': 11.00077748298645, 'ratio': 0.74609375},\n",
       " 131: {'gradient_norm': 11.089299440383911, 'ratio': 0.73828125},\n",
       " 132: {'gradient_norm': 11.177935659885406, 'ratio': 0.75},\n",
       " 133: {'gradient_norm': 11.267056375741959, 'ratio': 0.77734375},\n",
       " 134: {'gradient_norm': 11.356210559606552, 'ratio': 0.76171875},\n",
       " 135: {'gradient_norm': 11.445740699768066, 'ratio': 0.7421875},\n",
       " 136: {'gradient_norm': 11.535787045955658, 'ratio': 0.76171875},\n",
       " 137: {'gradient_norm': 11.625998944044113, 'ratio': 0.75},\n",
       " 138: {'gradient_norm': 11.716429889202118, 'ratio': 0.75390625},\n",
       " 139: {'gradient_norm': 11.807380706071854, 'ratio': 0.73828125},\n",
       " 140: {'gradient_norm': 11.898489892482758, 'ratio': 0.7578125},\n",
       " 141: {'gradient_norm': 11.990175575017929, 'ratio': 0.7578125},\n",
       " 142: {'gradient_norm': 12.081920206546783, 'ratio': 0.75390625},\n",
       " 143: {'gradient_norm': 12.174093514680862, 'ratio': 0.7578125},\n",
       " 144: {'gradient_norm': 12.266764611005783, 'ratio': 0.75},\n",
       " 145: {'gradient_norm': 12.359584271907806, 'ratio': 0.7734375},\n",
       " 146: {'gradient_norm': 12.45274305343628, 'ratio': 0.76171875},\n",
       " 147: {'gradient_norm': 12.546266317367554, 'ratio': 0.78125},\n",
       " 148: {'gradient_norm': 12.639903485774994, 'ratio': 0.7734375},\n",
       " 149: {'gradient_norm': 12.734217673540115, 'ratio': 0.75},\n",
       " 150: {'gradient_norm': 12.828870922327042, 'ratio': 0.74609375},\n",
       " 151: {'gradient_norm': 12.923717081546783, 'ratio': 0.7421875},\n",
       " 152: {'gradient_norm': 13.018764197826385, 'ratio': 0.7421875},\n",
       " 153: {'gradient_norm': 13.114079922437668, 'ratio': 0.7734375},\n",
       " 154: {'gradient_norm': 13.210312277078629, 'ratio': 0.75390625},\n",
       " 155: {'gradient_norm': 13.306661933660507, 'ratio': 0.76171875},\n",
       " 156: {'gradient_norm': 13.403321593999863, 'ratio': 0.7734375},\n",
       " 157: {'gradient_norm': 13.500409424304962, 'ratio': 0.7578125},\n",
       " 158: {'gradient_norm': 13.597803324460983, 'ratio': 0.78125},\n",
       " 159: {'gradient_norm': 13.69547438621521, 'ratio': 0.765625},\n",
       " 160: {'gradient_norm': 13.793410956859589, 'ratio': 0.75390625},\n",
       " 161: {'gradient_norm': 13.892038762569427, 'ratio': 0.7421875},\n",
       " 162: {'gradient_norm': 13.99116462469101, 'ratio': 0.7578125},\n",
       " 163: {'gradient_norm': 14.090282768011093, 'ratio': 0.7578125},\n",
       " 164: {'gradient_norm': 14.189568132162094, 'ratio': 0.77734375},\n",
       " 165: {'gradient_norm': 14.289610713720322, 'ratio': 0.79296875},\n",
       " 166: {'gradient_norm': 14.389895975589752, 'ratio': 0.7734375},\n",
       " 167: {'gradient_norm': 14.49107575416565, 'ratio': 0.765625},\n",
       " 168: {'gradient_norm': 14.59216234087944, 'ratio': 0.74609375},\n",
       " 169: {'gradient_norm': 14.69356682896614, 'ratio': 0.7421875},\n",
       " 170: {'gradient_norm': 14.795424073934555, 'ratio': 0.7890625},\n",
       " 171: {'gradient_norm': 14.897979825735092, 'ratio': 0.76171875},\n",
       " 172: {'gradient_norm': 15.000550538301468, 'ratio': 0.78125},\n",
       " 173: {'gradient_norm': 15.103919476270676, 'ratio': 0.76953125},\n",
       " 174: {'gradient_norm': 15.207093983888626, 'ratio': 0.7421875},\n",
       " 175: {'gradient_norm': 15.310974657535553, 'ratio': 0.765625},\n",
       " 176: {'gradient_norm': 15.415560483932495, 'ratio': 0.76953125},\n",
       " 177: {'gradient_norm': 15.520171880722046, 'ratio': 0.765625},\n",
       " 178: {'gradient_norm': 15.6251540184021, 'ratio': 0.734375},\n",
       " 179: {'gradient_norm': 15.731088519096375, 'ratio': 0.765625},\n",
       " 180: {'gradient_norm': 15.837016999721527, 'ratio': 0.74609375},\n",
       " 181: {'gradient_norm': 15.943196594715118, 'ratio': 0.765625},\n",
       " 182: {'gradient_norm': 16.049849092960358, 'ratio': 0.7734375},\n",
       " 183: {'gradient_norm': 16.157190442085266, 'ratio': 0.7578125},\n",
       " 184: {'gradient_norm': 16.264725923538208, 'ratio': 0.75390625},\n",
       " 185: {'gradient_norm': 16.372973680496216, 'ratio': 0.78515625},\n",
       " 186: {'gradient_norm': 16.481441974639893, 'ratio': 0.79296875},\n",
       " 187: {'gradient_norm': 16.59033763408661, 'ratio': 0.7734375},\n",
       " 188: {'gradient_norm': 16.699710071086884, 'ratio': 0.7578125},\n",
       " 189: {'gradient_norm': 16.809578478336334, 'ratio': 0.765625},\n",
       " 190: {'gradient_norm': 16.91995882987976, 'ratio': 0.74609375},\n",
       " 191: {'gradient_norm': 17.031018614768982, 'ratio': 0.76953125},\n",
       " 192: {'gradient_norm': 17.141946732997894, 'ratio': 0.765625},\n",
       " 193: {'gradient_norm': 17.253682851791382, 'ratio': 0.75390625},\n",
       " 194: {'gradient_norm': 17.365785658359528, 'ratio': 0.78125},\n",
       " 195: {'gradient_norm': 17.478001058101654, 'ratio': 0.765625},\n",
       " 196: {'gradient_norm': 17.59096658229828, 'ratio': 0.765625},\n",
       " 197: {'gradient_norm': 17.704331815242767, 'ratio': 0.77734375},\n",
       " 198: {'gradient_norm': 17.81843614578247, 'ratio': 0.7734375},\n",
       " 199: {'gradient_norm': 17.932753264904022, 'ratio': 0.78125},\n",
       " 200: {'gradient_norm': 18.047398388385773, 'ratio': 0.78515625},\n",
       " 201: {'gradient_norm': 18.162581622600555, 'ratio': 0.765625},\n",
       " 202: {'gradient_norm': 18.27871286869049, 'ratio': 0.78125},\n",
       " 203: {'gradient_norm': 18.394768238067627, 'ratio': 0.76171875},\n",
       " 204: {'gradient_norm': 18.51147300004959, 'ratio': 0.78515625},\n",
       " 205: {'gradient_norm': 18.62841647863388, 'ratio': 0.76953125},\n",
       " 206: {'gradient_norm': 18.745935082435608, 'ratio': 0.7734375},\n",
       " 207: {'gradient_norm': 18.864170253276825, 'ratio': 0.7734375},\n",
       " 208: {'gradient_norm': 18.982711255550385, 'ratio': 0.79296875},\n",
       " 209: {'gradient_norm': 19.101886987686157, 'ratio': 0.7734375},\n",
       " 210: {'gradient_norm': 19.221373558044434, 'ratio': 0.7578125},\n",
       " 211: {'gradient_norm': 19.341625571250916, 'ratio': 0.765625},\n",
       " 212: {'gradient_norm': 19.462221086025238, 'ratio': 0.76953125},\n",
       " 213: {'gradient_norm': 19.583232522010803, 'ratio': 0.77734375},\n",
       " 214: {'gradient_norm': 19.704752206802368, 'ratio': 0.765625},\n",
       " 215: {'gradient_norm': 19.826224446296692, 'ratio': 0.80078125},\n",
       " 216: {'gradient_norm': 19.94884157180786, 'ratio': 0.78125},\n",
       " 217: {'gradient_norm': 20.07178556919098, 'ratio': 0.78125},\n",
       " 218: {'gradient_norm': 20.195055842399597, 'ratio': 0.74609375},\n",
       " 219: {'gradient_norm': 20.31881672143936, 'ratio': 0.78125},\n",
       " 220: {'gradient_norm': 20.44358789920807, 'ratio': 0.75},\n",
       " 221: {'gradient_norm': 20.568585097789764, 'ratio': 0.7734375},\n",
       " 222: {'gradient_norm': 20.694271624088287, 'ratio': 0.76171875},\n",
       " 223: {'gradient_norm': 20.820148766040802, 'ratio': 0.77734375},\n",
       " 224: {'gradient_norm': 20.94649440050125, 'ratio': 0.7421875},\n",
       " 225: {'gradient_norm': 21.07378476858139, 'ratio': 0.78515625},\n",
       " 226: {'gradient_norm': 21.201270401477814, 'ratio': 0.76171875},\n",
       " 227: {'gradient_norm': 21.329369723796844, 'ratio': 0.7578125},\n",
       " 228: {'gradient_norm': 21.457706809043884, 'ratio': 0.76953125},\n",
       " 229: {'gradient_norm': 21.58712089061737, 'ratio': 0.734375},\n",
       " 230: {'gradient_norm': 21.716824293136597, 'ratio': 0.7421875},\n",
       " 231: {'gradient_norm': 21.846672356128693, 'ratio': 0.75390625},\n",
       " 232: {'gradient_norm': 21.97707736492157, 'ratio': 0.7890625},\n",
       " 233: {'gradient_norm': 22.108453571796417, 'ratio': 0.75390625},\n",
       " 234: {'gradient_norm': 22.24029529094696, 'ratio': 0.765625},\n",
       " 235: {'gradient_norm': 22.372337222099304, 'ratio': 0.77734375},\n",
       " 236: {'gradient_norm': 22.50506627559662, 'ratio': 0.77734375},\n",
       " 237: {'gradient_norm': 22.63826984167099, 'ratio': 0.78125},\n",
       " 238: {'gradient_norm': 22.77217733860016, 'ratio': 0.77734375},\n",
       " 239: {'gradient_norm': 22.906286597251892, 'ratio': 0.76953125},\n",
       " 240: {'gradient_norm': 23.041310787200928, 'ratio': 0.7578125},\n",
       " 241: {'gradient_norm': 23.176580667495728, 'ratio': 0.76953125},\n",
       " 242: {'gradient_norm': 23.312507331371307, 'ratio': 0.77734375},\n",
       " 243: {'gradient_norm': 23.449271142482758, 'ratio': 0.7578125},\n",
       " 244: {'gradient_norm': 23.58624166250229, 'ratio': 0.78515625},\n",
       " 245: {'gradient_norm': 23.723709881305695, 'ratio': 0.7578125},\n",
       " 246: {'gradient_norm': 23.86178195476532, 'ratio': 0.765625},\n",
       " 247: {'gradient_norm': 24.000246047973633, 'ratio': 0.78125},\n",
       " 248: {'gradient_norm': 24.13915252685547, 'ratio': 0.7734375},\n",
       " 249: {'gradient_norm': 24.27927553653717, 'ratio': 0.7890625},\n",
       " 250: {'gradient_norm': 24.419912815093994, 'ratio': 0.7578125},\n",
       " 251: {'gradient_norm': 24.560725331306458, 'ratio': 0.76171875},\n",
       " 252: {'gradient_norm': 24.701594829559326, 'ratio': 0.75390625},\n",
       " 253: {'gradient_norm': 24.8441104888916, 'ratio': 0.76171875},\n",
       " 254: {'gradient_norm': 24.986870765686035, 'ratio': 0.76171875},\n",
       " 255: {'gradient_norm': 25.13000512123108, 'ratio': 0.765625},\n",
       " 256: {'gradient_norm': 25.27361249923706, 'ratio': 0.77734375},\n",
       " 257: {'gradient_norm': 25.41790008544922, 'ratio': 0.76171875},\n",
       " 258: {'gradient_norm': 25.562822937965393, 'ratio': 0.7734375},\n",
       " 259: {'gradient_norm': 25.708543181419373, 'ratio': 0.78125},\n",
       " 260: {'gradient_norm': 25.85481321811676, 'ratio': 0.78125},\n",
       " 261: {'gradient_norm': 26.00069499015808, 'ratio': 0.7734375},\n",
       " 262: {'gradient_norm': 26.14835488796234, 'ratio': 0.73828125},\n",
       " 263: {'gradient_norm': 26.29592204093933, 'ratio': 0.765625},\n",
       " 264: {'gradient_norm': 26.44367206096649, 'ratio': 0.8046875},\n",
       " 265: {'gradient_norm': 26.59276509284973, 'ratio': 0.78515625},\n",
       " 266: {'gradient_norm': 26.742352604866028, 'ratio': 0.76953125},\n",
       " 267: {'gradient_norm': 26.892893195152283, 'ratio': 0.77734375},\n",
       " 268: {'gradient_norm': 27.043786644935608, 'ratio': 0.765625},\n",
       " 269: {'gradient_norm': 27.194628596305847, 'ratio': 0.78125},\n",
       " 270: {'gradient_norm': 27.346291303634644, 'ratio': 0.734375},\n",
       " 271: {'gradient_norm': 27.49903154373169, 'ratio': 0.77734375},\n",
       " 272: {'gradient_norm': 27.652106285095215, 'ratio': 0.765625},\n",
       " 273: {'gradient_norm': 27.805696606636047, 'ratio': 0.76953125},\n",
       " 274: {'gradient_norm': 27.959954857826233, 'ratio': 0.75},\n",
       " 275: {'gradient_norm': 28.11468732357025, 'ratio': 0.76171875},\n",
       " 276: {'gradient_norm': 28.26985538005829, 'ratio': 0.76171875},\n",
       " 277: {'gradient_norm': 28.425896286964417, 'ratio': 0.77734375},\n",
       " 278: {'gradient_norm': 28.58265733718872, 'ratio': 0.75},\n",
       " 279: {'gradient_norm': 28.739858508110046, 'ratio': 0.78515625},\n",
       " 280: {'gradient_norm': 28.898081421852112, 'ratio': 0.80078125},\n",
       " 281: {'gradient_norm': 29.055899262428284, 'ratio': 0.76171875},\n",
       " 282: {'gradient_norm': 29.215160965919495, 'ratio': 0.76953125},\n",
       " 283: {'gradient_norm': 29.37424063682556, 'ratio': 0.78125},\n",
       " 284: {'gradient_norm': 29.5346759557724, 'ratio': 0.7734375},\n",
       " 285: {'gradient_norm': 29.695329904556274, 'ratio': 0.7890625},\n",
       " 286: {'gradient_norm': 29.85700237751007, 'ratio': 0.796875},\n",
       " 287: {'gradient_norm': 30.01938307285309, 'ratio': 0.78125},\n",
       " 288: {'gradient_norm': 30.181984543800354, 'ratio': 0.76953125},\n",
       " 289: {'gradient_norm': 30.345494747161865, 'ratio': 0.76171875},\n",
       " 290: {'gradient_norm': 30.509477376937866, 'ratio': 0.76953125},\n",
       " 291: {'gradient_norm': 30.67408037185669, 'ratio': 0.77734375},\n",
       " 292: {'gradient_norm': 30.839555382728577, 'ratio': 0.76953125},\n",
       " 293: {'gradient_norm': 31.006603121757507, 'ratio': 0.77734375},\n",
       " 294: {'gradient_norm': 31.1726655960083, 'ratio': 0.76953125},\n",
       " 295: {'gradient_norm': 31.339122533798218, 'ratio': 0.75390625},\n",
       " 296: {'gradient_norm': 31.50662052631378, 'ratio': 0.79296875},\n",
       " 297: {'gradient_norm': 31.675524711608887, 'ratio': 0.75390625},\n",
       " 298: {'gradient_norm': 31.845024704933167, 'ratio': 0.74609375},\n",
       " 299: {'gradient_norm': 32.01534855365753, 'ratio': 0.79296875},\n",
       " 300: {'gradient_norm': 32.185391902923584, 'ratio': 0.7578125},\n",
       " 301: {'gradient_norm': 32.35620450973511, 'ratio': 0.78125},\n",
       " 302: {'gradient_norm': 32.528305411338806, 'ratio': 0.76953125},\n",
       " 303: {'gradient_norm': 32.70118188858032, 'ratio': 0.7734375},\n",
       " 304: {'gradient_norm': 32.873958230018616, 'ratio': 0.75390625},\n",
       " 305: {'gradient_norm': 33.04788112640381, 'ratio': 0.76953125},\n",
       " 306: {'gradient_norm': 33.22168457508087, 'ratio': 0.765625},\n",
       " 307: {'gradient_norm': 33.397424817085266, 'ratio': 0.7734375},\n",
       " 308: {'gradient_norm': 33.57203781604767, 'ratio': 0.77734375},\n",
       " 309: {'gradient_norm': 33.74798285961151, 'ratio': 0.78125},\n",
       " 310: {'gradient_norm': 33.92516505718231, 'ratio': 0.765625},\n",
       " 311: {'gradient_norm': 34.102701902389526, 'ratio': 0.76953125},\n",
       " 312: {'gradient_norm': 34.282018065452576, 'ratio': 0.76953125},\n",
       " 313: {'gradient_norm': 34.46059763431549, 'ratio': 0.76953125},\n",
       " 314: {'gradient_norm': 34.6399245262146, 'ratio': 0.76953125},\n",
       " 315: {'gradient_norm': 34.819557785987854, 'ratio': 0.796875},\n",
       " 316: {'gradient_norm': 35.0008202791214, 'ratio': 0.765625},\n",
       " 317: {'gradient_norm': 35.18311631679535, 'ratio': 0.78125},\n",
       " 318: {'gradient_norm': 35.36609375476837, 'ratio': 0.75390625},\n",
       " 319: {'gradient_norm': 35.54897451400757, 'ratio': 0.7734375},\n",
       " 320: {'gradient_norm': 35.732123017311096, 'ratio': 0.765625},\n",
       " 321: {'gradient_norm': 35.91589426994324, 'ratio': 0.77734375},\n",
       " 322: {'gradient_norm': 36.10158669948578, 'ratio': 0.78125},\n",
       " 323: {'gradient_norm': 36.28791308403015, 'ratio': 0.7734375},\n",
       " 324: {'gradient_norm': 36.47458612918854, 'ratio': 0.76953125},\n",
       " 325: {'gradient_norm': 36.66194140911102, 'ratio': 0.765625},\n",
       " 326: {'gradient_norm': 36.84932553768158, 'ratio': 0.7734375},\n",
       " 327: {'gradient_norm': 37.03783309459686, 'ratio': 0.796875},\n",
       " 328: {'gradient_norm': 37.22708761692047, 'ratio': 0.79296875},\n",
       " 329: {'gradient_norm': 37.41729283332825, 'ratio': 0.73828125},\n",
       " 330: {'gradient_norm': 37.607577323913574, 'ratio': 0.77734375},\n",
       " 331: {'gradient_norm': 37.799235343933105, 'ratio': 0.79296875},\n",
       " 332: {'gradient_norm': 37.99120259284973, 'ratio': 0.76953125},\n",
       " 333: {'gradient_norm': 38.184134006500244, 'ratio': 0.7734375},\n",
       " 334: {'gradient_norm': 38.3781476020813, 'ratio': 0.7734375},\n",
       " 335: {'gradient_norm': 38.571516275405884, 'ratio': 0.796875},\n",
       " 336: {'gradient_norm': 38.76641011238098, 'ratio': 0.77734375},\n",
       " 337: {'gradient_norm': 38.962186098098755, 'ratio': 0.77734375},\n",
       " 338: {'gradient_norm': 39.158870220184326, 'ratio': 0.765625},\n",
       " 339: {'gradient_norm': 39.35637187957764, 'ratio': 0.76953125},\n",
       " 340: {'gradient_norm': 39.55440902709961, 'ratio': 0.7890625},\n",
       " 341: {'gradient_norm': 39.75271511077881, 'ratio': 0.77734375},\n",
       " 342: {'gradient_norm': 39.9521381855011, 'ratio': 0.75},\n",
       " 343: {'gradient_norm': 40.15196967124939, 'ratio': 0.7734375},\n",
       " 344: {'gradient_norm': 40.35286259651184, 'ratio': 0.78125},\n",
       " 345: {'gradient_norm': 40.55444049835205, 'ratio': 0.76953125},\n",
       " 346: {'gradient_norm': 40.75673460960388, 'ratio': 0.7578125},\n",
       " 347: {'gradient_norm': 40.95958471298218, 'ratio': 0.78125},\n",
       " 348: {'gradient_norm': 41.161980628967285, 'ratio': 0.76171875},\n",
       " 349: {'gradient_norm': 41.36566424369812, 'ratio': 0.76953125},\n",
       " 350: {'gradient_norm': 41.57059335708618, 'ratio': 0.7890625},\n",
       " 351: {'gradient_norm': 41.77624201774597, 'ratio': 0.76171875},\n",
       " 352: {'gradient_norm': 41.98330354690552, 'ratio': 0.75390625},\n",
       " 353: {'gradient_norm': 42.19046139717102, 'ratio': 0.78125},\n",
       " 354: {'gradient_norm': 42.398000717163086, 'ratio': 0.7734375},\n",
       " 355: {'gradient_norm': 42.60657238960266, 'ratio': 0.78515625},\n",
       " 356: {'gradient_norm': 42.81582307815552, 'ratio': 0.8046875},\n",
       " 357: {'gradient_norm': 43.026315689086914, 'ratio': 0.80078125},\n",
       " 358: {'gradient_norm': 43.23787975311279, 'ratio': 0.7890625},\n",
       " 359: {'gradient_norm': 43.449955701828, 'ratio': 0.80078125},\n",
       " 360: {'gradient_norm': 43.66207504272461, 'ratio': 0.76171875},\n",
       " 361: {'gradient_norm': 43.875754833221436, 'ratio': 0.76953125},\n",
       " 362: {'gradient_norm': 44.0890474319458, 'ratio': 0.78125},\n",
       " 363: {'gradient_norm': 44.30408501625061, 'ratio': 0.7734375},\n",
       " 364: {'gradient_norm': 44.51921486854553, 'ratio': 0.7890625},\n",
       " 365: {'gradient_norm': 44.73639941215515, 'ratio': 0.78125},\n",
       " 366: {'gradient_norm': 44.95401906967163, 'ratio': 0.7890625},\n",
       " 367: {'gradient_norm': 45.17104434967041, 'ratio': 0.7734375},\n",
       " 368: {'gradient_norm': 45.389514446258545, 'ratio': 0.76953125},\n",
       " 369: {'gradient_norm': 45.608667850494385, 'ratio': 0.765625},\n",
       " 370: {'gradient_norm': 45.82883167266846, 'ratio': 0.76171875},\n",
       " 371: {'gradient_norm': 46.04927158355713, 'ratio': 0.75},\n",
       " 372: {'gradient_norm': 46.271812915802, 'ratio': 0.78515625},\n",
       " 373: {'gradient_norm': 46.49439787864685, 'ratio': 0.7734375},\n",
       " 374: {'gradient_norm': 46.717085123062134, 'ratio': 0.78125},\n",
       " 375: {'gradient_norm': 46.94135761260986, 'ratio': 0.79296875},\n",
       " 376: {'gradient_norm': 47.16493463516235, 'ratio': 0.7578125},\n",
       " 377: {'gradient_norm': 47.39039158821106, 'ratio': 0.77734375},\n",
       " 378: {'gradient_norm': 47.617377042770386, 'ratio': 0.7890625},\n",
       " 379: {'gradient_norm': 47.84530830383301, 'ratio': 0.80078125},\n",
       " 380: {'gradient_norm': 48.073323249816895, 'ratio': 0.7578125},\n",
       " 381: {'gradient_norm': 48.30136156082153, 'ratio': 0.7734375},\n",
       " 382: {'gradient_norm': 48.530616760253906, 'ratio': 0.78125},\n",
       " 383: {'gradient_norm': 48.75985527038574, 'ratio': 0.78125},\n",
       " 384: {'gradient_norm': 48.992390871047974, 'ratio': 0.75},\n",
       " 385: {'gradient_norm': 49.22423076629639, 'ratio': 0.77734375},\n",
       " 386: {'gradient_norm': 49.45765280723572, 'ratio': 0.7890625},\n",
       " 387: {'gradient_norm': 49.69104790687561, 'ratio': 0.78125},\n",
       " 388: {'gradient_norm': 49.92478966712952, 'ratio': 0.77734375},\n",
       " 389: {'gradient_norm': 50.15983819961548, 'ratio': 0.7734375},\n",
       " 390: {'gradient_norm': 50.39545130729675, 'ratio': 0.76953125},\n",
       " 391: {'gradient_norm': 50.6322877407074, 'ratio': 0.7578125},\n",
       " 392: {'gradient_norm': 50.86981010437012, 'ratio': 0.7578125},\n",
       " 393: {'gradient_norm': 51.108434438705444, 'ratio': 0.78515625},\n",
       " 394: {'gradient_norm': 51.346693992614746, 'ratio': 0.79296875},\n",
       " 395: {'gradient_norm': 51.5870578289032, 'ratio': 0.7890625},\n",
       " 396: {'gradient_norm': 51.82690382003784, 'ratio': 0.7578125},\n",
       " 397: {'gradient_norm': 52.06849479675293, 'ratio': 0.80859375},\n",
       " 398: {'gradient_norm': 52.310521602630615, 'ratio': 0.78515625},\n",
       " 399: {'gradient_norm': 52.55374455451965, 'ratio': 0.765625},\n",
       " 400: {'gradient_norm': 52.7981436252594, 'ratio': 0.78125},\n",
       " 401: {'gradient_norm': 53.04361701011658, 'ratio': 0.79296875},\n",
       " 402: {'gradient_norm': 53.289395809173584, 'ratio': 0.78125},\n",
       " 403: {'gradient_norm': 53.53540110588074, 'ratio': 0.7578125},\n",
       " 404: {'gradient_norm': 53.78147268295288, 'ratio': 0.80078125},\n",
       " 405: {'gradient_norm': 54.029757022857666, 'ratio': 0.76171875},\n",
       " 406: {'gradient_norm': 54.27883434295654, 'ratio': 0.7734375},\n",
       " 407: {'gradient_norm': 54.52985692024231, 'ratio': 0.76953125},\n",
       " 408: {'gradient_norm': 54.779369592666626, 'ratio': 0.76953125},\n",
       " 409: {'gradient_norm': 55.03154897689819, 'ratio': 0.76171875},\n",
       " 410: {'gradient_norm': 55.28399419784546, 'ratio': 0.78515625},\n",
       " 411: {'gradient_norm': 55.53618621826172, 'ratio': 0.78125},\n",
       " 412: {'gradient_norm': 55.79036593437195, 'ratio': 0.7578125},\n",
       " 413: {'gradient_norm': 56.04651641845703, 'ratio': 0.78515625},\n",
       " 414: {'gradient_norm': 56.30184721946716, 'ratio': 0.7890625},\n",
       " 415: {'gradient_norm': 56.55933618545532, 'ratio': 0.78125},\n",
       " 416: {'gradient_norm': 56.81707239151001, 'ratio': 0.765625},\n",
       " 417: {'gradient_norm': 57.07542324066162, 'ratio': 0.7578125},\n",
       " 418: {'gradient_norm': 57.33316421508789, 'ratio': 0.78515625},\n",
       " 419: {'gradient_norm': 57.59260392189026, 'ratio': 0.765625},\n",
       " 420: {'gradient_norm': 57.85382914543152, 'ratio': 0.76171875},\n",
       " 421: {'gradient_norm': 58.11520075798035, 'ratio': 0.76953125},\n",
       " 422: {'gradient_norm': 58.37825870513916, 'ratio': 0.765625},\n",
       " 423: {'gradient_norm': 58.6416540145874, 'ratio': 0.7734375},\n",
       " 424: {'gradient_norm': 58.90585470199585, 'ratio': 0.77734375},\n",
       " 425: {'gradient_norm': 59.17177963256836, 'ratio': 0.7734375},\n",
       " 426: {'gradient_norm': 59.43797039985657, 'ratio': 0.7734375},\n",
       " 427: {'gradient_norm': 59.70516633987427, 'ratio': 0.78515625},\n",
       " 428: {'gradient_norm': 59.97235608100891, 'ratio': 0.7734375},\n",
       " 429: {'gradient_norm': 60.24071288108826, 'ratio': 0.77734375},\n",
       " 430: {'gradient_norm': 60.51023817062378, 'ratio': 0.80078125},\n",
       " 431: {'gradient_norm': 60.78070259094238, 'ratio': 0.8125},\n",
       " 432: {'gradient_norm': 61.05229091644287, 'ratio': 0.7890625},\n",
       " 433: {'gradient_norm': 61.323495388031006, 'ratio': 0.78515625},\n",
       " 434: {'gradient_norm': 61.59647822380066, 'ratio': 0.75390625},\n",
       " 435: {'gradient_norm': 61.87032890319824, 'ratio': 0.7734375},\n",
       " 436: {'gradient_norm': 62.14510631561279, 'ratio': 0.78515625},\n",
       " 437: {'gradient_norm': 62.42236089706421, 'ratio': 0.765625},\n",
       " 438: {'gradient_norm': 62.69838094711304, 'ratio': 0.76953125},\n",
       " 439: {'gradient_norm': 62.97634553909302, 'ratio': 0.7734375},\n",
       " 440: {'gradient_norm': 63.25493907928467, 'ratio': 0.76953125},\n",
       " 441: {'gradient_norm': 63.53308916091919, 'ratio': 0.7734375},\n",
       " 442: {'gradient_norm': 63.81291675567627, 'ratio': 0.7421875},\n",
       " 443: {'gradient_norm': 64.09394979476929, 'ratio': 0.77734375},\n",
       " 444: {'gradient_norm': 64.37492084503174, 'ratio': 0.7890625},\n",
       " 445: {'gradient_norm': 64.65894174575806, 'ratio': 0.7734375},\n",
       " 446: {'gradient_norm': 64.94199132919312, 'ratio': 0.7890625},\n",
       " 447: {'gradient_norm': 65.22618246078491, 'ratio': 0.80078125},\n",
       " 448: {'gradient_norm': 65.51132774353027, 'ratio': 0.78125},\n",
       " 449: {'gradient_norm': 65.79834318161011, 'ratio': 0.7578125},\n",
       " 450: {'gradient_norm': 66.08576917648315, 'ratio': 0.80078125},\n",
       " 451: {'gradient_norm': 66.37413454055786, 'ratio': 0.78515625},\n",
       " 452: {'gradient_norm': 66.66282272338867, 'ratio': 0.7578125},\n",
       " 453: {'gradient_norm': 66.95202589035034, 'ratio': 0.76171875},\n",
       " 454: {'gradient_norm': 67.24282932281494, 'ratio': 0.7734375},\n",
       " 455: {'gradient_norm': 67.53417444229126, 'ratio': 0.77734375},\n",
       " 456: {'gradient_norm': 67.82731199264526, 'ratio': 0.76171875},\n",
       " 457: {'gradient_norm': 68.12073516845703, 'ratio': 0.78515625},\n",
       " 458: {'gradient_norm': 68.41520071029663, 'ratio': 0.78515625},\n",
       " 459: {'gradient_norm': 68.7103796005249, 'ratio': 0.78515625},\n",
       " 460: {'gradient_norm': 69.00745391845703, 'ratio': 0.78515625},\n",
       " 461: {'gradient_norm': 69.30441379547119, 'ratio': 0.765625},\n",
       " 462: {'gradient_norm': 69.60330486297607, 'ratio': 0.765625},\n",
       " 463: {'gradient_norm': 69.90092134475708, 'ratio': 0.765625},\n",
       " 464: {'gradient_norm': 70.20157766342163, 'ratio': 0.77734375},\n",
       " 465: {'gradient_norm': 70.50252056121826, 'ratio': 0.7890625},\n",
       " 466: {'gradient_norm': 70.80384492874146, 'ratio': 0.75390625},\n",
       " 467: {'gradient_norm': 71.10536432266235, 'ratio': 0.7734375},\n",
       " 468: {'gradient_norm': 71.40991640090942, 'ratio': 0.74609375},\n",
       " 469: {'gradient_norm': 71.7146668434143, 'ratio': 0.78125},\n",
       " 470: {'gradient_norm': 72.02083444595337, 'ratio': 0.7578125},\n",
       " 471: {'gradient_norm': 72.32764959335327, 'ratio': 0.78515625},\n",
       " 472: {'gradient_norm': 72.6346788406372, 'ratio': 0.76953125},\n",
       " 473: {'gradient_norm': 72.94344711303711, 'ratio': 0.78515625},\n",
       " 474: {'gradient_norm': 73.2519941329956, 'ratio': 0.796875},\n",
       " 475: {'gradient_norm': 73.56260871887207, 'ratio': 0.76953125},\n",
       " 476: {'gradient_norm': 73.87483072280884, 'ratio': 0.796875},\n",
       " 477: {'gradient_norm': 74.18696069717407, 'ratio': 0.79296875},\n",
       " 478: {'gradient_norm': 74.50026273727417, 'ratio': 0.77734375},\n",
       " 479: {'gradient_norm': 74.81362056732178, 'ratio': 0.76171875},\n",
       " 480: {'gradient_norm': 75.12760305404663, 'ratio': 0.76953125},\n",
       " 481: {'gradient_norm': 75.44452714920044, 'ratio': 0.76953125},\n",
       " 482: {'gradient_norm': 75.76107835769653, 'ratio': 0.76953125},\n",
       " 483: {'gradient_norm': 76.0779242515564, 'ratio': 0.76171875},\n",
       " 484: {'gradient_norm': 76.3980302810669, 'ratio': 0.78515625},\n",
       " 485: {'gradient_norm': 76.71782112121582, 'ratio': 0.7734375},\n",
       " 486: {'gradient_norm': 77.03753852844238, 'ratio': 0.77734375},\n",
       " 487: {'gradient_norm': 77.35970163345337, 'ratio': 0.75390625},\n",
       " 488: {'gradient_norm': 77.68331861495972, 'ratio': 0.7734375},\n",
       " 489: {'gradient_norm': 78.00522994995117, 'ratio': 0.78125},\n",
       " 490: {'gradient_norm': 78.33030223846436, 'ratio': 0.7578125},\n",
       " 491: {'gradient_norm': 78.65545988082886, 'ratio': 0.77734375},\n",
       " 492: {'gradient_norm': 78.98190593719482, 'ratio': 0.77734375},\n",
       " 493: {'gradient_norm': 79.3086838722229, 'ratio': 0.77734375},\n",
       " 494: {'gradient_norm': 79.63442611694336, 'ratio': 0.79296875},\n",
       " 495: {'gradient_norm': 79.9650206565857, 'ratio': 0.7734375},\n",
       " 496: {'gradient_norm': 80.29489946365356, 'ratio': 0.79296875},\n",
       " 497: {'gradient_norm': 80.62655448913574, 'ratio': 0.77734375},\n",
       " 498: {'gradient_norm': 80.95906496047974, 'ratio': 0.75},\n",
       " 499: {'gradient_norm': 81.29307174682617, 'ratio': 0.8046875},\n",
       " 500: {'gradient_norm': 81.62827587127686, 'ratio': 0.765625},\n",
       " 501: {'gradient_norm': 81.9636402130127, 'ratio': 0.765625},\n",
       " 502: {'gradient_norm': 82.29958772659302, 'ratio': 0.75390625},\n",
       " 503: {'gradient_norm': 82.63730192184448, 'ratio': 0.76171875},\n",
       " 504: {'gradient_norm': 82.97468328475952, 'ratio': 0.76953125},\n",
       " 505: {'gradient_norm': 83.31357097625732, 'ratio': 0.7734375},\n",
       " 506: {'gradient_norm': 83.65399837493896, 'ratio': 0.76953125},\n",
       " 507: {'gradient_norm': 83.99475812911987, 'ratio': 0.76171875},\n",
       " 508: {'gradient_norm': 84.33842182159424, 'ratio': 0.76171875},\n",
       " 509: {'gradient_norm': 84.67926788330078, 'ratio': 0.765625},\n",
       " 510: {'gradient_norm': 85.02186822891235, 'ratio': 0.76953125},\n",
       " 511: {'gradient_norm': 85.36766147613525, 'ratio': 0.78515625},\n",
       " 512: {'gradient_norm': 85.71307849884033, 'ratio': 0.78125},\n",
       " 513: {'gradient_norm': 86.06078672409058, 'ratio': 0.79296875},\n",
       " 514: {'gradient_norm': 86.40943384170532, 'ratio': 0.7890625},\n",
       " 515: {'gradient_norm': 86.75737524032593, 'ratio': 0.78125},\n",
       " 516: {'gradient_norm': 87.107168674469, 'ratio': 0.80078125},\n",
       " 517: {'gradient_norm': 87.45681381225586, 'ratio': 0.80078125},\n",
       " 518: {'gradient_norm': 87.80831050872803, 'ratio': 0.79296875},\n",
       " 519: {'gradient_norm': 88.1602144241333, 'ratio': 0.7734375},\n",
       " 520: {'gradient_norm': 88.51313495635986, 'ratio': 0.79296875},\n",
       " 521: {'gradient_norm': 88.86808061599731, 'ratio': 0.7890625},\n",
       " 522: {'gradient_norm': 89.2248969078064, 'ratio': 0.75390625},\n",
       " 523: {'gradient_norm': 89.58228015899658, 'ratio': 0.76171875},\n",
       " 524: {'gradient_norm': 89.93952655792236, 'ratio': 0.7734375},\n",
       " 525: {'gradient_norm': 90.29809999465942, 'ratio': 0.75},\n",
       " 526: {'gradient_norm': 90.65683460235596, 'ratio': 0.7734375},\n",
       " 527: {'gradient_norm': 91.01590251922607, 'ratio': 0.7578125},\n",
       " 528: {'gradient_norm': 91.37896299362183, 'ratio': 0.75390625},\n",
       " 529: {'gradient_norm': 91.74220514297485, 'ratio': 0.76171875},\n",
       " 530: {'gradient_norm': 92.10725545883179, 'ratio': 0.7890625},\n",
       " 531: {'gradient_norm': 92.47209310531616, 'ratio': 0.77734375},\n",
       " 532: {'gradient_norm': 92.83649349212646, 'ratio': 0.74609375},\n",
       " 533: {'gradient_norm': 93.20427751541138, 'ratio': 0.7734375},\n",
       " 534: {'gradient_norm': 93.5720067024231, 'ratio': 0.76171875},\n",
       " 535: {'gradient_norm': 93.93994426727295, 'ratio': 0.78125},\n",
       " 536: {'gradient_norm': 94.3096399307251, 'ratio': 0.7734375},\n",
       " 537: {'gradient_norm': 94.68195152282715, 'ratio': 0.77734375},\n",
       " 538: {'gradient_norm': 95.0544786453247, 'ratio': 0.77734375},\n",
       " 539: {'gradient_norm': 95.42618036270142, 'ratio': 0.7734375},\n",
       " 540: {'gradient_norm': 95.79872512817383, 'ratio': 0.796875},\n",
       " 541: {'gradient_norm': 96.1725378036499, 'ratio': 0.76953125},\n",
       " 542: {'gradient_norm': 96.54936599731445, 'ratio': 0.7890625},\n",
       " 543: {'gradient_norm': 96.92601680755615, 'ratio': 0.76953125},\n",
       " 544: {'gradient_norm': 97.30422067642212, 'ratio': 0.78125},\n",
       " 545: {'gradient_norm': 97.68297386169434, 'ratio': 0.77734375},\n",
       " 546: {'gradient_norm': 98.06190061569214, 'ratio': 0.76953125},\n",
       " 547: {'gradient_norm': 98.44335746765137, 'ratio': 0.78515625},\n",
       " 548: {'gradient_norm': 98.82479953765869, 'ratio': 0.81640625},\n",
       " 549: {'gradient_norm': 99.20869159698486, 'ratio': 0.78125},\n",
       " 550: {'gradient_norm': 99.59459352493286, 'ratio': 0.78515625},\n",
       " 551: {'gradient_norm': 99.98084354400635, 'ratio': 0.76171875},\n",
       " 552: {'gradient_norm': 100.36773490905762, 'ratio': 0.7734375},\n",
       " 553: {'gradient_norm': 100.75410842895508, 'ratio': 0.77734375},\n",
       " 554: {'gradient_norm': 101.14337396621704, 'ratio': 0.77734375},\n",
       " 555: {'gradient_norm': 101.53321695327759, 'ratio': 0.7578125},\n",
       " 556: {'gradient_norm': 101.92359924316406, 'ratio': 0.7734375},\n",
       " 557: {'gradient_norm': 102.31142807006836, 'ratio': 0.80078125},\n",
       " 558: {'gradient_norm': 102.70513725280762, 'ratio': 0.7578125},\n",
       " 559: {'gradient_norm': 103.10001182556152, 'ratio': 0.7890625},\n",
       " 560: {'gradient_norm': 103.49290084838867, 'ratio': 0.8125},\n",
       " 561: {'gradient_norm': 103.88801527023315, 'ratio': 0.7890625},\n",
       " 562: {'gradient_norm': 104.28399515151978, 'ratio': 0.79296875},\n",
       " 563: {'gradient_norm': 104.6810998916626, 'ratio': 0.78515625},\n",
       " 564: {'gradient_norm': 105.08060073852539, 'ratio': 0.76953125},\n",
       " 565: {'gradient_norm': 105.4807538986206, 'ratio': 0.78125},\n",
       " 566: {'gradient_norm': 105.88211250305176, 'ratio': 0.7734375},\n",
       " 567: {'gradient_norm': 106.28402137756348, 'ratio': 0.75390625},\n",
       " 568: {'gradient_norm': 106.68685531616211, 'ratio': 0.796875},\n",
       " 569: {'gradient_norm': 107.09137439727783, 'ratio': 0.7734375},\n",
       " 570: {'gradient_norm': 107.49501514434814, 'ratio': 0.78125},\n",
       " 571: {'gradient_norm': 107.90210819244385, 'ratio': 0.79296875},\n",
       " 572: {'gradient_norm': 108.30981922149658, 'ratio': 0.79296875},\n",
       " 573: {'gradient_norm': 108.71810817718506, 'ratio': 0.78125},\n",
       " 574: {'gradient_norm': 109.12621974945068, 'ratio': 0.78125},\n",
       " 575: {'gradient_norm': 109.5377721786499, 'ratio': 0.76953125},\n",
       " 576: {'gradient_norm': 109.94795608520508, 'ratio': 0.78125},\n",
       " 577: {'gradient_norm': 110.36198616027832, 'ratio': 0.79296875},\n",
       " 578: {'gradient_norm': 110.77515125274658, 'ratio': 0.78125},\n",
       " 579: {'gradient_norm': 111.18991661071777, 'ratio': 0.8125},\n",
       " 580: {'gradient_norm': 111.60450553894043, 'ratio': 0.7734375},\n",
       " 581: {'gradient_norm': 112.02070236206055, 'ratio': 0.79296875},\n",
       " 582: {'gradient_norm': 112.43900871276855, 'ratio': 0.77734375},\n",
       " 583: {'gradient_norm': 112.85716915130615, 'ratio': 0.76953125},\n",
       " 584: {'gradient_norm': 113.2749490737915, 'ratio': 0.79296875},\n",
       " 585: {'gradient_norm': 113.6964168548584, 'ratio': 0.7734375},\n",
       " 586: {'gradient_norm': 114.12178039550781, 'ratio': 0.76953125},\n",
       " 587: {'gradient_norm': 114.54378890991211, 'ratio': 0.77734375},\n",
       " 588: {'gradient_norm': 114.96851253509521, 'ratio': 0.78125},\n",
       " 589: {'gradient_norm': 115.39417934417725, 'ratio': 0.77734375},\n",
       " 590: {'gradient_norm': 115.8220272064209, 'ratio': 0.79296875},\n",
       " 591: {'gradient_norm': 116.24864101409912, 'ratio': 0.75390625},\n",
       " 592: {'gradient_norm': 116.67652225494385, 'ratio': 0.78515625},\n",
       " 593: {'gradient_norm': 117.10581874847412, 'ratio': 0.80078125},\n",
       " 594: {'gradient_norm': 117.53582382202148, 'ratio': 0.765625},\n",
       " 595: {'gradient_norm': 117.96873664855957, 'ratio': 0.77734375},\n",
       " 596: {'gradient_norm': 118.40078449249268, 'ratio': 0.79296875},\n",
       " 597: {'gradient_norm': 118.83394622802734, 'ratio': 0.76953125},\n",
       " 598: {'gradient_norm': 119.26921558380127, 'ratio': 0.765625},\n",
       " 599: {'gradient_norm': 119.70642566680908, 'ratio': 0.7734375},\n",
       " 600: {'gradient_norm': 120.14404392242432, 'ratio': 0.765625},\n",
       " 601: {'gradient_norm': 120.5799560546875, 'ratio': 0.7734375},\n",
       " 602: {'gradient_norm': 121.01974582672119, 'ratio': 0.76953125},\n",
       " 603: {'gradient_norm': 121.45957660675049, 'ratio': 0.765625},\n",
       " 604: {'gradient_norm': 121.90208911895752, 'ratio': 0.75390625},\n",
       " 605: {'gradient_norm': 122.34523487091064, 'ratio': 0.79296875},\n",
       " 606: {'gradient_norm': 122.78884696960449, 'ratio': 0.75390625},\n",
       " 607: {'gradient_norm': 123.23323440551758, 'ratio': 0.76953125},\n",
       " 608: {'gradient_norm': 123.67729759216309, 'ratio': 0.76171875},\n",
       " 609: {'gradient_norm': 124.1237096786499, 'ratio': 0.76171875},\n",
       " 610: {'gradient_norm': 124.57234954833984, 'ratio': 0.76953125},\n",
       " 611: {'gradient_norm': 125.02194404602051, 'ratio': 0.75390625},\n",
       " 612: {'gradient_norm': 125.46933841705322, 'ratio': 0.7734375},\n",
       " 613: {'gradient_norm': 125.92263984680176, 'ratio': 0.79296875},\n",
       " 614: {'gradient_norm': 126.37499809265137, 'ratio': 0.73828125},\n",
       " 615: {'gradient_norm': 126.82997989654541, 'ratio': 0.78125},\n",
       " 616: {'gradient_norm': 127.28324604034424, 'ratio': 0.7890625},\n",
       " 617: {'gradient_norm': 127.73885345458984, 'ratio': 0.765625},\n",
       " 618: {'gradient_norm': 128.19572162628174, 'ratio': 0.76171875},\n",
       " 619: {'gradient_norm': 128.65277862548828, 'ratio': 0.76953125},\n",
       " 620: {'gradient_norm': 129.11099529266357, 'ratio': 0.78125},\n",
       " 621: {'gradient_norm': 129.57083702087402, 'ratio': 0.76953125},\n",
       " 622: {'gradient_norm': 130.03199100494385, 'ratio': 0.796875},\n",
       " 623: {'gradient_norm': 130.49621200561523, 'ratio': 0.75},\n",
       " 624: {'gradient_norm': 130.9570655822754, 'ratio': 0.78125},\n",
       " 625: {'gradient_norm': 131.42246532440186, 'ratio': 0.765625},\n",
       " 626: {'gradient_norm': 131.88705348968506, 'ratio': 0.765625},\n",
       " 627: {'gradient_norm': 132.3541660308838, 'ratio': 0.7578125},\n",
       " 628: {'gradient_norm': 132.82104682922363, 'ratio': 0.78125},\n",
       " 629: {'gradient_norm': 133.28904914855957, 'ratio': 0.77734375},\n",
       " 630: {'gradient_norm': 133.7587251663208, 'ratio': 0.7734375},\n",
       " 631: {'gradient_norm': 134.2297945022583, 'ratio': 0.75},\n",
       " 632: {'gradient_norm': 134.70373916625977, 'ratio': 0.7578125},\n",
       " 633: {'gradient_norm': 135.1750316619873, 'ratio': 0.765625},\n",
       " 634: {'gradient_norm': 135.6498737335205, 'ratio': 0.78515625},\n",
       " 635: {'gradient_norm': 136.12538051605225, 'ratio': 0.7421875},\n",
       " 636: {'gradient_norm': 136.60237312316895, 'ratio': 0.765625},\n",
       " 637: {'gradient_norm': 137.08086585998535, 'ratio': 0.78125},\n",
       " 638: {'gradient_norm': 137.56199741363525, 'ratio': 0.7734375},\n",
       " 639: {'gradient_norm': 138.03937339782715, 'ratio': 0.7734375},\n",
       " 640: {'gradient_norm': 138.52101039886475, 'ratio': 0.7734375},\n",
       " 641: {'gradient_norm': 139.00224685668945, 'ratio': 0.734375},\n",
       " 642: {'gradient_norm': 139.48856830596924, 'ratio': 0.77734375},\n",
       " 643: {'gradient_norm': 139.97405529022217, 'ratio': 0.77734375},\n",
       " 644: {'gradient_norm': 140.4592456817627, 'ratio': 0.78515625},\n",
       " 645: {'gradient_norm': 140.94734001159668, 'ratio': 0.7734375},\n",
       " 646: {'gradient_norm': 141.43645477294922, 'ratio': 0.76953125},\n",
       " 647: {'gradient_norm': 141.9244089126587, 'ratio': 0.7734375},\n",
       " 648: {'gradient_norm': 142.41310024261475, 'ratio': 0.765625},\n",
       " 649: {'gradient_norm': 142.89972972869873, 'ratio': 0.8125},\n",
       " 650: {'gradient_norm': 143.3933343887329, 'ratio': 0.76171875},\n",
       " 651: {'gradient_norm': 143.88767337799072, 'ratio': 0.796875},\n",
       " 652: {'gradient_norm': 144.38434982299805, 'ratio': 0.765625},\n",
       " 653: {'gradient_norm': 144.87919521331787, 'ratio': 0.78515625},\n",
       " 654: {'gradient_norm': 145.3778419494629, 'ratio': 0.76953125},\n",
       " 655: {'gradient_norm': 145.87808513641357, 'ratio': 0.7734375},\n",
       " 656: {'gradient_norm': 146.37808799743652, 'ratio': 0.75390625},\n",
       " 657: {'gradient_norm': 146.88084888458252, 'ratio': 0.765625},\n",
       " 658: {'gradient_norm': 147.38383197784424, 'ratio': 0.78125},\n",
       " 659: {'gradient_norm': 147.88356494903564, 'ratio': 0.7734375},\n",
       " 660: {'gradient_norm': 148.38894844055176, 'ratio': 0.796875},\n",
       " 661: {'gradient_norm': 148.89534950256348, 'ratio': 0.74609375},\n",
       " 662: {'gradient_norm': 149.40384674072266, 'ratio': 0.77734375},\n",
       " 663: {'gradient_norm': 149.90803718566895, 'ratio': 0.77734375},\n",
       " 664: {'gradient_norm': 150.41395568847656, 'ratio': 0.76953125},\n",
       " 665: {'gradient_norm': 150.9248924255371, 'ratio': 0.78125},\n",
       " 666: {'gradient_norm': 151.43810939788818, 'ratio': 0.7578125},\n",
       " 667: {'gradient_norm': 151.95223236083984, 'ratio': 0.78515625},\n",
       " 668: {'gradient_norm': 152.46451377868652, 'ratio': 0.7890625},\n",
       " 669: {'gradient_norm': 152.97788429260254, 'ratio': 0.76953125},\n",
       " 670: {'gradient_norm': 153.49491596221924, 'ratio': 0.7890625},\n",
       " 671: {'gradient_norm': 154.00998306274414, 'ratio': 0.77734375},\n",
       " 672: {'gradient_norm': 154.530424118042, 'ratio': 0.78125},\n",
       " 673: {'gradient_norm': 155.05012798309326, 'ratio': 0.77734375},\n",
       " 674: {'gradient_norm': 155.5706615447998, 'ratio': 0.78515625},\n",
       " 675: {'gradient_norm': 156.09056758880615, 'ratio': 0.77734375},\n",
       " 676: {'gradient_norm': 156.61608219146729, 'ratio': 0.78125},\n",
       " 677: {'gradient_norm': 157.13659477233887, 'ratio': 0.765625},\n",
       " 678: {'gradient_norm': 157.6606683731079, 'ratio': 0.7734375},\n",
       " 679: {'gradient_norm': 158.18243026733398, 'ratio': 0.765625},\n",
       " 680: {'gradient_norm': 158.70897102355957, 'ratio': 0.77734375},\n",
       " 681: {'gradient_norm': 159.2397165298462, 'ratio': 0.7890625},\n",
       " 682: {'gradient_norm': 159.77209854125977, 'ratio': 0.7734375},\n",
       " 683: {'gradient_norm': 160.30391883850098, 'ratio': 0.76953125},\n",
       " 684: {'gradient_norm': 160.83562755584717, 'ratio': 0.76953125},\n",
       " 685: {'gradient_norm': 161.36936473846436, 'ratio': 0.7734375},\n",
       " 686: {'gradient_norm': 161.90105724334717, 'ratio': 0.7890625},\n",
       " 687: {'gradient_norm': 162.43699264526367, 'ratio': 0.78125},\n",
       " 688: {'gradient_norm': 162.9759702682495, 'ratio': 0.7890625},\n",
       " 689: {'gradient_norm': 163.51337909698486, 'ratio': 0.77734375},\n",
       " 690: {'gradient_norm': 164.05012702941895, 'ratio': 0.7890625},\n",
       " 691: {'gradient_norm': 164.59241199493408, 'ratio': 0.77734375},\n",
       " 692: {'gradient_norm': 165.13508319854736, 'ratio': 0.7578125},\n",
       " 693: {'gradient_norm': 165.6781349182129, 'ratio': 0.765625},\n",
       " 694: {'gradient_norm': 166.2239122390747, 'ratio': 0.76171875},\n",
       " 695: {'gradient_norm': 166.76664638519287, 'ratio': 0.7734375},\n",
       " 696: {'gradient_norm': 167.30982494354248, 'ratio': 0.79296875},\n",
       " 697: {'gradient_norm': 167.85913372039795, 'ratio': 0.765625},\n",
       " 698: {'gradient_norm': 168.40788459777832, 'ratio': 0.78515625},\n",
       " 699: {'gradient_norm': 168.9559907913208, 'ratio': 0.77734375},\n",
       " 700: {'gradient_norm': 169.50634860992432, 'ratio': 0.77734375},\n",
       " 701: {'gradient_norm': 170.0580530166626, 'ratio': 0.7578125},\n",
       " 702: {'gradient_norm': 170.612286567688, 'ratio': 0.76953125},\n",
       " 703: {'gradient_norm': 171.1672601699829, 'ratio': 0.78515625},\n",
       " 704: {'gradient_norm': 171.7224817276001, 'ratio': 0.78515625},\n",
       " 705: {'gradient_norm': 172.27638912200928, 'ratio': 0.7734375},\n",
       " 706: {'gradient_norm': 172.83104991912842, 'ratio': 0.77734375},\n",
       " 707: {'gradient_norm': 173.3898515701294, 'ratio': 0.79296875},\n",
       " 708: {'gradient_norm': 173.95052242279053, 'ratio': 0.7890625},\n",
       " 709: {'gradient_norm': 174.51312065124512, 'ratio': 0.80078125},\n",
       " 710: {'gradient_norm': 175.07687950134277, 'ratio': 0.78125},\n",
       " 711: {'gradient_norm': 175.64063930511475, 'ratio': 0.8125},\n",
       " 712: {'gradient_norm': 176.20371055603027, 'ratio': 0.7734375},\n",
       " 713: {'gradient_norm': 176.7735834121704, 'ratio': 0.78125},\n",
       " 714: {'gradient_norm': 177.34297466278076, 'ratio': 0.7421875},\n",
       " 715: {'gradient_norm': 177.91159057617188, 'ratio': 0.765625},\n",
       " 716: {'gradient_norm': 178.47869968414307, 'ratio': 0.78125},\n",
       " 717: {'gradient_norm': 179.05125522613525, 'ratio': 0.80078125},\n",
       " 718: {'gradient_norm': 179.61922550201416, 'ratio': 0.77734375},\n",
       " 719: {'gradient_norm': 180.19378662109375, 'ratio': 0.7734375},\n",
       " 720: {'gradient_norm': 180.76961612701416, 'ratio': 0.76171875},\n",
       " 721: {'gradient_norm': 181.3447093963623, 'ratio': 0.7890625},\n",
       " 722: {'gradient_norm': 181.9206418991089, 'ratio': 0.76953125},\n",
       " 723: {'gradient_norm': 182.49659252166748, 'ratio': 0.78125},\n",
       " 724: {'gradient_norm': 183.07938385009766, 'ratio': 0.78125},\n",
       " 725: {'gradient_norm': 183.65770530700684, 'ratio': 0.796875},\n",
       " 726: {'gradient_norm': 184.24008560180664, 'ratio': 0.7578125},\n",
       " 727: {'gradient_norm': 184.81927299499512, 'ratio': 0.7734375},\n",
       " 728: {'gradient_norm': 185.40628242492676, 'ratio': 0.765625},\n",
       " 729: {'gradient_norm': 185.98951530456543, 'ratio': 0.78125},\n",
       " 730: {'gradient_norm': 186.5799674987793, 'ratio': 0.76953125},\n",
       " 731: {'gradient_norm': 187.16963386535645, 'ratio': 0.79296875},\n",
       " 732: {'gradient_norm': 187.75705528259277, 'ratio': 0.77734375},\n",
       " 733: {'gradient_norm': 188.34965133666992, 'ratio': 0.76171875},\n",
       " 734: {'gradient_norm': 188.94366645812988, 'ratio': 0.77734375},\n",
       " 735: {'gradient_norm': 189.5329532623291, 'ratio': 0.765625},\n",
       " 736: {'gradient_norm': 190.12424278259277, 'ratio': 0.76171875},\n",
       " 737: {'gradient_norm': 190.7155990600586, 'ratio': 0.7421875},\n",
       " 738: {'gradient_norm': 191.31289291381836, 'ratio': 0.74609375},\n",
       " 739: {'gradient_norm': 191.91073417663574, 'ratio': 0.75390625},\n",
       " 740: {'gradient_norm': 192.51129150390625, 'ratio': 0.77734375},\n",
       " 741: {'gradient_norm': 193.11260604858398, 'ratio': 0.77734375},\n",
       " 742: {'gradient_norm': 193.71748733520508, 'ratio': 0.7734375},\n",
       " 743: {'gradient_norm': 194.32325172424316, 'ratio': 0.76171875},\n",
       " 744: {'gradient_norm': 194.92353057861328, 'ratio': 0.7734375},\n",
       " 745: {'gradient_norm': 195.52915000915527, 'ratio': 0.765625},\n",
       " 746: {'gradient_norm': 196.1353702545166, 'ratio': 0.765625},\n",
       " 747: {'gradient_norm': 196.73957061767578, 'ratio': 0.75390625},\n",
       " 748: {'gradient_norm': 197.34916496276855, 'ratio': 0.78125},\n",
       " 749: {'gradient_norm': 197.95527267456055, 'ratio': 0.77734375},\n",
       " 750: {'gradient_norm': 198.5626564025879, 'ratio': 0.76953125},\n",
       " 751: {'gradient_norm': 199.1772632598877, 'ratio': 0.73828125},\n",
       " 752: {'gradient_norm': 199.78933143615723, 'ratio': 0.79296875},\n",
       " 753: {'gradient_norm': 200.40295791625977, 'ratio': 0.79296875},\n",
       " 754: {'gradient_norm': 201.01840209960938, 'ratio': 0.7734375},\n",
       " 755: {'gradient_norm': 201.63680458068848, 'ratio': 0.77734375},\n",
       " 756: {'gradient_norm': 202.2532787322998, 'ratio': 0.77734375},\n",
       " 757: {'gradient_norm': 202.8735294342041, 'ratio': 0.79296875},\n",
       " 758: {'gradient_norm': 203.49342155456543, 'ratio': 0.76171875},\n",
       " 759: {'gradient_norm': 204.1150951385498, 'ratio': 0.7734375},\n",
       " 760: {'gradient_norm': 204.73562622070312, 'ratio': 0.76171875},\n",
       " 761: {'gradient_norm': 205.35809516906738, 'ratio': 0.78125},\n",
       " 762: {'gradient_norm': 205.9850730895996, 'ratio': 0.77734375},\n",
       " 763: {'gradient_norm': 206.6116008758545, 'ratio': 0.765625},\n",
       " 764: {'gradient_norm': 207.2398796081543, 'ratio': 0.77734375},\n",
       " 765: {'gradient_norm': 207.8736057281494, 'ratio': 0.73046875},\n",
       " 766: {'gradient_norm': 208.5038833618164, 'ratio': 0.75},\n",
       " 767: {'gradient_norm': 209.13631057739258, 'ratio': 0.78515625},\n",
       " 768: {'gradient_norm': 209.76690673828125, 'ratio': 0.78125},\n",
       " 769: {'gradient_norm': 210.4011573791504, 'ratio': 0.80078125},\n",
       " 770: {'gradient_norm': 211.03765678405762, 'ratio': 0.76171875},\n",
       " 771: {'gradient_norm': 211.67300415039062, 'ratio': 0.76953125},\n",
       " 772: {'gradient_norm': 212.31138801574707, 'ratio': 0.7890625},\n",
       " 773: {'gradient_norm': 212.9522533416748, 'ratio': 0.76953125},\n",
       " 774: {'gradient_norm': 213.59198188781738, 'ratio': 0.77734375},\n",
       " 775: {'gradient_norm': 214.23265266418457, 'ratio': 0.7421875},\n",
       " 776: {'gradient_norm': 214.87779235839844, 'ratio': 0.79296875},\n",
       " 777: {'gradient_norm': 215.52118301391602, 'ratio': 0.7578125},\n",
       " 778: {'gradient_norm': 216.17082023620605, 'ratio': 0.7734375},\n",
       " 779: {'gradient_norm': 216.81902694702148, 'ratio': 0.73828125},\n",
       " 780: {'gradient_norm': 217.46926498413086, 'ratio': 0.765625},\n",
       " 781: {'gradient_norm': 218.11647415161133, 'ratio': 0.765625},\n",
       " 782: {'gradient_norm': 218.77127647399902, 'ratio': 0.7734375},\n",
       " 783: {'gradient_norm': 219.41819381713867, 'ratio': 0.78515625},\n",
       " 784: {'gradient_norm': 220.069974899292, 'ratio': 0.78515625},\n",
       " 785: {'gradient_norm': 220.7212905883789, 'ratio': 0.7578125},\n",
       " 786: {'gradient_norm': 221.37624168395996, 'ratio': 0.75390625},\n",
       " 787: {'gradient_norm': 222.03577041625977, 'ratio': 0.78125},\n",
       " 788: {'gradient_norm': 222.69427680969238, 'ratio': 0.77734375},\n",
       " 789: {'gradient_norm': 223.35394096374512, 'ratio': 0.79296875},\n",
       " 790: {'gradient_norm': 224.02195930480957, 'ratio': 0.8046875},\n",
       " 791: {'gradient_norm': 224.6826343536377, 'ratio': 0.7734375},\n",
       " 792: {'gradient_norm': 225.34439659118652, 'ratio': 0.80078125},\n",
       " 793: {'gradient_norm': 226.00977516174316, 'ratio': 0.77734375},\n",
       " 794: {'gradient_norm': 226.67813110351562, 'ratio': 0.765625},\n",
       " 795: {'gradient_norm': 227.34686470031738, 'ratio': 0.75390625},\n",
       " 796: {'gradient_norm': 228.01238822937012, 'ratio': 0.7734375},\n",
       " 797: {'gradient_norm': 228.68229866027832, 'ratio': 0.75},\n",
       " 798: {'gradient_norm': 229.35350608825684, 'ratio': 0.76953125},\n",
       " 799: {'gradient_norm': 230.0253143310547, 'ratio': 0.77734375},\n",
       " 800: {'gradient_norm': 230.6963176727295, 'ratio': 0.7734375},\n",
       " 801: {'gradient_norm': 231.3713550567627, 'ratio': 0.78515625},\n",
       " 802: {'gradient_norm': 232.04792594909668, 'ratio': 0.7890625},\n",
       " 803: {'gradient_norm': 232.7255744934082, 'ratio': 0.7734375},\n",
       " 804: {'gradient_norm': 233.40734672546387, 'ratio': 0.77734375},\n",
       " 805: {'gradient_norm': 234.0898838043213, 'ratio': 0.7734375},\n",
       " 806: {'gradient_norm': 234.76902389526367, 'ratio': 0.80078125},\n",
       " 807: {'gradient_norm': 235.45489311218262, 'ratio': 0.78125},\n",
       " 808: {'gradient_norm': 236.14299774169922, 'ratio': 0.76171875},\n",
       " 809: {'gradient_norm': 236.82660484313965, 'ratio': 0.7890625},\n",
       " 810: {'gradient_norm': 237.51228141784668, 'ratio': 0.76953125},\n",
       " 811: {'gradient_norm': 238.1976318359375, 'ratio': 0.7734375},\n",
       " 812: {'gradient_norm': 238.89006233215332, 'ratio': 0.78515625},\n",
       " 813: {'gradient_norm': 239.58292770385742, 'ratio': 0.8046875},\n",
       " 814: {'gradient_norm': 240.2773151397705, 'ratio': 0.7734375},\n",
       " 815: {'gradient_norm': 240.96793174743652, 'ratio': 0.765625},\n",
       " 816: {'gradient_norm': 241.66020011901855, 'ratio': 0.78125},\n",
       " 817: {'gradient_norm': 242.3523120880127, 'ratio': 0.77734375},\n",
       " 818: {'gradient_norm': 243.04656600952148, 'ratio': 0.796875},\n",
       " 819: {'gradient_norm': 243.74719619750977, 'ratio': 0.77734375},\n",
       " 820: {'gradient_norm': 244.45038414001465, 'ratio': 0.76953125},\n",
       " 821: {'gradient_norm': 245.1504306793213, 'ratio': 0.765625},\n",
       " 822: {'gradient_norm': 245.8505916595459, 'ratio': 0.76171875},\n",
       " 823: {'gradient_norm': 246.55231857299805, 'ratio': 0.765625},\n",
       " 824: {'gradient_norm': 247.26054191589355, 'ratio': 0.77734375},\n",
       " 825: {'gradient_norm': 247.97056007385254, 'ratio': 0.75390625},\n",
       " 826: {'gradient_norm': 248.6803684234619, 'ratio': 0.74609375},\n",
       " 827: {'gradient_norm': 249.3879909515381, 'ratio': 0.78125},\n",
       " 828: {'gradient_norm': 250.0986270904541, 'ratio': 0.78515625},\n",
       " 829: {'gradient_norm': 250.80297088623047, 'ratio': 0.76171875},\n",
       " 830: {'gradient_norm': 251.52062034606934, 'ratio': 0.78125},\n",
       " 831: {'gradient_norm': 252.23607635498047, 'ratio': 0.76953125},\n",
       " 832: {'gradient_norm': 252.95202827453613, 'ratio': 0.78125},\n",
       " 833: {'gradient_norm': 253.66607856750488, 'ratio': 0.7890625},\n",
       " 834: {'gradient_norm': 254.3817310333252, 'ratio': 0.79296875},\n",
       " 835: {'gradient_norm': 255.0957794189453, 'ratio': 0.79296875},\n",
       " 836: {'gradient_norm': 255.8096809387207, 'ratio': 0.78125},\n",
       " 837: {'gradient_norm': 256.53604888916016, 'ratio': 0.7421875},\n",
       " 838: {'gradient_norm': 257.26398849487305, 'ratio': 0.7578125},\n",
       " 839: {'gradient_norm': 257.9896011352539, 'ratio': 0.7890625},\n",
       " 840: {'gradient_norm': 258.71533584594727, 'ratio': 0.7734375},\n",
       " 841: {'gradient_norm': 259.44480323791504, 'ratio': 0.75390625},\n",
       " 842: {'gradient_norm': 260.1748390197754, 'ratio': 0.765625},\n",
       " 843: {'gradient_norm': 260.90869903564453, 'ratio': 0.765625},\n",
       " 844: {'gradient_norm': 261.63901138305664, 'ratio': 0.75},\n",
       " 845: {'gradient_norm': 262.37139892578125, 'ratio': 0.77734375},\n",
       " 846: {'gradient_norm': 263.1032524108887, 'ratio': 0.7734375},\n",
       " 847: {'gradient_norm': 263.8401679992676, 'ratio': 0.75390625},\n",
       " 848: {'gradient_norm': 264.57572174072266, 'ratio': 0.7890625},\n",
       " 849: {'gradient_norm': 265.31248474121094, 'ratio': 0.765625},\n",
       " 850: {'gradient_norm': 266.0496139526367, 'ratio': 0.75},\n",
       " 851: {'gradient_norm': 266.7931900024414, 'ratio': 0.77734375},\n",
       " 852: {'gradient_norm': 267.530574798584, 'ratio': 0.7734375},\n",
       " 853: {'gradient_norm': 268.2720031738281, 'ratio': 0.7890625},\n",
       " 854: {'gradient_norm': 269.0132637023926, 'ratio': 0.7734375},\n",
       " 855: {'gradient_norm': 269.75199127197266, 'ratio': 0.7578125},\n",
       " 856: {'gradient_norm': 270.49994468688965, 'ratio': 0.78515625},\n",
       " 857: {'gradient_norm': 271.25304412841797, 'ratio': 0.796875},\n",
       " 858: {'gradient_norm': 272.00281715393066, 'ratio': 0.77734375},\n",
       " 859: {'gradient_norm': 272.7539348602295, 'ratio': 0.77734375},\n",
       " 860: {'gradient_norm': 273.5077819824219, 'ratio': 0.78125},\n",
       " 861: {'gradient_norm': 274.2638111114502, 'ratio': 0.76953125},\n",
       " 862: {'gradient_norm': 275.01485443115234, 'ratio': 0.76953125},\n",
       " 863: {'gradient_norm': 275.7610092163086, 'ratio': 0.78125},\n",
       " 864: {'gradient_norm': 276.521520614624, 'ratio': 0.75390625},\n",
       " 865: {'gradient_norm': 277.28448486328125, 'ratio': 0.75390625},\n",
       " 866: {'gradient_norm': 278.0450954437256, 'ratio': 0.7734375},\n",
       " 867: {'gradient_norm': 278.8045463562012, 'ratio': 0.7578125},\n",
       " 868: {'gradient_norm': 279.5634250640869, 'ratio': 0.78125},\n",
       " 869: {'gradient_norm': 280.332706451416, 'ratio': 0.7578125},\n",
       " 870: {'gradient_norm': 281.09911155700684, 'ratio': 0.75390625},\n",
       " 871: {'gradient_norm': 281.8652153015137, 'ratio': 0.765625},\n",
       " 872: {'gradient_norm': 282.63402366638184, 'ratio': 0.7890625},\n",
       " 873: {'gradient_norm': 283.4067497253418, 'ratio': 0.76953125},\n",
       " 874: {'gradient_norm': 284.1784496307373, 'ratio': 0.77734375},\n",
       " 875: {'gradient_norm': 284.94068145751953, 'ratio': 0.75},\n",
       " 876: {'gradient_norm': 285.7179946899414, 'ratio': 0.765625},\n",
       " 877: {'gradient_norm': 286.4878234863281, 'ratio': 0.76171875},\n",
       " 878: {'gradient_norm': 287.2635154724121, 'ratio': 0.7734375},\n",
       " 879: {'gradient_norm': 288.04489517211914, 'ratio': 0.77734375},\n",
       " 880: {'gradient_norm': 288.82362937927246, 'ratio': 0.7890625},\n",
       " 881: {'gradient_norm': 289.609188079834, 'ratio': 0.7734375},\n",
       " 882: {'gradient_norm': 290.3880844116211, 'ratio': 0.79296875},\n",
       " 883: {'gradient_norm': 291.1729145050049, 'ratio': 0.7578125},\n",
       " 884: {'gradient_norm': 291.95537185668945, 'ratio': 0.76171875},\n",
       " 885: {'gradient_norm': 292.7386531829834, 'ratio': 0.796875},\n",
       " 886: {'gradient_norm': 293.52842903137207, 'ratio': 0.76953125},\n",
       " 887: {'gradient_norm': 294.31897163391113, 'ratio': 0.77734375},\n",
       " 888: {'gradient_norm': 295.1013641357422, 'ratio': 0.77734375},\n",
       " 889: {'gradient_norm': 295.89414405822754, 'ratio': 0.78125},\n",
       " 890: {'gradient_norm': 296.68888092041016, 'ratio': 0.7578125},\n",
       " 891: {'gradient_norm': 297.4799633026123, 'ratio': 0.77734375},\n",
       " 892: {'gradient_norm': 298.2678966522217, 'ratio': 0.75390625},\n",
       " 893: {'gradient_norm': 299.0602321624756, 'ratio': 0.78125},\n",
       " 894: {'gradient_norm': 299.85252571105957, 'ratio': 0.77734375},\n",
       " 895: {'gradient_norm': 300.65739822387695, 'ratio': 0.796875},\n",
       " 896: {'gradient_norm': 301.4594955444336, 'ratio': 0.78125},\n",
       " 897: {'gradient_norm': 302.2602596282959, 'ratio': 0.7890625},\n",
       " 898: {'gradient_norm': 303.06651496887207, 'ratio': 0.76953125},\n",
       " 899: {'gradient_norm': 303.8737487792969, 'ratio': 0.78125},\n",
       " 900: {'gradient_norm': 304.67809295654297, 'ratio': 0.765625},\n",
       " 901: {'gradient_norm': 305.4840316772461, 'ratio': 0.76953125},\n",
       " 902: {'gradient_norm': 306.29025650024414, 'ratio': 0.75390625},\n",
       " 903: {'gradient_norm': 307.10228157043457, 'ratio': 0.7734375},\n",
       " 904: {'gradient_norm': 307.91743087768555, 'ratio': 0.7578125},\n",
       " 905: {'gradient_norm': 308.72711753845215, 'ratio': 0.765625},\n",
       " 906: {'gradient_norm': 309.54163360595703, 'ratio': 0.76953125},\n",
       " 907: {'gradient_norm': 310.35324478149414, 'ratio': 0.76953125},\n",
       " 908: {'gradient_norm': 311.174596786499, 'ratio': 0.76171875},\n",
       " 909: {'gradient_norm': 311.9982395172119, 'ratio': 0.74609375},\n",
       " 910: {'gradient_norm': 312.81076431274414, 'ratio': 0.76171875},\n",
       " 911: {'gradient_norm': 313.6381893157959, 'ratio': 0.76953125},\n",
       " 912: {'gradient_norm': 314.4610900878906, 'ratio': 0.75390625},\n",
       " 913: {'gradient_norm': 315.28553009033203, 'ratio': 0.76953125},\n",
       " 914: {'gradient_norm': 316.11288833618164, 'ratio': 0.76953125},\n",
       " 915: {'gradient_norm': 316.9386157989502, 'ratio': 0.77734375},\n",
       " 916: {'gradient_norm': 317.7645683288574, 'ratio': 0.76953125},\n",
       " 917: {'gradient_norm': 318.59117698669434, 'ratio': 0.78515625},\n",
       " 918: {'gradient_norm': 319.4240322113037, 'ratio': 0.7578125},\n",
       " 919: {'gradient_norm': 320.2538013458252, 'ratio': 0.79296875},\n",
       " 920: {'gradient_norm': 321.0899715423584, 'ratio': 0.75390625},\n",
       " 921: {'gradient_norm': 321.92416763305664, 'ratio': 0.78125},\n",
       " 922: {'gradient_norm': 322.7527503967285, 'ratio': 0.76171875},\n",
       " 923: {'gradient_norm': 323.59429931640625, 'ratio': 0.7734375},\n",
       " 924: {'gradient_norm': 324.43421173095703, 'ratio': 0.7578125},\n",
       " 925: {'gradient_norm': 325.2713985443115, 'ratio': 0.76171875},\n",
       " 926: {'gradient_norm': 326.1108093261719, 'ratio': 0.7421875},\n",
       " 927: {'gradient_norm': 326.950159072876, 'ratio': 0.76171875},\n",
       " 928: {'gradient_norm': 327.79736709594727, 'ratio': 0.74609375},\n",
       " 929: {'gradient_norm': 328.64735412597656, 'ratio': 0.7734375},\n",
       " 930: {'gradient_norm': 329.49534606933594, 'ratio': 0.75390625},\n",
       " 931: {'gradient_norm': 330.3463954925537, 'ratio': 0.78515625},\n",
       " 932: {'gradient_norm': 331.1942939758301, 'ratio': 0.76953125},\n",
       " 933: {'gradient_norm': 332.03756523132324, 'ratio': 0.76171875},\n",
       " 934: {'gradient_norm': 332.88731384277344, 'ratio': 0.7734375},\n",
       " 935: {'gradient_norm': 333.7434310913086, 'ratio': 0.7734375},\n",
       " 936: {'gradient_norm': 334.6052074432373, 'ratio': 0.78515625},\n",
       " 937: {'gradient_norm': 335.45491218566895, 'ratio': 0.765625},\n",
       " 938: {'gradient_norm': 336.3149471282959, 'ratio': 0.765625},\n",
       " 939: {'gradient_norm': 337.17593574523926, 'ratio': 0.77734375},\n",
       " 940: {'gradient_norm': 338.0359802246094, 'ratio': 0.765625},\n",
       " 941: {'gradient_norm': 338.90167808532715, 'ratio': 0.75},\n",
       " 942: {'gradient_norm': 339.76644134521484, 'ratio': 0.77734375},\n",
       " 943: {'gradient_norm': 340.63369369506836, 'ratio': 0.78125},\n",
       " 944: {'gradient_norm': 341.50125885009766, 'ratio': 0.7578125},\n",
       " 945: {'gradient_norm': 342.37368392944336, 'ratio': 0.75},\n",
       " 946: {'gradient_norm': 343.2423782348633, 'ratio': 0.76171875},\n",
       " 947: {'gradient_norm': 344.1152801513672, 'ratio': 0.7578125},\n",
       " 948: {'gradient_norm': 344.98655700683594, 'ratio': 0.765625},\n",
       " 949: {'gradient_norm': 345.8381996154785, 'ratio': 0.76171875},\n",
       " 950: {'gradient_norm': 346.7208251953125, 'ratio': 0.78125},\n",
       " 951: {'gradient_norm': 347.5931968688965, 'ratio': 0.77734375},\n",
       " 952: {'gradient_norm': 348.47348403930664, 'ratio': 0.77734375},\n",
       " 953: {'gradient_norm': 349.34806060791016, 'ratio': 0.7578125},\n",
       " 954: {'gradient_norm': 350.2301902770996, 'ratio': 0.77734375},\n",
       " 955: {'gradient_norm': 351.11980056762695, 'ratio': 0.7734375},\n",
       " 956: {'gradient_norm': 352.00853729248047, 'ratio': 0.78125},\n",
       " 957: {'gradient_norm': 352.8941230773926, 'ratio': 0.76171875},\n",
       " 958: {'gradient_norm': 353.7853088378906, 'ratio': 0.7890625},\n",
       " 959: {'gradient_norm': 354.67892837524414, 'ratio': 0.765625},\n",
       " 960: {'gradient_norm': 355.5683784484863, 'ratio': 0.7734375},\n",
       " 961: {'gradient_norm': 356.4562568664551, 'ratio': 0.7421875},\n",
       " 962: {'gradient_norm': 357.3481216430664, 'ratio': 0.765625},\n",
       " 963: {'gradient_norm': 358.23917388916016, 'ratio': 0.7734375},\n",
       " 964: {'gradient_norm': 359.1345100402832, 'ratio': 0.765625},\n",
       " 965: {'gradient_norm': 360.027587890625, 'ratio': 0.7734375},\n",
       " 966: {'gradient_norm': 360.9185371398926, 'ratio': 0.7734375},\n",
       " 967: {'gradient_norm': 361.82149505615234, 'ratio': 0.75},\n",
       " 968: {'gradient_norm': 362.72475814819336, 'ratio': 0.76953125},\n",
       " 969: {'gradient_norm': 363.62622451782227, 'ratio': 0.7734375},\n",
       " 970: {'gradient_norm': 364.5277290344238, 'ratio': 0.765625},\n",
       " 971: {'gradient_norm': 365.4373588562012, 'ratio': 0.77734375},\n",
       " 972: {'gradient_norm': 366.3485641479492, 'ratio': 0.76953125},\n",
       " 973: {'gradient_norm': 367.264892578125, 'ratio': 0.76953125},\n",
       " 974: {'gradient_norm': 368.1758155822754, 'ratio': 0.74609375},\n",
       " 975: {'gradient_norm': 369.09536361694336, 'ratio': 0.765625},\n",
       " 976: {'gradient_norm': 370.0023536682129, 'ratio': 0.78125},\n",
       " 977: {'gradient_norm': 370.915470123291, 'ratio': 0.76953125},\n",
       " 978: {'gradient_norm': 371.829833984375, 'ratio': 0.765625},\n",
       " 979: {'gradient_norm': 372.7421531677246, 'ratio': 0.75},\n",
       " 980: {'gradient_norm': 373.64623260498047, 'ratio': 0.7734375},\n",
       " 981: {'gradient_norm': 374.5691680908203, 'ratio': 0.76171875},\n",
       " 982: {'gradient_norm': 375.48779296875, 'ratio': 0.734375},\n",
       " 983: {'gradient_norm': 376.41205978393555, 'ratio': 0.7734375},\n",
       " 984: {'gradient_norm': 377.33166122436523, 'ratio': 0.76171875},\n",
       " 985: {'gradient_norm': 378.2616310119629, 'ratio': 0.78125},\n",
       " 986: {'gradient_norm': 379.1910209655762, 'ratio': 0.75390625},\n",
       " 987: {'gradient_norm': 380.1252670288086, 'ratio': 0.7578125},\n",
       " 988: {'gradient_norm': 381.05605697631836, 'ratio': 0.75390625},\n",
       " 989: {'gradient_norm': 381.9882164001465, 'ratio': 0.78515625},\n",
       " 990: {'gradient_norm': 382.9197425842285, 'ratio': 0.76171875},\n",
       " 991: {'gradient_norm': 383.85595321655273, 'ratio': 0.78125},\n",
       " 992: {'gradient_norm': 384.7872543334961, 'ratio': 0.77734375},\n",
       " 993: {'gradient_norm': 385.7270050048828, 'ratio': 0.77734375},\n",
       " 994: {'gradient_norm': 386.67148208618164, 'ratio': 0.765625},\n",
       " 995: {'gradient_norm': 387.6026153564453, 'ratio': 0.7734375},\n",
       " 996: {'gradient_norm': 388.54345321655273, 'ratio': 0.765625},\n",
       " 997: {'gradient_norm': 389.4865188598633, 'ratio': 0.76953125},\n",
       " 998: {'gradient_norm': 390.4400520324707, 'ratio': 0.78515625},\n",
       " 999: {'gradient_norm': 391.38323974609375, 'ratio': 0.7734375},\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_norm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "770aee70-bf5f-4357-a822-a5c726ea44ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_losses_1 = [r['val_loss'] for r in history_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c53a94cd-9044-4756-890e-4b13ff9c286f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_losses_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09fac4f1-9b7a-46c9-9e7f-df4b03a5424b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minimal_ratio_eps = [i['ratio'] for i in gradient_norm_1.values() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f74da2a-8aad-496c-8922-74112ab24154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(minimal_ratio_eps,val_losses_1)\n",
    "plt.xlabel('loss')\n",
    "plt.ylabel('minimal ratio')\n",
    "plt.title('loss_vs_minimal ratio');\n",
    "plt.plot()\n",
    "plt.savefig(\"loss_vs_minimal ratio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe63ee2-b80d-474a-bbd1-dfa34c841f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepL",
   "language": "python",
   "name": "deepl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
